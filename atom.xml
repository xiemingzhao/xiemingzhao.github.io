<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>小火箭的博客</title>
  
  <subtitle>愿世界和平！！！</subtitle>
  <link href="https://www.xiemingzhao.com/atom.xml" rel="self"/>
  
  <link href="https://www.xiemingzhao.com/"/>
  <updated>2025-04-05T17:35:50.233Z</updated>
  <id>https://www.xiemingzhao.com/</id>
  
  <author>
    <name>小火箭</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>精排序列建模经典方案综述</title>
    <link href="https://www.xiemingzhao.com/posts/ubsmodel.html"/>
    <id>https://www.xiemingzhao.com/posts/ubsmodel.html</id>
    <published>2024-12-20T16:00:00.000Z</published>
    <updated>2025-04-05T17:35:50.233Z</updated>
    
    <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>在互联网应用的精排模型中，往往在<code>特征工程</code>、<code>样本构建</code>、<code>Loss 设计</code>、<code>模型结构</code>等方向进行迭代优化。其中，涉及特征与结构的<strong>用户行为序列建模</strong>是近几年的热点之一。</p><p>序列建模一般有2大方向：</p><ul><li>检索的序列更长；</li><li>建模的更精准。</li></ul><p>下面梳理近几年的经典序列建模方案，基本也是围绕上述 2 大方向进行不断优化的。</p><h2 id="1-DIN"><a href="#1-DIN" class="headerlink" title="1 DIN"></a>1 DIN</h2><h3 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1 概述"></a>1.1 概述</h3><p>论文：<a href="https://arxiv.org/abs/1706.06978">DIN: Deep Interest Network for Click-Through Rate Prediction</a><br>来源：2018，阿里</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel0.png" alt="ubsmodel0"></p><p><strong>思想：为了序列建模的更精准，通过 DIN 的 Attention 结构来替换 Base Model 的 Sum-Pooling 结构。</strong></p><span id="more"></span><h3 id="1-2-方案"><a href="#1-2-方案" class="headerlink" title="1.2 方案"></a>1.2 方案</h3><h4 id="1-2-1-DIN-的序列检索结构"><a href="#1-2-1-DIN-的序列检索结构" class="headerlink" title="1.2.1 DIN 的序列检索结构"></a>1.2.1 DIN 的序列检索结构</h4><p>历史行为中的不同物品对候选物品影响应该是有差异的，<code>Attention</code> 结构正是想打破 <code>Sum-Pooling</code> 的这种缺点。即在 <code>Sum-Pooling</code> 前，基于 <code>Activation Unit</code> （图右上）算出 <code>Weight</code>，然后做 <code>Weighted-Pooling</code>。</p><p>值得注意的是，<code>Activation Unit</code> 中的 <code>Out Product</code> 部分，在实践中往往如下处理（供参考），主要是为了增加非线性：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.concat([query, seq, query - seq, query * seq])</span><br></pre></td></tr></table></figure></p><h4 id="1-2-2-Dice-替代-PReLU"><a href="#1-2-2-Dice-替代-PReLU" class="headerlink" title="1.2.2 Dice 替代 PReLU"></a>1.2.2 Dice 替代 PReLU</h4><p><code>PReLU</code> 激活函数更容易出现参数更新缓慢甚至梯度消失的问题，论文使用更具泛化性的 <code>Dice</code> 激活函数。其二者的公式和函数图像如下所示：</p><p><code>PReLU</code>：</p><script type="math/tex; mode=display">f(s)= \begin{cases} s & \mathrm{if~}s>0 \\ \alpha s & \mathrm{if~}s\leq0. & \end{cases}=p(s) \cdot s+(1-p(s))\cdot\alpha s</script><p>其中， $p(s)=I(s &lt; 0)$ 为指示函数</p><p><code>Dice</code>：</p><script type="math/tex; mode=display">f(s)=p(s) \cdot s+(1-p(s)) \cdot \alpha s,p(s)=\frac{1}{1+e^{-\frac{s-E[s]}{\sqrt{Var[s]+\epsilon}}}}</script><p>其中，$\epsilon$一般取$10^{-8}$。可以发现：</p><ul><li><code>Dice</code> 是 <code>PReLu</code> 的推广，当 E[s] = 0，Var[s]=0 时，Dice 退化为 PReLU;</li><li>其核心思想是根据输入数据的分布自适应地调整校正。</li></ul><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel1.png" alt="ubsmodel1"></p><h4 id="1-2-3-GAUC-替代-AUC"><a href="#1-2-3-GAUC-替代-AUC" class="headerlink" title="1.2.3 GAUC 替代 AUC"></a>1.2.3 GAUC 替代 AUC</h4><p><code>AUC</code> 代表模型对样本整体的排序能力，不区分用户类型，比如高低活。</p><p>而实际线上应用的时候，不同用户之间是不需要对比的，<strong>更重要的是：同一个用户下，不同 item 能否区分准确。</strong></p><p>故论文提出了 <code>GAUC</code>：</p><script type="math/tex; mode=display">\mathrm{GAUC}=\frac{\sum_{i=1}^n\#impression_i\times\mathrm{AUC}_i}{\sum_{i=1}^n\#impression_i}</script><p>其中，$n$ 表示 User 的数量。</p><blockquote><p>实际中，建议 AUC 和 GAUC 结合一起判断，且后者一般要求 user 级别正负样本兼有。</p></blockquote><h4 id="1-2-4-Mini-batch-Aware-Regularization"><a href="#1-2-4-Mini-batch-Aware-Regularization" class="headerlink" title="1.2.4 Mini-batch Aware Regularization"></a>1.2.4 Mini-batch Aware Regularization</h4><p>是一种 <code>Adaptive</code> 的正则化方法。行为物品的参数空间大，使得模型容易过拟合，但传统的 L2 正则会对所有参数应用，效率低。<br>故论文提出了 <code>Mini-batch Aware Regularization</code> 方案：</p><script type="math/tex; mode=display">\begin{aligned}&L_{2}\left(w\right)=\left|\left|w\right|\right|^{2}=\sum_{j=1}^{K}\left|\left|w_{j}\right|\right|^{2}=\sum_{\left(x,y\right)\in S}\sum_{j=1}^{K}\frac{I\left(x_{j}\neq0\right)}{n_{j}}\left|\left|w_{j}\right|\right|^{2} \\&=\sum_{j=1}^{K}\sum_{m=1}^{B}\sum_{(x,y)\in B_{m}}\frac{I(x_{j}\neq0)}{n_{j}}||w_{j}||^{2} \\&=\sum_{j=1}^{K}\sum_{m=1}^{B}\frac{max_{(x,y)\in B_{m}}[I(x_{j}\neq0)]}{n_{j}}\left|\left|w_{j}\right|\right|^{2}\end{aligned}</script><p>其中，</p><ul><li>K：特征空间的维度；</li><li>S：全局样本；</li><li>B：mini-batch 的个数；</li><li>$x_j$：每个样本第 j 个特征值；</li><li>$I(x_j \ne 0)$：mini-batch 内，第 j 个特征值均为0的时候该值为0，否则为1；</li><li>$n_j$：样本中第 j 个特征出现的次数。</li></ul><p>最后一步，<strong>将所有的 $w_j$ 相加转为了只加最大（非0）的一次</strong>，如此高频（更重要）的特征正则权重就会变小，衰减慢一些。</p><h3 id="1-3-小记："><a href="#1-3-小记：" class="headerlink" title="1.3 小记："></a>1.3 小记：</h3><ul><li>DIN 优化了序列中不同 item 的权重、激活函数、正则方案以及评估指标；</li><li>但没有考虑序列先后关系、兴趣变化，且一般仅适用于短序列（论文中 14 天，序列长平均 35）。</li></ul><h2 id="2-DIEN"><a href="#2-DIEN" class="headerlink" title="2 DIEN"></a>2 DIEN</h2><h3 id="2-1-概述"><a href="#2-1-概述" class="headerlink" title="2.1 概述"></a>2.1 概述</h3><p>论文：<a href="https://arxiv.org/pdf/1809.03672">Deep Interest Evolution Network</a><br>来源：2019，阿里</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel2.png" alt="ubsmodel2"></p><p><strong>思想：引入 GRU 构建抽取兴趣层，使用 AUGRU 结构来做兴趣演化层，意在改善 DIN 没有考虑的行为先后关系和兴趣演变过程。</strong></p><h3 id="2-2-方案"><a href="#2-2-方案" class="headerlink" title="2.2 方案"></a>2.2 方案</h3><h4 id="2-2-1-兴趣提取层（Interest-Extractor-Layer）"><a href="#2-2-1-兴趣提取层（Interest-Extractor-Layer）" class="headerlink" title="2.2.1 兴趣提取层（Interest Extractor Layer）"></a>2.2.1 兴趣提取层（Interest Extractor Layer）</h4><p>实际中，行为序列可能比较长（14天平均30+），用户兴趣也在不断变迁。所以使用 <code>GRU</code> 来对用户行为之间的依赖进行建模。</p><p>选择 GRU 的原因是：</p><ul><li>克服了 RNN 的梯度消失问题；</li><li>速度比 LSTM 快。</li></ul><p>结合模型图，GRU 的结构如下所示：</p><script type="math/tex; mode=display">\begin{aligned}& \mathbf{u}_{t}=\sigma(W^{u}\mathbf{i}_{t}+U^{u}\mathbf{h}_{t-1}+\mathbf{b}^{u}), \\& \mathbf{r}_{t}=\sigma(W^{r}\mathbf{i}_{t}+U^{r}\mathbf{h}_{t-1}+\mathbf{b}^{r}), \\& \tilde{\mathbf{h}}_{t}=\mathrm{tanh}(W^{h}\mathbf{i}_{t}+\mathbf{r}_{t}\circ U^{h}\mathbf{h}_{t-1}+\mathbf{b}^{h}), \\& \mathbf{h}_{t}=(\mathbf{1}-\mathbf{u}_{t})\circ\mathbf{h}_{t-1}+\mathbf{u}_{t}\circ\tilde{\mathbf{h}}_{t}\end{aligned}</script><p>其中，</p><ul><li>$\sigma$是 simoid 激活函数；</li><li>$\circ$是元素乘；</li><li>$W^u,W^r,W^h \in \mathbb{R}^{n_H \times n_I}$，$U^z,U^r,U^h \in n_H \times n_H$，$n_H,n_I$分别是隐层和输入层的 size；</li><li>$i_t = e_b[t]$是序列中第 t 个物品的 embedding，也是 GRU 的输入。</li></ul><h4 id="2-2-2-辅助-Loss"><a href="#2-2-2-辅助-Loss" class="headerlink" title="2.2.2 辅助 Loss"></a>2.2.2 辅助 Loss</h4><p>使用辅助 Loss 想要解决的问题：</p><ul><li>GRU 只能学习行为间的依赖，不能有效地学习用户兴趣；</li><li>$L_{target}$ 只包含最终的目标信息，GRU 的隐层没有有效地监督信息；</li><li>辅助 item embedding 的学习更有效的信息。</li></ul><p>具体做法（结合上图）：用户 $i$ 的序列为 $b$，$t$ 时刻的$e_b^i[t]$对应的隐层状态为$h_t$，给其找一个正样本和一个负样本来构建辅助 Loss。</p><p><code>正样本</code>：点击序列的下一个 item，记为$e_b^i[t+1]$；<br><code>负样本</code>：除正样本$e_b^i[t+1]$之外的随机采样，记为$\hat e_b^i[t+1]$。</p><p>则辅助 Loss 为：</p><script type="math/tex; mode=display">L_{aux}= - \frac{1}{N}(\sum_{i=1}^N\sum_{t} \log \sigma {(h_t^i,e_b^i[t+1])} + \log (1 - \sigma{(h_t^i,\hat e_b^i[t+1]))})</script><p>故整体 Loss 为：</p><script type="math/tex; mode=display">L=L_{target}+α \ast L_{aux}</script><h4 id="2-2-3-兴趣演进层（Interset-Evolving-Layer）"><a href="#2-2-3-兴趣演进层（Interset-Evolving-Layer）" class="headerlink" title="2.2.3 兴趣演进层（Interset Evolving Layer）"></a>2.2.3 兴趣演进层（Interset Evolving Layer）</h4><p>该层是对 <code>target item</code> 相关的兴趣演化进行建模，使用的是带注意力更新门的 <code>GRU</code>，称为 <code>AUGRU</code>，即通过使用兴趣状态和 target item 计算得到的注意力权重。计算方式如下：</p><script type="math/tex; mode=display">a_t = \frac{\exp{(h_t \cdot W \cdot e_a)}}{\sum_{j = 1}^T \exp{(h_j \cdot W \cdot e_a)}}</script><p>其中，$e_a$ 是 target Ad 的 embedding。</p><p>针对此注意力，作者有 3 种用法：</p><ol><li><code>AIGRU</code>（GRU with attentional input）<br>直接与 Interset Evolving Layer 的输入相乘，即$i_t’ = h_t \asrt a_t$。</li><li><code>AGRU</code>（Attention based GRU）<br>替换 GRU 种的更新门，即 $h<em>t’ = (1 - a_t) \ast h</em>{t-1}’ + a_t \ast \tilde h_t’$。</li><li><code>AUGRU</code>（GRU with attentional update gate）<script type="math/tex; mode=display">\begin{aligned}& \tilde{\mathbf{u}}_{t}=a_{t}*\mathbf{u}_{t}, \\& \mathbf{h}_{t}=(1-\tilde{\mathbf{u}}_{t})\circ\mathbf{h}_{t-1}+\tilde{\mathbf{u}}_{t}\circ\tilde{\mathbf{h}}_{t}\end{aligned}</script></li></ol><p>这里我们将公式各项表示跟前述的 GRU 做了对齐利于理解，实际上就是构建了 $\tilde{\mathbf{u}}<em>{t}$ 来代替 $\mathbf{u}</em>{t}$。</p><h3 id="2-3-小记"><a href="#2-3-小记" class="headerlink" title="2.3 小记"></a>2.3 小记</h3><ul><li><code>GRU</code> 和 <code>AUGRU</code> 增加了模型的复杂度，一定程度能够提高对兴趣的学习，辅助 Loss 的作用不可忽视；</li><li>结构复杂带来的算力瓶颈是一大要害，作者提到作者提到了 GPU 优化、并行化、模型压缩等缓解方式，整体依然只能扛住 30-50 长度的序列；</li><li>作者有提过，DIN 的 Attention 在序列变长（&gt;100）后容易出现信息淹没，此处 GRU 也做了不少优化；</li><li>该模型叠加了不少复杂结构，是否真的有效用，这可能需要以具体场景中的时间为准。</li></ul><h2 id="3-BST"><a href="#3-BST" class="headerlink" title="3 BST"></a>3 BST</h2><h3 id="3-1-概述"><a href="#3-1-概述" class="headerlink" title="3.1 概述"></a>3.1 概述</h3><p>论文：<a href="https://arxiv.org/pdf/1905.06874">Behavior Sequence Transformer for E-commerce Recommendation in Alibaba</a><br>来源：2019，阿里</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel3.png" alt="ubsmodel3"></p><p><strong>思想：将当时比较火热的 Transformer 种的 Multi-head Self-attention 结构应用在用户行为序列建模中。</strong></p><h3 id="3-2-方案"><a href="#3-2-方案" class="headerlink" title="3.2 方案"></a>3.2 方案</h3><p><code>Transformer</code> 模型的细节这里不过多介绍，其 <code>Encoder</code> 中的每个 <code>Layer</code> 一般由 4 个子层构成，如上图右上。作者核心就是应用了这一部分。简单阐述为下面2块：</p><ul><li>将用户序列和 target item 看作整个 sequence 作为 Transformer Layer 的输入；</li><li>引入时序位置信息。</li></ul><p>其中，时序位置信息构建如下：</p><script type="math/tex; mode=display">pos(v_i) = t(v_t) - t(v_i)</script><p>实际上就是序列中每个点击行为距离 targte item 的时间差。</p><h3 id="3-3-小记"><a href="#3-3-小记" class="headerlink" title="3.3 小记"></a>3.3 小记</h3><p>客观上，这篇文章或多或少引起了一些<em>争议：是不是为了蹭 Transformer 热度，水分大不大。</em></p><p>至于到底如何，每个算法工程师可能都有自己的见解。这里我们罗列一些相对比较重要的疑问，供思考和讨论：</p><ol><li>Transformer 后做 concat 入模，是否合适？</li><li>为了做到上述1，限制了序列长度为20，是否具有效性？</li><li>时序位置信息的引入，在单位、分桶等处理细节上没有披露。</li><li>为何选择将 target item 并入一起做 Multi-head Self-attention，而没有做 Target Attention？</li></ol><h2 id="4-DSIN"><a href="#4-DSIN" class="headerlink" title="4 DSIN"></a>4 DSIN</h2><h3 id="4-1-概述"><a href="#4-1-概述" class="headerlink" title="4.1 概述"></a>4.1 概述</h3><p>论文：<a href="https://arxiv.org/pdf/1905.06482">Deep Session Interest Network for Click-Through Rate Prediction</a><br>来源：2019，阿里</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel4.png" alt="ubsmodel4"></p><p><strong>思想：用户在不同的 session 中行为差异明显，这是 DIEN 等没有考虑的，DSIN 中将序列分成多个 session 来处理。</strong></p><h3 id="4-2-方案"><a href="#4-2-方案" class="headerlink" title="4.2 方案"></a>4.2 方案</h3><p><strong><code>DSIN</code> 网络结构分为四层</strong></p><h4 id="4-2-1-Session划分层（Session-Division-Layer）"><a href="#4-2-1-Session划分层（Session-Division-Layer）" class="headerlink" title="4.2.1 Session划分层（Session Division Layer）"></a>4.2.1 Session划分层（Session Division Layer）</h4><p>Session 的划分方法：用户在行为序列中，超过半小时间隔处作为 Session 的切分点。</p><h4 id="4-2-2-Session兴趣抽取层（Session-Interest-Extractor-Layer）"><a href="#4-2-2-Session兴趣抽取层（Session-Interest-Extractor-Layer）" class="headerlink" title="4.2.2 Session兴趣抽取层（Session Interest Extractor Layer）"></a>4.2.2 Session兴趣抽取层（Session Interest Extractor Layer）</h4><p>引入 <code>bias encoding</code>，如下所示：</p><script type="math/tex; mode=display">\mathbf{BE}_{(k,t,c)} = \mathbf{w}_k^K + \mathbf{w}_t^T + \mathbf{w}_c^C</script><p>其中，</p><ul><li>$\mathbf{w}^K \in \mathbb{R} ^{K}$ 是 session 的 bias；</li><li>$\mathbf{w}^T \in \mathbb{R} ^{T}$ 是 session 内行为位置的 bias；</li><li>$\mathbf{w}^C \in \mathbb{R} ^{d_{model}}$ 是行为序列 item 的 embedding 每个元素的 bias。</li></ul><p>最终的序列 embedding 为：</p><script type="math/tex; mode=display">\mathbf{Q} = \mathbf{Q} +\mathbf{BE}</script><p><strong>注意：虽然$\mathbf{BE}$维度是$K \times T \times d<em>{model}$，但实际上参数个数为$K + T + d</em>{model}$。</strong></p><p>最后针对用户序列应用 Multi-head Self-attention 来抽取兴趣，该结构不再赘述，输出记为 $\mathbf{I}$。</p><h4 id="4-2-3-Session兴趣交互层（Session-Interest-Interacting-Layer）"><a href="#4-2-3-Session兴趣交互层（Session-Interest-Interacting-Layer）" class="headerlink" title="4.2.3 Session兴趣交互层（Session Interest Interacting Layer）"></a>4.2.3 Session兴趣交互层（Session Interest Interacting Layer）</h4><p>对用户 Session 的兴趣迁移进行建模，作者使用了 <code>Bi-LSTM</code> 结构，该层的最终隐层状态是前后向隐层状态的融合：</p><script type="math/tex; mode=display">\mathbf{H}_t=\overrightarrow{\mathbf{h}_{ft}} \oplus \overleftarrow{\mathbf{h}_{bt}}</script><p>其中，$\overrightarrow{\mathbf{h}<em>{ft}}$ 和 $\overleftarrow{\mathbf{h}</em>{bt}}$ 分别是前向和后向的 LSTM 隐层输出状态。而 $\oplus$ 文中没有明确解释，但我们结合模型图符号惯例以及 Bi-LSTM 的原理，很容易理解其应该是 <code>concat</code>。</p><h4 id="4-2-4-Session兴趣激活层（Session-Interest-Activating-Layer）"><a href="#4-2-4-Session兴趣激活层（Session-Interest-Activating-Layer）" class="headerlink" title="4.2.4 Session兴趣激活层（Session Interest Activating Layer）"></a>4.2.4 Session兴趣激活层（Session Interest Activating Layer）</h4><p>该层主要就是通过 2 个 <code>Activation Unit</code> 结构来抽取和 target Itemv相关的 Session 兴趣 embedding。可以看到，图中 <code>Activation Unit</code> 是一个经典的 <code>Target Attention</code> 结构。</p><p>黄色的部分（更浅层）：</p><script type="math/tex; mode=display">\begin{aligned}& a_k^{I}=\frac{\exp(\mathbf{I}_{k}\mathbf{W}^{I}\mathbf{X}^{I}))}{\sum_{k}^{K}\exp(\mathbf{I}_{k}\mathbf{W}^{I}\mathbf{X}^{I})} \\& \mathbf{U}^{I}=\sum_k^Ka_k^I\mathbf{I}_k\end{aligned}</script><p>其中，$\mathbf{X}^{I}$ 就是 Query 部分，来自图左侧的 Item Filed 构建的 Embedding，$\mathbf{W}^{I}$是转换矩阵。<br>而对于蓝色部分（更深层），则就是把$\mathbf{X}^{I}$、$\mathbf{W}^{I}$换成对应的深层参数$\mathbf{X}^{H}$、$\mathbf{W}^{H}$，其余计算保持不变。</p><h3 id="4-3-小记"><a href="#4-3-小记" class="headerlink" title="4.3 小记"></a>4.3 小记</h3><p><code>DSIN</code> 本身也是循着提升序列检索精度的方向：</p><ul><li>将序列拆分成不同的 Session 提供了一定的先验信息；</li><li>使用 <code>MHTA</code>、 <code>Bi-LSTM</code> 以及 <code>Target Attention</code> 一系列操作，具体有无效用，见仁见智，以具体的时间结果为准。</li></ul><h2 id="5-MIMN"><a href="#5-MIMN" class="headerlink" title="5 MIMN"></a>5 MIMN</h2><h3 id="5-1-概述"><a href="#5-1-概述" class="headerlink" title="5.1 概述"></a>5.1 概述</h3><p>论文：<a href="https://arxiv.org/pdf/1905.09248">Practice on Long Sequential User Behavior Modeling for Click-Through Rate Prediction</a><br>来源：2019，阿里</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel5.png" alt="ubsmodel5"></p><p><strong>思想：基于 DIEN 和 DSIN 的优势，MIMN 构建独立的 UIC 模块来更新用户兴趣 embedding，更新只依赖行为 event，不依赖 request。</strong></p><h3 id="5-2-方案"><a href="#5-2-方案" class="headerlink" title="5.2 方案"></a>5.2 方案</h3><h4 id="5-2-1-挑战"><a href="#5-2-1-挑战" class="headerlink" title="5.2.1 挑战"></a>5.2.1 挑战</h4><ul><li>序列建模中使用的用户行为序列越长，收益越大。（这点相信大多数场景经验都满足）</li><li>直接扩增序列会带来显著的 存储问题 和 性能问题。（论文披露：序列150-1k时，存储1T-6T，QPS=500时性能14ms-200m，要求&lt;30ms）</li></ul><p>核心解决思路如下图：</p><ul><li>不存储用户原始的行为序列，只存储用户的兴趣 embedding；</li><li>用户的兴趣 embedding 是可迭代更新的，并且其只依赖用户行为的 event，独立于 Server，在 request 时直接获取可降低 RT。</li></ul><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel6.png" alt="ubsmodel6"></p><h4 id="5-2-2-神经元图灵机（Neural-Turing-Machine）"><a href="#5-2-2-神经元图灵机（Neural-Turing-Machine）" class="headerlink" title="5.2.2 神经元图灵机（Neural Turing Machine）"></a>5.2.2 神经元图灵机（Neural Turing Machine）</h4><p>使用记忆参数 $\mathbf{M<em>t}$来存储序列信息，且有 $m$ 个槽位（slot），$\left {\mathbf{M_t}(i) \right } |</em>{i=1}^m$。</p><p>其更新和读取主要由下面2部分构成。</p><p><strong>1. 记忆读取（Memory Read）</strong><br>控制器生成一个寻址的 <code>key</code> 为 $k_t$，针对所有的 <code>memory slot</code> 计算权重：</p><script type="math/tex; mode=display">\mathbf{w}_t^r(i)=\frac{\exp(K(\mathbf{k}_t,\mathbf{M}_t(i)))}{\sum_j^m\exp(K(\mathbf{k}_t,\mathbf{M}_t(j)))},for \ i=1,2,...m</script><p>其中，</p><script type="math/tex; mode=display">K\left(\mathbf{k}_t,\mathbf{M}_t(i)\right)=\frac{\mathbf{k}_t^T\mathbf{M}_t(i)}{\|\mathbf{k}_t\|\|\mathbf{M}_t(i)\|}</script><p>最后输出为：</p><script type="math/tex; mode=display">\mathbf{r}_t=\sum_i^mw_t^r(i)\mathbf{M}_t(i)</script><p><strong>2. 记忆写入（memory write）</strong><br>首先控制器也会类似 <code>Memory Read</code> 生成一个 $\mathbf{w_t^w}$，此外还会生成加和向量项$\mathbf{a_t}$和衰减向量项$\mathbf{e_t}$。记忆矩阵$\mathbf{M_t}$的更新如下：</p><script type="math/tex; mode=display">\mathbf{M_t=(1-E_t)\odot M_{t-1}+A_t}</script><p>其中，</p><ul><li>$\mathbf{E_t} = \mathbf{w}_t^w \otimes \mathbf{e}_t$；</li><li>$\mathbf{A}<em>{\mathbf{t}}=\mathbf{w}</em>{t}^{\mathbf{w}} \otimes \mathbf{a}_{t}$；</li><li>$\odot$，$\otimes$ 分别表示向量内积和外积。</li></ul><h4 id="5-2-3-内存利用率正则（Memory-utilization-regularization）"><a href="#5-2-3-内存利用率正则（Memory-utilization-regularization）" class="headerlink" title="5.2.3 内存利用率正则（Memory utilization regularization）"></a>5.2.3 内存利用率正则（Memory utilization regularization）</h4><p>原始的 <code>NTM</code> 往往有<strong>内存利用不均衡问题，文章的解决方案是：根据不同记忆槽位的写入权重的方差来进行正则。</strong></p><script type="math/tex; mode=display">\mathbf{g}_t=\sum_{c=1}^t\mathbf{w}_c^{\tilde{w}}</script><p>如上所示是截止时间步$t$的累积更新权重，其中$\mathbf{w}_c^{\tilde{w}}$如下构建：</p><script type="math/tex; mode=display">\begin{aligned}&P_t= softmax(W_g \mathbf{g}_t) \\ & \mathbf{w}_t^{\tilde{w}}=\mathbf{w}_t^wP_t\end{aligned}</script><p>其中，$\mathbf{w}_t^w$是上述提到的原始写入权重，$P_t$是转换矩阵，$W_g$是由下列正则 Loss 学习得到：</p><script type="math/tex; mode=display">\begin{aligned}&\mathbf{w}^{\tilde{w}}=\sum_{t=1}^T\mathbf{w}_t^{\tilde{w}},\\&\mathbf{L}_{reg}=\lambda\sum_{i=1}^m\left(\mathbf{w}^{\tilde{w}}(i)-\frac{1}{m}\sum_{i=1}^m\mathbf{w}^{\tilde{w}}(i)\right)^2\end{aligned}</script><h4 id="5-2-4-记忆感知单元（Memory-Induction-Unit）"><a href="#5-2-4-记忆感知单元（Memory-Induction-Unit）" class="headerlink" title="5.2.4 记忆感知单元（Memory Induction Unit）"></a>5.2.4 记忆感知单元（Memory Induction Unit）</h4><p><code>NTM</code> 的 memory 一般是存储<code>原始信息</code>的，而 MIMN 的此模块的设计是<strong>为了捕捉高阶信息</strong>。如下图，<code>UBS</code> 会被分成多个 <code>Channel</code>，即 slot，假设 $m$ 个。<br>那么在第$\mathbf{t}$时间步的时候，会从 m 个 channel 中根据$\mathbf{w}_t^r(i)$选择 topK 个 channel，对于其中的每一个 channel i 按照下述更新：</p><script type="math/tex; mode=display">\mathrm{S}_t(i)=\mathrm{GRU}(\mathrm{S}_{t-1}(i),\mathrm{M}_t(i),e_t)</script><p>其中，</p><ul><li>$\mathrm{M}_t(i)$是 NTM 的第 i 个 memory slot；</li><li>$e_t$表示新增行为 item 的 embedding。</li></ul><p><strong>需要注意：不同 channel 的 GRU 参数是共享的。</strong><br><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel7.png" alt="ubsmodel7"></p><h3 id="5-3-小记"><a href="#5-3-小记" class="headerlink" title="5.3 小记"></a>5.3 小记</h3><ul><li>开篇的存储问题降到了 2.7T，性能压力降到了 19ms；</li><li>但模块上的独立，在效果上是否会有一定的折损，不同场景可能有一定差异；</li><li>普适度上也有一定限制，作者提到2点：行为数据较丰富；行为 event 量 &lt; 模型 request 量（否则 UIC 起不到缓解性能的作用）。</li></ul><p>此外，其团队提到由于资源占用、迭代受限，该框架不久后就放弃了这条路线。</p><h2 id="6-SIM"><a href="#6-SIM" class="headerlink" title="6 SIM"></a>6 SIM</h2><h3 id="6-1-概述"><a href="#6-1-概述" class="headerlink" title="6.1 概述"></a>6.1 概述</h3><p>论文：<a href="https://arxiv.org/pdf/2006.05639">Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction</a><br>来源：2020，阿里</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel8.png" alt="ubsmodel8"></p><p><strong>思想：为了处理更长的行为序列，构建 GSU（泛检索）+ESU（精检索）两阶段的框架，是一个非常有实战价值的做法。</strong></p><h3 id="6-2-方案"><a href="#6-2-方案" class="headerlink" title="6.2 方案"></a>6.2 方案</h3><h4 id="6-2-1-挑战"><a href="#6-2-1-挑战" class="headerlink" title="6.2.1 挑战"></a>6.2.1 挑战</h4><ul><li>序列越长效果越好，尤其是用户行为活跃度高时，长序列就更重要；</li><li>MIMN 处理的序列超过 1k 时效果变差，缺少和 target item 的交互。</li></ul><p>作者提出了 <code>GSU</code>（General Search Unit） + <code>ESU</code>（Exact Search Unit） 的方案，如上模型图所示，可以说开辟了序列建模又一新范式。</p><h4 id="6-2-2-一阶段-GSU（General-Search-Unit，通用搜索单元）"><a href="#6-2-2-一阶段-GSU（General-Search-Unit，通用搜索单元）" class="headerlink" title="6.2.2 一阶段 GSU（General Search Unit，通用搜索单元）"></a>6.2.2 一阶段 GSU（General Search Unit，通用搜索单元）</h4><p>该阶段很明显是要从行为序列中粗筛 topK 个与 target item 相关的 candidate item，作者在这里介绍了2种方法，<code>hard-seach</code> 和 <code>soft search</code>。</p><script type="math/tex; mode=display">r_i=\begin{cases}Sign(C_i=C_a)&hard-search\\(W_b\mathbf{e}_i)\odot(W_a\mathbf{e}_a)^T&soft-search&\end{cases}</script><p><strong>1. hard-seach</strong><br>顾名思义，相对比较粗糙但直接有效，<strong>即行为序列中与 target item 具有同类目的就可以作为 candidate item（如模型图中上所示）。</strong></p><blockquote><p>这里有一个点：类目也是一种泛指，具体用几级类目？能不能用其他维度？都需要根据实际场景来选择。</p></blockquote><p><strong>但经验上，选择的维度一定要在业务场景中举足轻重，当然也可以是多个</strong>。比如电商的根类目、叶子类目、品牌，内容社区的话题、语言等。</p><p><strong>2. soft-search</strong></p><p>上述 hard 方式虽然简单直接，<em>但依赖检索类目的质量，相关性无法保障。</em></p><p><strong>一个朴素的想法便是：使用 target item 的 embedding 去检索序列中 item emebdding 距离近的 topK。</strong></p><p>如上公式所示，</p><ul><li>$W_b,W_a$ 均是<code>变换矩阵</code>；</li><li>$e_a,e_i$ 分别是 target item 和 candidate item 的 embedding；</li><li>$\odot$ 表示<code>内积</code>。</li></ul><p>需要注意，作者提出因短期兴趣和长期兴趣分布有差异，故它们的 item embedding 不能 <code>share</code>，<strong>针对 <code>soft-search</code> 模块单独构建了一个网络来辅助学习</strong>，如上图左所示。</p><h4 id="6-2-3-二阶段-ESU（Exact-Search-Unit，精准搜索单元）"><a href="#6-2-3-二阶段-ESU（Exact-Search-Unit，精准搜索单元）" class="headerlink" title="6.2.3 二阶段 ESU（Exact Search Unit，精准搜索单元）"></a>6.2.3 二阶段 ESU（Exact Search Unit，精准搜索单元）</h4><p>经过 <code>GSU</code>，序列长度一般下降一个量级以上，该阶段能够应用相对比较复杂的序列建模结构，如模型图右所示。</p><ul><li>短序列使用的是 <code>DEIN</code> 结构；</li><li>长序列经过 <code>GSU</code> 检索的 topK 则使用 <code>Multi-head Attention</code> 结构。</li></ul><p>最后则是将两个阶段进行联合 training (soft-search 的时候)：</p><script type="math/tex; mode=display">Loss=\alpha Loss_{GSU} + \beta Loss_{ESU}</script><h3 id="6-3-小记"><a href="#6-3-小记" class="headerlink" title="6.3 小记"></a>6.3 小记</h3><ul><li>文章使用 180 天数据构建长期序列，最长 54000，比 MIMN 提升 54 倍，性能增加 5ms；</li><li>在 GSU 部分，hard-search 方案几乎没有性能问题，针对 soft-search 文章提到可以使用 MIPS 指令集优化等加速；</li><li>该方案思路新颖，实践效果佳，也为业界开启了 GSU+ESU 的迭代方向。</li></ul><h2 id="7-ETA"><a href="#7-ETA" class="headerlink" title="7 ETA"></a>7 ETA</h2><h3 id="7-1-概述"><a href="#7-1-概述" class="headerlink" title="7.1 概述"></a>7.1 概述</h3><p>论文：<a href="https://arxiv.org/pdf/2108.04468">End-to-End User Behavior Retrieval in Click-Through RatePrediction Model</a><br>来源：2021，阿里</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel9.png" alt="ubsmodel9"></p><p><strong>思想：用 LSH 来加速 GSU 环节，并将 GSU 融入到 ESU 环节，构建端到端，相对两阶段来增加检索一致性。</strong></p><h3 id="7-2-方案"><a href="#7-2-方案" class="headerlink" title="7.2 方案"></a>7.2 方案</h3><h4 id="7-2-1-SimHash"><a href="#7-2-1-SimHash" class="headerlink" title="7.2.1 SimHash"></a>7.2.1 SimHash</h4><p>这个是 <code>ETA</code> 在 <code>GSU</code> 加速的核心，<code>SimHash</code> 是<strong>一种局部敏感哈希(LSH)方法，能够近似的计算向量间的相似度</strong>，说白了就是为了改善向量内积检索的速度。</p><p>文章通过伪代码和向量旋转来解释 <code>SimHash</code> 的原理，我们这里直接讲实操可能更利于理解。假设 $\mathbf{e} \in \mathbb{R^{n \times d}}$表示行为序列的 item embedding，$n,d$ 是序列长度和 embedding size。</p><p>那么，<code>SimHash</code> 步骤如下：</p><ul><li>固定一个随机生成的 Hash 矩阵 $\mathbf{H} \in \mathbb{R}^{d \times m}$，其中 m 是超参数，代表 <code>Hash 编码后的维度</code>；</li><li>对于每个$e_k$，按照如下方式构建 SimHash 的编码 $sig_k \in \mathbb{R}^{1 \times m}$：</li></ul><script type="math/tex; mode=display">temp_k[i] =\sum_{j=1}^{d}\mathrm{sgn}(e_{k}[j]*H[j][i])</script><script type="math/tex; mode=display">sig_{k}[i] = 1 \ if \ temp_k[i] < 0 \ else \ 0</script><blockquote><p>相当于所有的 $d$ 维的 item embedding 都经过 $\mathbf{H} \in \mathbb{R}^{d \times m}$ 编码成了 $m$ 维的二进制向量了。</p></blockquote><h4 id="7-2-2-模型"><a href="#7-2-2-模型" class="headerlink" title="7.2.2 模型"></a>7.2.2 模型</h4><p>如上模型图所示:</p><ul><li>针对每个 target item（$e_t$），对其进行 SimHash 编码成<code>二进制向量</code>$h_t$；</li><li>对用户行为序列中的 candidate item（$e<em>{k+1}$）也进行同样的 SimHash 编码成二进制向量$h</em>{k+1}$；</li><li>基于上述，使用<code>汉明距离</code>来检索与 target item 最近的 topK 个candidate item，完成 GSU 部分；</li><li>将上述 topK 个 item 作为 ESU 的输入，构建 <code>Multi-head Target Attention</code>，其余雷同。</li></ul><p><strong>需要注意的是：</strong></p><ul><li><code>Offline Training</code> 时，ETA 中的 SimHash、GSU、ESU 这整个过程是一个 End-to-End 的，即每一 step，除了 Hash 映射不变外，其他参数都在 update；</li><li><code>Online Serving</code> 时，因为不管是 target 还是 candidate item，它们的 embeding 和 Hash Matrix 都是不变的，故可以提前计算它们的 <code>SimHash Sig</code>，线上直接 lookup 即可使用。</li></ul><h3 id="7-3-小记"><a href="#7-3-小记" class="headerlink" title="7.3 小记"></a>7.3 小记</h3><ul><li>文章提到 ETA 效果优于 SIM，且 SimHash 检索后 Attention 和直接全序列 Attention 在 AUC 只差 0.1%；</li><li>也提到 ETA 性能相比于 dot-product 更优（32ms-19ms），因为将检索依赖的 embedding 转换成了更低维的二进制向量，使得检索时速度增加；</li><li>加速 GSU、提高 GSU 和 ESU 环节的一致性，确实是沿着两阶段方向的一个重点迭代思路，当然这种改善具体提升多少还依赖实践情况。</li></ul><h2 id="8-SDIM"><a href="#8-SDIM" class="headerlink" title="8 SDIM"></a>8 SDIM</h2><h3 id="8-1-概述"><a href="#8-1-概述" class="headerlink" title="8.1 概述"></a>8.1 概述</h3><p>论文：<a href="https://arxiv.org/pdf/2205.10249">Sampling Is All You Need on Modeling Long-Term User Behaviors for CTR Prediction</a><br>来源：2022，美团</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel10.png" alt="ubsmodel10"></p><p><strong>思路：结合 SIM 和 ETA 的优势，提出使用 Hash Transform 和 Sample-Based Attention 来替换 <code>GSU+ESU</code> 的框架（如模型图右侧所示），直接构建 <code>End-to-End</code> 模型。</strong></p><h3 id="8-2-方案"><a href="#8-2-方案" class="headerlink" title="8.2 方案"></a>8.2 方案</h3><h4 id="8-2-1-挑战"><a href="#8-2-1-挑战" class="headerlink" title="8.2.1 挑战"></a>8.2.1 挑战</h4><ul><li>GSU 的存在，使得检索 topK 可能会存在信息堵塞的情况，比如相似 item 占位太多；</li><li>既然 SimHash 已经应用到了 GSU 部分，有没有可能打通 ESU 部分构建 End-to-End。</li></ul><p>作者的解决方案的<strong>关键点是：既然 target item 和 candidate item 都可以通过 SimHash 来编码，并且还可以计算近似的相似度，如果基于此还能获取到 embedidng 就完成了全局 Attention 的替换。</strong></p><p><code>SDIM</code> 模型的全称是 Sampling-based Deep Interest Modeling。</p><p>如模型图左上所示，实际上是通过2步：</p><ul><li>将 UBS 进行 Hashing 后编码成<code>签名映射表</code>；</li><li>将 target item 也进行 Hashing 编码成签名，去上述映射表直接检索聚合成最终的 <code>Attention Embedding</code>。</li></ul><h4 id="8-2-2-Multi-Round-Hash"><a href="#8-2-2-Multi-Round-Hash" class="headerlink" title="8.2.2 Multi-Round Hash"></a>8.2.2 Multi-Round Hash</h4><p>这里的思路与 ETA 极其相似，但为了打通 ESU 部分，做了一些改进。</p><p>针对 UBS 中任一 item 的 embedding 记为 $x$，先构建基础的 <code>SimHash 编码</code>，这一步与 ETA 一致：</p><script type="math/tex; mode=display">h(\mathbf{x},\mathbf{R})=\mathrm{sign}(\mathbf{R}\mathbf{x})</script><p>其中，</p><ul><li>$\mathbf{R} \in \mathbb{R}^{m \times d}$是 <code>Hash 矩阵</code>，<strong>m 是 Hash 编码后的维度，d 是 item embedding size</strong>；</li><li>$h(\mathbf{x},\mathbf{R}) \in \mathbb{R}^{m}$是 Hash 编码结果，<code>m 维</code>。</li></ul><p>假设 UBS 长度为 T，我们就可以得到 T 个 m 维的 Hash Code。如下模型图左下，$T=4,\ m = 4$。</p><p>给定超参数 $\tau$，代表需要将 <code>Hash Code</code> 分组的宽度，如下图中 $\tau=2$，则每个 item 的 <code>Hash Code</code> 可以被分成 2 组，图中黄色和绿色部分。</p><p>然后我们将每个 item 同组的 Hash Code 聚合成一张 <code>Hash SigSignature Table</code>，其中：</p><ul><li><code>sig.</code> 存储的是该组<code>去重的 Hash Code</code>；</li><li><code>value</code> 存储的是对应的 <code>norm embedding</code>，它是由相同 sig. 对应的 item embedding 进行归一化（norm）得到。</li></ul><p>可以看到，这里的<strong>思想是基于 <code>Hash Code</code> ，将局部位置相似的 item embedding 聚合作为局部信息的表征，实际上是一种聚类的思想，容易联想到向量检索算法中的 PQ（乘积量化）。</strong></p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel11.png" alt="ubsmodel11"></p><h4 id="8-2-3-Hash-Based-Sampling"><a href="#8-2-3-Hash-Based-Sampling" class="headerlink" title="8.2.3 Hash-Based Sampling"></a>8.2.3 Hash-Based Sampling</h4><p>有了上述的基础，这里就比较明朗了：</p><ul><li>首先针对 target item 也行同样的 Hash 编码并按照 $\tau$ 宽度进行分组<br>（理论上组数应该与前述的 Hash SigSignature Table 个数一样。）</li><li>将每一组的 <code>sig.</code> 作为 <code>key</code> 去对应的 <code>Hash SigSignature Table</code> 中查询 <code>value</code>，作为结果 <code>embedding</code>；</li><li>将所有查到的 <code>value</code> 进行 pooling，得到最终 <code>Target Attention</code> 的结果 <code>Embedding</code>。</li></ul><p>至此，完成了对 <code>GSU+ESU</code> 的替换，是一种 End-to-End 的对长序列进行 Target Attention 建模的结构。</p><h3 id="8-3-小记"><a href="#8-3-小记" class="headerlink" title="8.3 小记"></a>8.3 小记</h3><p>实际上，个人直观的思路是直接用 SimHash 后的汉明距离倒数作为 Attention Weight 来计算，但作者没有选择，可能存在的原因：</p><ul><li>汉明距离作为召回可能尚可，作为 weight 可能噪声大，序的分辨度也许不高；</li><li>相似度计算简单了，但需要处理的长度依然太长。</li></ul><p>回到 <code>SDIM</code>，作者提到：</p><ul><li>效果上，对比 ETA 由 AUC+0.6%-1%；</li><li>性能上，较 ETA 快 3 倍。</li><li>如下图所示，<code>SDIM</code> 与传统的 Target Attention 的结果对比，相似度很高。</li><li>参数 m 越大效果越好，但过大性价比不高；</li><li>参数$\tau$的增大，AUC 先增后减。<em>因为：太小，分组太多，泛化不够；太大，分组太少，组内区分度不够</em>。</li></ul><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel12.png" alt="ubsmodel12"></p><h2 id="9-TWIN"><a href="#9-TWIN" class="headerlink" title="9 TWIN"></a>9 TWIN</h2><h3 id="9-1-概述"><a href="#9-1-概述" class="headerlink" title="9.1 概述"></a>9.1 概述</h3><p>论文：<a href="https://arxiv.org/pdf/2302.02352">TWIN: TWo-stage Interest Network for Lifelong User Behavior Modeling in CTR</a><br>来源：2023，快手</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel13.png" alt="ubsmodel13"></p><p><strong>思想：ESU 是 Target Attention，GSU 的检索方式越对齐一致性越好。将 GSU 的 Target Attention 计算进行拆分，固有属性部分做缓存后 Lookup，交叉部分降维后作 Bias。</strong></p><h3 id="9-2-方案"><a href="#9-2-方案" class="headerlink" title="9.2 方案"></a>9.2 方案</h3><h4 id="9-2-1-挑战"><a href="#9-2-1-挑战" class="headerlink" title="9.2.1 挑战"></a>9.2.1 挑战</h4><blockquote><p>ESU 和 GSU 往往存在一致性问题： GSU 和 ESU 在序列 Item 与 Target Item 的相似计算方式上不一样, 从而导致 GSU 检索的 topK 往往与 ESU 有差异。（如下图）</p></blockquote><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel14.png" alt="ubsmodel14"></p><p><strong>诸如 ETA、SDIM 都是通过使用其他近似算法来优化 GSU 过程</strong>，使得 GSU 可以处理更长序列，同时逼近 Target Attention。但，上述近似算法始终与 <code>MHTA</code> 算法有一定差异，<strong><code>TWIN</code> 则是通过将 Attention 进行拆解，将 GSU 的 Target Attention 部分更进一步的逼近于 <code>MHTA</code>。</strong></p><h4 id="9-2-2-Behavior-Feature-Splits-and-Linear-Projection"><a href="#9-2-2-Behavior-Feature-Splits-and-Linear-Projection" class="headerlink" title="9.2.2 Behavior Feature Splits and Linear Projection"></a>9.2.2 Behavior Feature Splits and Linear Projection</h4><p>序列特征的分解与线性映射，是为了提升 Attention 模块性能。因 Multi-Head Target Attention（MHTA）的<strong>主要耗时在于两部分：序列信息做线性映射、内积加权和。</strong></p><p>序列特征可以分为：</p><ul><li><code>固有特征</code>（如标题、作者、视频ID等）；</li><li><code>交互特征</code>（如点击时间、观看时长等）。</li></ul><p>其中，</p><ul><li><code>固有特征</code>独立于 user，包括其行为序列，可以提前计算存储下来，线上直接 lookup 即可。</li><li><code>交叉特征</code>不能使用缓存方案，与 user 行为序列有关，但每个 user 最多只看每个 item 一次。</li></ul><p>基于上述特性，我们<strong>将交叉特征线性映射为 1 维。</strong></p><p>假设 UBS 为$[s_1,s_2,…,s_L]$ ，对应的<code>特征矩阵</code>为 $K$。则$K$可以拆分为两部分，如下：</p><script type="math/tex; mode=display">K\triangleq[K_h,K_c]\in R^{L\times(H+C)}</script><p>其中 $K_h \in R^{L \times H}$ 是<code>固有特征</code>， $K_c \in R^{L \times C}$ 是则是<code>交互特征</code>部分。</p><p>如上所述， $K_h$ 可以提前离线计算并缓存供线上 Lookup 使用。<br>对于<code>交互特征</code> $K_c$，假设有$J$个，每个 8 维，文章提到可将其均映射为 1 维，如下所示：</p><script type="math/tex; mode=display">K_{c}W^{c}\triangleq[K_{c,1}W_{1}^{c},\ldots,K_{c,J}W_{J}^{c}</script><p>其中 $K_{c,j} \in R^{L \times 8}$ 为第 $j$ 个<code>交互特征</code>，$W_j^c \in R^8$ 则是对应的<code>权重参数</code>。</p><h4 id="9-2-3-Target-Attention-in-TWIN"><a href="#9-2-3-Target-Attention-in-TWIN" class="headerlink" title="9.2.3 Target Attention in TWIN"></a>9.2.3 Target Attention in TWIN</h4><p>上述的操作主要都是为了提速，当然在 Attention 部分也做了适配改造。</p><ul><li><code>Q、K</code> 的固有属性部分直接 Lookup <code>缓存</code>得到；</li><li>降维后的交叉特征部分则作为 <code>Bias</code> 项；</li><li>Target Item 仅与固有特征做内积（<em>快手曝光频控一次，故 Target Item 没有交叉特征</em>）。</li></ul><script type="math/tex; mode=display">\alpha = \frac{(K_h W^h)(q^T W^q)^T}{\sqrt{d_k}}+(K_c W^c) \beta</script><p>则，<strong>这里的 $\alpha$ 实际上就是 Target Attention 的内积结果</strong>。</p><ul><li>GSU 阶段用这个对序列 Item 做粗筛 Top100；</li><li>ESU 阶段对这 Top100 再做一次简化的 Target Attention。</li></ul><p>如下所示：</p><script type="math/tex; mode=display">Attention(q^{T} W^{q},K_{h} W^{h},K_{c} W^{c},K W^{v})=Softmax(\alpha)^{T}K W^{v}</script><p><strong>注意：ESU 的$\alpha$实际上是重新计算的，不是 GSU 中的。</strong></p><p>文章提到，实际业务中使用 <code>MHTA</code>，且 head 数为 4，所以最终如下：</p><script type="math/tex; mode=display">TWIN=Concat(\mathrm{head}_1,...,\mathrm{head}_4)W^o</script><script type="math/tex; mode=display">\mathrm{head}_a=\mathrm{Attention}(\mathbf{q}^\top W_a^q,K_hW_a^h,K_cW_a^c,KW_a^v),a\in\{1,...,4\}</script><p>其中，$W^o$是 head 之间的权重，也是模型学习得到。</p><h3 id="9-3-小记"><a href="#9-3-小记" class="headerlink" title="9.3 小记"></a>9.3 小记</h3><p><code>TWIN</code> 的有效性主要得益于 3 点：</p><ol><li>作者将序列特征 拆分成了 固有属性 和 交互特征，分别使用缓存（命中率99.3%）和降维分而治之；</li><li>基于上述，对 Target Attention 做了简化；</li><li>业务上，Target Item 与 UBS 没有交互提供了上述可拆分的支持。</li></ol><p><code>TWIN</code> 进一步提高了 GSU 和 ESU 部分的一致性（如下图所示），GSU 也用上了 Target Attention，且能够支持 $10^5$的序列。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel15.png" alt="ubsmodel15"></p><p><strong>但这里有一个问题：如果 GSU 能够做到 Target Attention 为什么不统一成全局 ESU，还要保留 GSU 来粗筛 top100 来给 ESU？</strong></p><p>实际上，这有 2 个原因：</p><ol><li>Q、K 做了简化，V 的 Project、 Weight Pooling 以及 bp 都是很耗时的过程，且 100 后的$\alpha$往往都很小信息量不大，所以截取 top100 还是很具有性价比的；</li><li>虽然 GSU 和 ESU 的 Attention 结构一样，但分数上依然存在些许差异。因为 GSU 是离线计算，其参数更新速度没有 ESU 部分快。故 ESU 部分重新计算$\alpha$，性能可支持、实时性更高、准确度更好。</li></ol><h2 id="10-TWIN-V2"><a href="#10-TWIN-V2" class="headerlink" title="10 TWIN-V2"></a>10 TWIN-V2</h2><h3 id="10-1-概述"><a href="#10-1-概述" class="headerlink" title="10.1 概述"></a>10.1 概述</h3><p>论文：<a href="https://arxiv.org/pdf/2407.16357v2">TWIN V2: Scaling Ultra-Long User Behavior Sequence Modeling for Enhanced CTR Prediction at Kuaishou</a><br>来源：2024，快手</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel16.png" alt="ubsmodel16"></p><p><strong>思想：为了使得 TWIN 可以支持到10^6量级，TWIN-V2 基于层次聚类，对 UBS 中的 Item 做聚类后得到簇序列，从而将序列量级从 Item 元素的$10^6$量级降到簇元素下的$10^5$量级，之后再应用 TWIN 即可。</strong></p><h3 id="10-2-方案"><a href="#10-2-方案" class="headerlink" title="10.2 方案"></a>10.2 方案</h3><h4 id="10-2-1-Hierarchical-Clustering"><a href="#10-2-1-Hierarchical-Clustering" class="headerlink" title="10.2.1 Hierarchical Clustering"></a>10.2.1 Hierarchical Clustering</h4><p>Item 数量太多，则将 Item 聚类成<code>簇</code>，变成<code>簇序列</code>，量级下来后，以簇为 <code>新 Item</code>支持完成 <code>TWIN</code> 模型。</p><p><code>Item 分层</code>：对 UBS 的各个 Item $v_j$，根据<code>完播率</code>$p_j=playing \ timevideo \ duration$分成 <code>M 组</code>(文章中M=5)，可以使用<code>等宽分组</code>。这里实际上是对用户偏好进行了显式分层。</p><p><code>Item 聚类成簇</code>：这里文章给了算法的伪代码，这里简述要点。</p><ul><li>逐个处理 M 组序列，分别对其进行聚类；</li><li>每个簇内部最多包含 $\gamma$个 Item，如某组序列的 Item 总数少于此，整体作为一簇；</li><li>数量够的，计算需要的聚类数 $\delta \leftarrow \lfloor |V|^{0.3} \rfloor$；</li><li>根据 Item 的 Embedding，将该组内的 Item 进行 Kmeans 聚类，聚类数为上述 $\delta$。</li></ul><p>最终将原始 UBS 的 Item 序列即 $S=[s<em>1,s_2,\cdots,s_T]$转化成了<code>簇序列</code>，即$C=[c</em>{1},c<em>{2},\cdots,c</em>{\hat{T}}]$。</p><p>此外，文章提到：</p><ul><li>层次聚类 2 周完整更新一次，毕竟是全生命周期的，计算量大；</li><li>Embedding Server 来源 GSU 的固有属性, 每隔15分钟进行同步；</li><li>实践中簇的内部大小$\gamma=20$，而最终的簇个数平均为 10，相当于将序列量级下降1级。</li></ul><h4 id="10-2-2-Extracting-Cluster-Representation"><a href="#10-2-2-Extracting-Cluster-Representation" class="headerlink" title="10.2.2 Extracting Cluster Representation"></a>10.2.2 Extracting Cluster Representation</h4><p>在得到各个簇之后，需要构建<code>簇的表征</code>，否则下游的模型无法使用。逻辑上也是将簇内 Item 两种类型的特征单独分开处理。</p><p><code>连续型特征</code>，<strong>取簇内各 Item 的均值</strong>:</p><script type="math/tex; mode=display">\mathbf{c}_{1:N_2}^{(i)}=\frac{1}{|c_i|}\sum_{v\in c_i}\mathbf{x}_{1:N_2}^{(v)}</script><p>但<code>分类型特征</code>，均值就没意义了。文中提到<strong>从簇中选取一个代表性的 Item 来表示，筛选方案是：与聚类中心的距离最小的</strong>。</p><script type="math/tex; mode=display">v=\arg\min_{v\in c_{i}}\|\mathrm{k}_{v}-\mathrm{k}_{\mathrm{centroid}}\|_{2}^{2}</script><p>最后将分类型和连续型特征 <code>concat</code> 即可作为簇的 Embedding 了。</p><h4 id="10-2-3-Cluster-aware-Target-Attention"><a href="#10-2-3-Cluster-aware-Target-Attention" class="headerlink" title="10.2.3 Cluster-aware Target Attention"></a>10.2.3 Cluster-aware Target Attention</h4><blockquote><p>原始序列从$S$已经下降一个量级到$C$了，并且对应的 Embedding 也具备，可以直接应用 TWIN 模型了。</p></blockquote><script type="math/tex; mode=display">\alpha=\frac{(\mathrm{K}_h\mathrm{W}^h)(\mathrm{q}^\top\mathrm{W}^q)^\top}{\sqrt{d}_k}+(\mathrm{K}_c\mathrm{W}^c)\beta</script><p>注意力分数依然按照上述计算，但文章提到，这时候的元素已经不再是 Item 了，<strong>如果不同的类簇有相同的 Score，那么簇内 Item 数更多的理论上更置信。</strong></p><p>故，对注意力分做了矫正：</p><script type="math/tex; mode=display">\alpha^{\prime}=\alpha+\ln\mathbf{n}</script><p>其中$\mathbf{n}$是簇内 Item 的数量。在 GSU 和 ESU 环节均使用$\alpha^{\prime}$来计算注意力分，其余环节与 TWIN 保持一致。</p><h3 id="10-3-小记"><a href="#10-3-小记" class="headerlink" title="10.3 小记"></a>10.3 小记</h3><p>为了支撑更大的量级，在 TWIN-V2 中，选择<strong>将问题转化为 TWIN 能处理的量级，方法就是对原始的 Item 进行分层聚类，从而将原始的 Item 序列转化为低一个量级的聚类簇序列。</strong></p><p>文章在实验部分提到效果较为显著，<strong>但聚类本身容易带来信息丢失</strong>，尤其是下面2个环节：</p><ul><li>$M,\gamma,\delta$的超参数选择；</li><li>簇的类型特征的表征。</li></ul><p>故，该方法的实际效用如何，还需要以具体场景的实践结果为准。</p><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>除了上述提到的一系列文章外，业内当然还有不少其他方面的研究成果。笔者没有再进一步整理，一方面考虑到篇幅过大，另一方面也是个人判断方案的普适性。</p><p>上述展开的一系列成果比较契合序列建模迭代的 2 大方向且成果往往也在多个场景实践落地，更具参考价值。</p><p>当然，这里也附上部分近年的相关文章供参考：<br><a href="https://arxiv.org/pdf/2311.10764">DGIN</a>（2024，美团）<br><a href="https://dl.acm.org/doi/pdf/10.1145/3589335.3648308">ASIF</a>（2024，蚂蚁）<br><a href="https://dl.acm.org/doi/pdf/10.1145/3589335.3648301">SUM</a>（2024，META）<br><a href="https://arxiv.org/pdf/2110.11337">LURM</a>（2023，阿里）<br><a href="https://arxiv.org/pdf/2312.06424">LCN</a>（2024，腾讯）<br><a href="https://arxiv.org/pdf/2402.02842">Trinity</a>（2024，字节）</p><p><strong>参考文章：</strong></p><p><a href="https://zhuanlan.zhihu.com/p/4544607237">抖音/阿里/美团/微信/快手长序列兴趣建模经典方案探索</a><br><a href="https://zhuanlan.zhihu.com/p/699924066">一文梳理近年推荐长序列兴趣建模经典方案</a><br><a href="https://mp.weixin.qq.com/s/RQ1iBs8ftvNR0_xB7X8Erg">阿里妈妈点击率预估中的长期兴趣建模</a><br><a href="https://zhuanlan.zhihu.com/p/433135805">推荐系统——精排篇【3】</a><br><a href="https://zhuanlan.zhihu.com/p/51623339">推荐系统中的注意力机制——阿里深度兴趣网络（DIN）</a><br><a href="https://zhuanlan.zhihu.com/p/50758485">详解阿里之Deep Interest Evolution Network(AAAI 2019)</a><br><a href="https://zhuanlan.zhihu.com/p/78544498">简析阿里 BST: 当用户行为序列邂逅Transformer</a><br><a href="https://zhuanlan.zhihu.com/p/89700141">DSIN（Deep Session Interest Network ）分享</a><br><a href="https://zhuanlan.zhihu.com/p/94432395">阿里妈妈长期用户历史行为建模——MIMN模型详解</a><br><a href="https://zhuanlan.zhihu.com/p/154401513">[SIM论文] 超长兴趣建模视角CTR预估：Search-based Interest Model</a><br><a href="https://zhuanlan.zhihu.com/p/444065581">阿里ETA(End-to-End Target Attention)模型</a><br><a href="https://zhuanlan.zhihu.com/p/525604184">【论文解读|CIKM’2022】基于采样的超长序列建模算法 SDIM</a><br><a href="https://zhuanlan.zhihu.com/p/606047328">快手终身序列建模方案—TWIN</a><br><a href="https://zhuanlan.zhihu.com/p/699725252">精排最终也是样本的艺术</a></p><hr>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h2&gt;&lt;p&gt;在互联网应用的精排模型中，往往在&lt;code&gt;特征工程&lt;/code&gt;、&lt;code&gt;样本构建&lt;/code&gt;、&lt;code&gt;Loss 设计&lt;/code&gt;、&lt;code&gt;模型结构&lt;/code&gt;等方向进行迭代优化。其中，涉及特征与结构的&lt;strong&gt;用户行为序列建模&lt;/strong&gt;是近几年的热点之一。&lt;/p&gt;
&lt;p&gt;序列建模一般有2大方向：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;检索的序列更长；&lt;/li&gt;
&lt;li&gt;建模的更精准。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面梳理近几年的经典序列建模方案，基本也是围绕上述 2 大方向进行不断优化的。&lt;/p&gt;
&lt;h2 id=&quot;1-DIN&quot;&gt;&lt;a href=&quot;#1-DIN&quot; class=&quot;headerlink&quot; title=&quot;1 DIN&quot;&gt;&lt;/a&gt;1 DIN&lt;/h2&gt;&lt;h3 id=&quot;1-1-概述&quot;&gt;&lt;a href=&quot;#1-1-概述&quot; class=&quot;headerlink&quot; title=&quot;1.1 概述&quot;&gt;&lt;/a&gt;1.1 概述&lt;/h3&gt;&lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/1706.06978&quot;&gt;DIN: Deep Interest Network for Click-Through Rate Prediction&lt;/a&gt;&lt;br&gt;来源：2018，阿里&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel0.png&quot; alt=&quot;ubsmodel0&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;思想：为了序列建模的更精准，通过 DIN 的 Attention 结构来替换 Base Model 的 Sum-Pooling 结构。&lt;/strong&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="算法总结" scheme="https://www.xiemingzhao.com/categories/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
    <category term="精排模型" scheme="https://www.xiemingzhao.com/categories/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/%E7%B2%BE%E6%8E%92%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="精排" scheme="https://www.xiemingzhao.com/tags/%E7%B2%BE%E6%8E%92/"/>
    
    <category term="序列建模" scheme="https://www.xiemingzhao.com/tags/%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1/"/>
    
    <category term="DIN" scheme="https://www.xiemingzhao.com/tags/DIN/"/>
    
    <category term="DIEN" scheme="https://www.xiemingzhao.com/tags/DIEN/"/>
    
    <category term="BST" scheme="https://www.xiemingzhao.com/tags/BST/"/>
    
    <category term="DSIN" scheme="https://www.xiemingzhao.com/tags/DSIN/"/>
    
    <category term="MIMN" scheme="https://www.xiemingzhao.com/tags/MIMN/"/>
    
    <category term="SIM" scheme="https://www.xiemingzhao.com/tags/SIM/"/>
    
    <category term="ETA" scheme="https://www.xiemingzhao.com/tags/ETA/"/>
    
    <category term="SDIM" scheme="https://www.xiemingzhao.com/tags/SDIM/"/>
    
    <category term="TWIN" scheme="https://www.xiemingzhao.com/tags/TWIN/"/>
    
    <category term="TWIN-V2" scheme="https://www.xiemingzhao.com/tags/TWIN-V2/"/>
    
  </entry>
  
  <entry>
    <title>Listwise 在重排的应用</title>
    <link href="https://www.xiemingzhao.com/posts/listwisererank.html"/>
    <id>https://www.xiemingzhao.com/posts/listwisererank.html</id>
    <published>2024-09-07T16:00:00.000Z</published>
    <updated>2025-04-05T17:35:50.252Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p><em>本文先假定读者对推荐系统有一定了解，尤其是召回、精排和重排3个部分，如此阅读本文可能更顺畅。</em></p><p><code>LTR</code>（Learning to Rank）即排序学习，在当下的搜广推业务中应用广泛，思路上主要分为四类：</p><ol><li>基于 <code>Point-wise</code>。主要考虑 user、item、以及 context 信息特征，仅关注当前 item 自身的效率，比如 <code>Wide &amp; Deep</code>、<code>ESSM</code> 等。</li><li>基于 <code>Pair-wise</code>。训练时通过损失函数来预估 <code>item pair</code> 之间的相对位置关系，高 Label 排在低 Label 前则为正例，否则，为反例。模型的目标是减少错误 item pair 的个数，不考虑列表信息，模型训练复杂度问我较高，比较典型的如 <code>RankNet</code>。</li><li>基于 <code>List-wise</code>。训练时候对每个 <code>request</code> 下的整个 <code>item list</code> 构建 Loss，拟合最优序列分布。比较典型如 <code>LambdaMART</code>、 <code>DLCM</code>、以及 <code>ListMLE</code> 等。</li><li>基于 <code>Generator-evaluator</code>。一般分为 2 模块，序列的召回和排序。重排召回即按照一定排序算法，生成一系列预期 <code>reward</code> 较高的候选 List，再针对这些上下关系已经固定的 List 进行整页建模预估 <code>reward</code>，选择 <code>top1 list</code> 输出。</li></ol><p>本文将重点聚焦在第 4 种，即基于 <code>Generative-evaluate</code> 的 <code>LTR</code> 模型，因为此类做法有两个特点：</p><ul><li>可真正实现对 List 进行整页建模的；</li><li>可在重排环节实践落地的。</li></ul><span id="more"></span><p>虽然第 3 种一般也称为 <code>List-wise</code>，但实际上更多的是对精排模型训练的 <code>Loss</code> 进行优化，最终 infer 的时候依然是 <code>Point-wise</code>。当然，它也是有一定效果，不过不是本文想讨论的重点。本文主要基于 <code>Generative-evaluate</code> 来讨论对整个 List 的建模和预估，并总结部分笔者的实践经验。</p><h2 id="2-模型原理"><a href="#2-模型原理" class="headerlink" title="2 模型原理"></a>2 模型原理</h2><h3 id="2-1-场景"><a href="#2-1-场景" class="headerlink" title="2.1 场景"></a>2.1 场景</h3><blockquote><p>重排的环节，往往是基于用户 U 和上下文 C，需要从上游（一般是精排）给到候选物品集合 I 中选出 K 个并组成有序 List 展示给前端用户。</p></blockquote><p>故，<strong>重点就在如何挑选并排序成最优列表 O，以达到业务目标上的最大化</strong>，比如点击、下拉曝光、订单转化等。一般传统做法都是基于精排多目标预估融合分，使用 <code>MMR/DPP</code> 等重排算法，再结合业务规则，进行重排。</p><p><strong>精排缺陷</strong>：</p><ul><li>精排 <code>pointwise</code> 建模缺少空间信息的考虑，即当前 item 的上下内容（一般该假设都成立）；</li><li>基于精排分+规则的贪心重排方案往往与全局最优偏差较远；</li><li><code>MMR,DPP</code> 重排也是在目标中给予多样性一些权重，各有优劣。</li></ul><p><strong>重排目标</strong>：</p><ul><li>需要考虑序列空间信息，例如序列窗口特征、或 <code>transformer</code> 结构；</li><li>建模优化目标要做到 List 整体，而不是 Pointwise 式；</li><li>框架能够具备不断发掘更优序列的能力，防止系统模型退化。</li></ul><h3 id="2-2-方案思路"><a href="#2-2-方案思路" class="headerlink" title="2.2 方案思路"></a>2.2 方案思路</h3><p><strong>一个排序结果的好坏，需要 List 固定后，才能结合整个空间排布和上下文信息来真正得进行 Listwise 评估。</strong></p><p>基于上述讨论，我们将重排分为了两个阶段：</p><ul><li><code>重排-召回</code>：使用 MMR 等不同算法，朝着业务目标生成多样的候选序列作为序列召回集；</li><li><code>重排-排序</code>：构建序列评估模型，对候选序列集进行真正的 Listwise 评估，选出最优序列。</li></ul><p>如下图所示，右侧上半部分单点重排便是一个典型的传统重排方案。相对应的，下半部分是结合了序列检索和序列评估的重排模块。</p><p>我们将从候选集合中选择不同的物品排列成不同的序列视作推荐系统的召回，再从这些序列中选出最好的就可以作为推荐系统中的排序，之后便可以直接输出了。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/listwise/listwise0.png" alt="listwise0"></p><h3 id="2-3-重排-召回"><a href="#2-3-重排-召回" class="headerlink" title="2.3 重排-召回"></a>2.3 重排-召回</h3><p>之所以要有该环节，除了需要让序列的空间分布固定以利于 Listwise 评估外，还有<strong>一个重要的假设：传统的贪心重排算法是很难趋向全局最优的。</strong></p><p>所以该环节<strong>重点是如何生成尽可能高效但又具有差异的序列集合</strong>，序列的多和差异是为了增加逼近最优序列的概率。</p><blockquote><p>虽然最优序列我们可能永远做不到，但相比 Base 的单序列（Original List）增加了找到更优 List 的可能，因为一个<strong>兜底的做法就是将 Original List 也加入到候选序列集合中。</strong></p></blockquote><p>实际上也是<strong>多路召回思路</strong>：</p><ul><li>使用不同的序列生成模型来生成序列；</li><li>同一序列生成模型下，生成不同目标的序列。</li></ul><p>这里介绍几个常用的序列生成模型和如何生成不同目标的序列，其实<strong>核心思想就是不同算法生成序列，或同一个算法采用不同的参数来生成</strong>。可以想一下，这些生成算法单独应用时，想必有一些调参的工作，现在好了，我们可以把想要的参数都作为一个序列生成的候选。</p><h4 id="2-3-1-MMR-模型"><a href="#2-3-1-MMR-模型" class="headerlink" title="2.3.1 MMR 模型"></a>2.3.1 MMR 模型</h4><p><code>MMR</code> 全称 <code>Maximal Marginal Relevance</code>，即<code>最大边界相关法</code>，是一种应用较为广泛的重排模型。</p><p>其核心公式如下所示：</p><script type="math/tex; mode=display">MMR=Arg \mathop{max}\limits_{ D_i \in R \backslash S } [\lambda Sim_1(D_i, Q) - (1-\lambda) \mathop{max}\limits_{D_j \in S} Sim_2(D_I,D_j)]</script><p>相信有不少算法工程师在早期也用过此模型，那可以基于此生成多样的序列候选。一般有 3 种方式：</p><ol><li><code>Lambda</code>的超参数变化，可以生成效率和多样性侧重程度不同的序列；</li><li>$Sim_1$ 往往是<code>效率分</code>，其可以有不同的融合方式，比如更侧重点击、或者订单，如此也可以生产不同的序列；</li><li>$Sim_2$ 往往是<code>多样性分</code>，其也可以有不同的构建方式，比如增强品类差异，削弱序列品牌，或者增加多样性计算窗口等。</li></ol><p>而上述三种方式独立使用外，又可以互相叠加，如此一种重排模型便可以生产多种多样的序列。当然数量上也需要控制，主要是出于性能的考虑，所以也需要算法工程师结合业务认识来选取序列生成的配置。</p><h4 id="2-3-2-DPP-模型"><a href="#2-3-2-DPP-模型" class="headerlink" title="2.3.2 DPP 模型"></a>2.3.2 DPP 模型</h4><p>其是通过构建核矩阵后使用行列式求解的一种算法，逻辑细节不是这里的重点，我们主要看其在生成中对序列价值度量方式：</p><script type="math/tex; mode=display">logdet(L_{R_u}) = \sum_{i \in R_u} log(r_{u,i}^2) + logdet(S_{R_u})</script><p>其中，第一项是<code>效率分的变换</code>，第二项是<code>相似性矩阵的行列式变换</code>。</p><p>同样的逻辑，该模型也可以通过修改不同的环节来实现生成不同的序列，如：</p><ul><li>效率分 $r_{u,i}$ 的融合分方式差异化；</li><li>相似性 $S_{ij}$ 的构建方式差异化。</li></ul><h4 id="2-3-3-Beamsearch"><a href="#2-3-3-Beamsearch" class="headerlink" title="2.3.3 Beamsearch"></a>2.3.3 Beamsearch</h4><p><code>Beam Search</code> （束搜索）是一种在有限集合中寻找最优有序子集的搜索算法。可以简述为：</p><ul><li>初始 m 个序列；</li><li>每轮给 m 个序列各选前 k 个最优的且满足业务逻辑的候选 item；</li><li>从 m x k 个候选序列再选择 m 个序列进入下一轮；</li><li>重复上述步骤直到序列长度满足要求。</li></ul><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/listwise/listwise1.png" alt="listwise1"></p><p>可以看到，该方法实际上是基于贪心算法扩大了搜索范围，<strong>涉及调参的环节也很多，都可以作为序列生成的差异化来源</strong>。例如：</p><ul><li>序列个数 m 以及每轮搜索物品数 k；</li><li>选取物品时依赖的分数，效率分、多样性分等；</li><li>业务逻辑（如打散）的强弱；</li><li>首轮种子、中间轮保留序列的评估方式。</li></ul><h4 id="2-3-4-GRN"><a href="#2-3-4-GRN" class="headerlink" title="2.3.4 GRN"></a>2.3.4 GRN</h4><p>既然经常强调序列重排的时候要关注空间信息，那么参考 NLP 的 <code>seq2seq</code> 结构，如果构建一个深度模型对候选序列循环排序，每轮将已排序的作为 input，似乎能够解此问题。例如 <code>GRN</code>（Generative Reranking Network）就是这样一个探索，模型结构如下图。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/listwise/listwise2.png" alt="listwise2"></p><p>可以看到使用了 <code>GRU</code> 来捕捉序列信息的，当然也有用 <code>Transformer</code> 结构的做法。但不管是结构 ，都是一种 <code>往前看</code>的生成模型，即输入包含前面已经排好的序列。不过这类生产模型有一些<strong>缺点：构建和迭代复杂；线上推理时间长；差异化生成复杂度高</strong>。</p><h3 id="2-4-重排-排序"><a href="#2-4-重排-排序" class="headerlink" title="2.4 重排-排序"></a>2.4 重排-排序</h3><p>在<code>重排-召回</code>环节完成之后，我们可以得到了一系列高效且丰富多样的候选 List，一般建议在几十至几百，当然这个主要由场景的价值增益、资源成本等决定。</p><p>这些候选 List 可能<strong>在物料上有差异，但空间排布上的差异更是重点</strong>。所以，对这些 List 的价值预估可以基于精排分设计 <code>Reward</code> 公式，但如果想更精准的预估，就需要进行真正的 Listwise 建模。<br><strong>思路：要模拟用户浏览时的决策，对于一个物料是否有兴趣，除了精排关注的因素外，其上下的空间排版就至关重要，就需要在特征和模型结构上重点弥补这部分信息。</strong></p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/listwise/listwise3.png" alt="listwise3"></p><h4 id="2-4-1-分位置建模"><a href="#2-4-1-分位置建模" class="headerlink" title="2.4.1 分位置建模"></a>2.4.1 分位置建模</h4><p><strong>方法：对已排好序列的每个位置的 item 进行相关目标的预估</strong>。例如 pctr、pcvr 等，目标就是比精排预估得更准。</p><p><strong>优势</strong>：</p><ul><li>逻辑简单，利用空间排版特征、Listwise 模型结构来弥补精排不足；</li><li>方便评估，可直接与精排进行 AUC 等指标对比；</li><li>兼容性高，下游度量序列价值的融合公式往往不用修改。</li></ul><p><strong>缺点</strong>：非整序列建模，依然通过对每个 item 进行预估。</p><h4 id="2-4-2-整序列建模"><a href="#2-4-2-整序列建模" class="headerlink" title="2.4.2 整序列建模"></a>2.4.2 整序列建模</h4><p>与分位置建模对应的就是整序列建模，<strong>即 Label 不再是序列中每个 item 的业务价值，而是整个序列的</strong>。比如，分位置建模 ctr（点击率） 就可以变成对整序列建模 ipv（点击数）。</p><p>但是，在这里有一个<code>不同点</code>：</p><ul><li>分位置的时候，Label 还是可以对齐精排，如 ctr 使用 sigmoid 做二分类任务；</li><li>整序列建模的时候，Label 往往就不是二值的了，不能简单的做二分类，会损失信息。</li></ul><p><strong>解法：分层多分类</strong>。</p><blockquote><p>以 ctr 对应的 ipv 为例，假设序列长度为 n，ctr 的 Label 为 0/1，那么 ipv 的 Label 范围就是 0-n，这 n+1 种可能。</p></blockquote><p>那么就可以在每两个取值之间作为分割点，构建二分类任务，总计 n 个。<br>比如，第 $i (1&lt;= i &lt;= n)$ 个 Label 就是：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Label_i = \begin&#123;cases&#125;</span><br><span class="line">   0 &amp;\text&#123;if &#125; ipv&lt;i \\</span><br><span class="line">   1 &amp;\text&#123;if &#125; ipv&gt;=i</span><br><span class="line">\end&#123;cases&#125;</span><br></pre></td></tr></table></figure></p><p>如果这 n 个分层 Label 的预估项为$p_i,i \in [1,n]$，那么最后 <code>ipv</code> 的预估项应该为：</p><script type="math/tex; mode=display">p^{ipv} = \sum_{i = 1} ^ n (i \cdot p_i \cdot (1 - p_{i+1}))</script><p>其中，最后一项中 $p_{n+1}=1$即可。</p><h4 id="2-4-3-评估目标"><a href="#2-4-3-评估目标" class="headerlink" title="2.4.3 评估目标"></a>2.4.3 评估目标</h4><p>评估目标的设立，依赖前述对业务建模的目标，最后进<strong>多目标之间的融合评估</strong>，这点与精排环节类似。<br>一般可能会涉及，<strong>曝光PV，点击IPV，转化ORD，多样性DIV，客单价AOV，停留时长TIME等</strong>。<br>这里重点介绍2个具有代表性的，点击IPV，多样性DIV。</p><p><strong>点击IPV</strong><br>这是一个比较明确的统计量，用户的点击次数。<br>该指标具有<code>局部累加性</code>：即每个请求贡献的 IPV 是可以直接累积到整体的，不会有衰减。<br>一个建议的构建方式是：</p><script type="math/tex; mode=display">p^{IPV} = \sum_{pos = 1}^n p^{expose} * p^{ctr}</script><p>其中，$p^{expose}$ 是曝光概率，可以是模型预估的，如没有可以去掉或者用离线统计代替。<br>当然，这里针对的是分位置建模，如果是整序列建模，则对应的 $p^{ipv}$ 在上述已介绍。</p><p><strong>多样性DIV</strong><br>相对于上面而言，该指标需要依赖业务来明确具体的统计量是什么。<br>且往往不具有<strong>局部累加性</strong>，比如<code>用户点击的类目数</code>，它是有一个 user 维度去重的概念。<br>这种情况比较复杂，可以尝试下面2种。</p><p>类目信息熵：</p><script type="math/tex; mode=display">cate_{ce} = -\sum_{cid \in C} p_{cid} \cdot (Math.log(p_{cid}) / Math.log(2)) \cdot ctr_{cid}</script><p>其中，$p<em>{cid}$是对应 cid 类目的占比，$ctr</em>{cid}$则是该类目的点击率某统计量，比如均值等。</p><p>未点击类目期望：</p><script type="math/tex; mode=display">newcate_{clk} = -\sum_{cid \in C} isnew \cdot ctr_{cid}</script><p>其中，isnew 是指当前 cid（类目id）是否是 user 未点击的，$ctr<em>{cid}$同样是一个统计量，可以采用反概率：$ctr</em>{cid}=1 - \prod_{i} (1 - p^{ctr}_i)$。</p><blockquote><p><strong>需要强调的是：真正是否有效，与数据的分布、业务的逻辑等有很大关系，需要结合具体业务理解来尝试和优化，以实验反馈为准。</strong></p></blockquote><h2 id="3-实战思考"><a href="#3-实战思考" class="headerlink" title="3 实战思考"></a>3 实战思考</h2><p>在实际落地应用中，实际上有很多的坑需要踩，这里将结合自己的一些实践经历，总结一些相对比较重要的细节，供大家参考和自己复习。</p><h3 id="3-1-序列生成的多样性"><a href="#3-1-序列生成的多样性" class="headerlink" title="3.1 序列生成的多样性"></a>3.1 序列生成的多样性</h3><p>在<code>重排-召回</code>环节我们提到需要生成丰富多样的候选序列，那么这其中一个便是序列的多样性。当然，具体多样性是指什么，需要根据实际业务场景来，比如:</p><ul><li>电商的品牌、品类、店铺等；</li><li>视频的作者、类型、演员等；</li><li>图文的作者、主题、风格等。</li></ul><p>在重排中，往往会有一个多样性的设置，一般是 <strong>hard 规则（兜底） + soft 度量（排序）</strong>。其中 soft 度量多样性的部分，以 MMR 为例，会有2个要素：</p><ul><li><strong>物品间多样性计算方式</strong>。比如 embedding cos、属性 match 等；</li><li><strong>物品选择时多样性统计窗口</strong>。比如 MMR 每轮多样性分数的统计窗口，即往前看多少个物品。</li></ul><p>在<strong>多样性计算方式</strong>上：</p><ul><li>属性 <code>match</code> 简单且往往有效，但维度的选择一定要根据业务场景来，要选用户关注的多样性维度；</li><li>使用 <code>embedding cos</code>，需要注意 embedding 的生成方式，用户关注的多样性维度要体现在其中，反之像基于行为训练的往往会不尽如意；</li><li>一般建议结合 <code>embedding cos</code> + 属性 <code>match</code> 的方式；</li><li>多样性不同属性维度的权重可以作为序列生成的差异性来源。</li></ul><p>在<strong>多样性统计窗口</strong>上：</p><ul><li>离当前 item 越近的权重可以设置越高；</li><li>窗口的大小可以作为序列差异性的来源，往往比较有效。</li></ul><h3 id="3-2-“Beamsearech-X”序列生成框架"><a href="#3-2-“Beamsearech-X”序列生成框架" class="headerlink" title="3.2 “Beamsearech+X”序列生成框架"></a>3.2 “Beamsearech+X”序列生成框架</h3><p><code>Beamsearch</code> 是束搜索，<strong>它是对穷举搜索和贪心搜索这两种偏极端算法的一种折中之策</strong>。即在每一轮选择候选的时候，既不是贪心的只选得分最高的，也不是穷举得遍历每种组合，而是选择 topK 个候选增加寻到更优 List 的可能性，具体算法细节这里不再赘述。</p><blockquote><p>这里笔者重点想介绍的是基于其构建的一种拓展方案，即“Beamsearech+X”序列生成框架。怎么理解呢？</p></blockquote><p>前面提到 <code>Beamsearch</code> 主要是在每轮选取的时候拓展了候选，增加靠近全局最优的可能性。但，需要注意的是，在每一轮选取的时候依然需要有一个 <code>Function</code> 来度量候选加入后对 List 的影响好坏。而 Function 的构建实际上就退化成了单 List 的排序，这部分有一些成熟的候选：</p><ul><li>效率分贪心；</li><li>MMR；</li><li>DPP等。</li></ul><p>于是，上面的候选算法都可以作为 <code>Beamsearch+X</code> 中的 <code>X</code> 项。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/listwise/listwise4.png" alt="listwise4"></p><p>我们以 <code>MMR</code> 为例，如上图所示是一种 <code>Beamsearch+MMR</code> 实现方案：</p><ol><li><code>种子选取</code>：根据物品分 Score 选取 K 个作为 List 种子；</li><li><code>生成候选</code>：中间每轮，为每个种子 List 选取 M 个候选，例如使用 MMR；</li><li><code>序列子评估</code>：从每个种子的 M 个候选 List 中选择一个整体价值最高的保留；</li><li><code>完成序列</code>：重复上述直到序列长度满足业务要求，将序列集合作为该路召回。</li></ol><p>如此，其他的单序列生成都可以融入到 <code>Beamsearch+X</code> 体系中，拓展生成能力。那么 <code>Beamsearch+X</code> 的召回丰富度，除了可以继承 <code>X</code> 生成算法的丰富度调整方式外，<code>Beamsearch</code> 本身的一些特征也可以作为调整项，比如：</p><ul><li>种子物品的选取方式，如效率 topK、多样性 topK等；</li><li>种子个数 K、每轮候选 M；</li><li>子评估的构建方式，如统计、LR等；</li><li>子评估的排序方式，比如子序列候选混合或独立。</li></ul><h3 id="3-3-候选评估模型"><a href="#3-3-候选评估模型" class="headerlink" title="3.3 候选评估模型"></a>3.3 候选评估模型</h3><p>在序列生成环节，前述提到的 <code>MMR</code>、<code>Beamsearch+X</code>、<code>DPP</code>等均是动态规划类或搜索类模型，并不是一个真正训练得到的排序模型。<br>当然，<code>GRN</code> 类的 <code>Seq2Seq</code> 模型属于训练得到的深度模型，但往往难以覆盖广泛的序列召回需要，并且性能容易陷入瓶颈。</p><p>在序列生成每轮检索候选时，也是可以通过构建一个简单的模型来代替的，如下图所示：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/listwise/listwise5.png" alt="listwise5"></p><p>生成的时候，没有办法同时考虑上文和下文，往往只有上文，即已经排序好的 List 部分。故，就可以将<strong>前序物品序列</strong>作为主要的 input 信息，然后评估候选物品加入后的效率。如此构建一个有监督模型。但往往要注意性能，因为在生成的时候，要多轮调用，所以<strong>一般建议使用简单的 LR 模型即可</strong>。</p><h3 id="3-4-Listwise-特征设计"><a href="#3-4-Listwise-特征设计" class="headerlink" title="3.4 Listwise 特征设计"></a>3.4 Listwise 特征设计</h3><p>前面介绍过重排与精排的差异及其目标，所以在这里的特征设计上，我们要结合上述2点来考虑：</p><ul><li><code>上游特征</code>的使用:<ul><li>例如精排分，召回渠道，供给分布等；</li><li>此处非常重要，<strong>一方面是不浪费推荐系统上游的产出，另一方面也是重排环节效果&gt;=精排的保障</strong>。</li></ul></li><li><code>空间分布</code>的特征:<ul><li>例如窗口类目分布等，可以是前置窗口，也可是后置，当然前后横跨的窗口也是需要的；</li><li>这部分是重排模型的<strong>核心差异和优势之处，对这部分信息的捕捉好决定了重排效果增益空间</strong>。</li></ul></li></ul><blockquote><p>这里有一个风险：如果使用不当，容易对上游精排分数等形成强依赖，使得系统上下游耦合过重。</p></blockquote><p>如，过度依赖精排分，如果其因迭代带来分数分布明显变化，容易直接带崩重排，从而让重排成为精排迭代的一个限制。</p><p><strong>分析原因</strong>：带崩往往是由于分数分布变化，出现一些重排没见过的特征值。<br><strong>解法思路</strong>：分布可以变化，但新特征值/量纲跨越要尽量少，故可对上游特征进行脱敏。</p><blockquote><p>比如将原始分，归一化，分段，取分位点等等，以此尽量捕捉有效信息的同时，避免因分值变化而带来新特征值爆发、量纲跨越等问题。</p></blockquote><p><strong>参考文章</strong>：<br><a href="https://www.zhihu.com/question/20569832/answer/2292533580">LTR （learning to Rank） 在互联网中目前发展如何？</a><br><a href="https://www.infoq.cn/article/a1tj74y7V2EKFikKYcwv?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">KDD23|RankFormer:用Transformer去做LTR任务</a><br><a href="https://zhuanlan.zhihu.com/p/518821008">ListNet和ListMLE</a><br><a href="https://zhuanlan.zhihu.com/p/685323123">KDD’23 | 阿里, 排序和校准联合建模: 让listwise模型也能用于CTR预估</a><br><a href="https://mp.weixin.qq.com/s/YoQNDdsRE6LZWJXrRKubLw">Generator-Evaluator重排模型在淘宝流式场景的实践</a><br><a href="https://mp.weixin.qq.com/s/mYLFqBEM79hn9YSFGjVu3w">序列检索系统在淘宝首页信息流重排中的实践</a><br><a href="https://www.6aiq.com/article/1644883063745">渠江涛：重排序在快手短视频推荐系统中的演进</a><br><a href="https://juejin.cn/post/7210310775276519484">算法实践总结V3：重排在快手短视频</a><br><a href="https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E5%90%8C%E5%9F%8E%E5%A4%9A%E6%A0%B7%E6%80%A7%E7%AE%97%E6%B3%95%E5%9C%A8%E9%83%A8%E8%90%BD%E7%9A%84%E5%AE%9E%E8%B7%B5%E5%92%8C%E6%80%9D%E8%80%83/">同城多样性算法在部落的实践和思考</a><br><a href="https://www.biaodianfu.com/learning-to-ranking.html">排序优化算法Learning to Ranking</a><br><a href="https://blog.csdn.net/qq_36478718/article/details/122598406">Learning to Rank : ListNet与ListMLE</a></p><hr>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-引言&quot;&gt;&lt;a href=&quot;#1-引言&quot; class=&quot;headerlink&quot; title=&quot;1 引言&quot;&gt;&lt;/a&gt;1 引言&lt;/h2&gt;&lt;p&gt;&lt;em&gt;本文先假定读者对推荐系统有一定了解，尤其是召回、精排和重排3个部分，如此阅读本文可能更顺畅。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;LTR&lt;/code&gt;（Learning to Rank）即排序学习，在当下的搜广推业务中应用广泛，思路上主要分为四类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;基于 &lt;code&gt;Point-wise&lt;/code&gt;。主要考虑 user、item、以及 context 信息特征，仅关注当前 item 自身的效率，比如 &lt;code&gt;Wide &amp;amp; Deep&lt;/code&gt;、&lt;code&gt;ESSM&lt;/code&gt; 等。&lt;/li&gt;
&lt;li&gt;基于 &lt;code&gt;Pair-wise&lt;/code&gt;。训练时通过损失函数来预估 &lt;code&gt;item pair&lt;/code&gt; 之间的相对位置关系，高 Label 排在低 Label 前则为正例，否则，为反例。模型的目标是减少错误 item pair 的个数，不考虑列表信息，模型训练复杂度问我较高，比较典型的如 &lt;code&gt;RankNet&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;基于 &lt;code&gt;List-wise&lt;/code&gt;。训练时候对每个 &lt;code&gt;request&lt;/code&gt; 下的整个 &lt;code&gt;item list&lt;/code&gt; 构建 Loss，拟合最优序列分布。比较典型如 &lt;code&gt;LambdaMART&lt;/code&gt;、 &lt;code&gt;DLCM&lt;/code&gt;、以及 &lt;code&gt;ListMLE&lt;/code&gt; 等。&lt;/li&gt;
&lt;li&gt;基于 &lt;code&gt;Generator-evaluator&lt;/code&gt;。一般分为 2 模块，序列的召回和排序。重排召回即按照一定排序算法，生成一系列预期 &lt;code&gt;reward&lt;/code&gt; 较高的候选 List，再针对这些上下关系已经固定的 List 进行整页建模预估 &lt;code&gt;reward&lt;/code&gt;，选择 &lt;code&gt;top1 list&lt;/code&gt; 输出。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;本文将重点聚焦在第 4 种，即基于 &lt;code&gt;Generative-evaluate&lt;/code&gt; 的 &lt;code&gt;LTR&lt;/code&gt; 模型，因为此类做法有两个特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可真正实现对 List 进行整页建模的；&lt;/li&gt;
&lt;li&gt;可在重排环节实践落地的。&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="算法总结" scheme="https://www.xiemingzhao.com/categories/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
    <category term="重排模型" scheme="https://www.xiemingzhao.com/categories/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/%E9%87%8D%E6%8E%92%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="Listwise" scheme="https://www.xiemingzhao.com/tags/Listwise/"/>
    
    <category term="重排" scheme="https://www.xiemingzhao.com/tags/%E9%87%8D%E6%8E%92/"/>
    
    <category term="MMR" scheme="https://www.xiemingzhao.com/tags/MMR/"/>
    
    <category term="DPP" scheme="https://www.xiemingzhao.com/tags/DPP/"/>
    
    <category term="BeamSearch" scheme="https://www.xiemingzhao.com/tags/BeamSearch/"/>
    
    <category term="GRN" scheme="https://www.xiemingzhao.com/tags/GRN/"/>
    
  </entry>
  
  <entry>
    <title>Transformer 解析</title>
    <link href="https://www.xiemingzhao.com/posts/transformer.html"/>
    <id>https://www.xiemingzhao.com/posts/transformer.html</id>
    <published>2022-07-23T16:00:00.000Z</published>
    <updated>2025-04-04T17:59:03.441Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1 背景"></a>1 背景</h2><p>算法工程师在成长道路上基本绕不开深度学习，而 <code>Transformer</code> 模型更是其中的经典，它在2017年的<a href="https://arxiv.org/abs/1706.03762">《Attention is All You Need》</a>论文中被提出，直接掀起了 <code>Attention</code> 机制在深度模型中的广泛应用潮流。</p><p>在该模型中有许多奇妙的想法启发了诸多算法工程师的学习创造，为了让自己回顾复习更加方便，亦或让在学习的读者更轻松地理解，便写了这篇文章。形式上，在参考诸多优秀文章和博客后，这里还是采用结构与代码并行阐述的模式。</p><span id="more"></span><h2 id="2-Transformer-概述"><a href="#2-Transformer-概述" class="headerlink" title="2 Transformer 概述"></a>2 Transformer 概述</h2><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/transformer0.png" alt="transformer0"></p><p>如上图所示的是论文中对 Transformer 模型的结构概述，自己初学时对此图有些难以理解。回过头来看，实际上作者默认读者是一个对深度学习较为熟悉的，所以隐去了部分细节信息，仅将最核心的建模思想绘制了出来。</p><p>在这里，我想再降低一下门槛，提高复习和阅读的舒适度。需要指出的是，论文提出该模型是基于<strong>nlp 中翻译任务</strong>的，所以是一个 <code>seq2seq</code> 的结构，如下图所示。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/transformer1.png" alt="transformer1"></p><p>图中表明了输入的句子经过多个编码器 <code>encoder</code> 后再经过多个解码器 <code>decoder</code> 得到最后的预估结果。那么重点就在于以下四个部分：</p><ul><li>input</li><li>encoder</li><li>decoder</li><li>output</li></ul><p>结合上述的模型图，将这四个部分详细展示的话可以表示成如下结构。实际上此图与论文中的结构图如出一辙，但是相对更易于理解一些。下面将基于此结构，结合 <a href="https://github.com/Kyubyong/transformer.git">Kyubyong</a> 的 tf 实现代码，详细分析每个模块。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/transformer2.png" alt="transformer2"></p><h2 id="3-模块解析"><a href="#3-模块解析" class="headerlink" title="3 模块解析"></a>3 模块解析</h2><h3 id="3-1-Input"><a href="#3-1-Input" class="headerlink" title="3.1 Input"></a>3.1 Input</h3><p>模型核心的入口便是 <code>train</code> 方法模块，如下所示，在 <code>input</code> 有的情况下，前馈网络是比较清晰简洁的，只有 <code>encode</code> 和 <code>decode</code>，与模型结构图一致。其余的代码便是主要用来构建训练 <code>loss</code> 和优化器 <code>opt</code> 的。需要注意的是 <code>encode</code> 模块并不完全等价于模型结构图中的 <code>encoder</code>，后者是前者中的一部分。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, xs, ys</span>):</span><br><span class="line">   <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">   Returns</span></span><br><span class="line"><span class="string">   loss: scalar.</span></span><br><span class="line"><span class="string">   train_op: training operation</span></span><br><span class="line"><span class="string">   global_step: scalar.</span></span><br><span class="line"><span class="string">   summaries: training summary node</span></span><br><span class="line"><span class="string">   &#x27;&#x27;&#x27;</span></span><br><span class="line">   <span class="comment"># forward 前向</span></span><br><span class="line">   memory, sents1, src_masks = <span class="variable language_">self</span>.encode(xs)    <span class="comment"># 编码</span></span><br><span class="line">   logits, preds, y, sents2 = <span class="variable language_">self</span>.decode(ys, memory, src_masks)    <span class="comment"># 解码</span></span><br><span class="line"> </span><br><span class="line">   <span class="comment"># train scheme</span></span><br><span class="line">   y_ = label_smoothing(tf.one_hot(y, depth=<span class="variable language_">self</span>.hp.vocab_size))    <span class="comment"># 平滑标签</span></span><br><span class="line">   ce = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_)    <span class="comment"># softmax分类</span></span><br><span class="line">   nonpadding = tf.to_float(tf.not_equal(y, <span class="variable language_">self</span>.token2idx[<span class="string">&quot;&lt;pad&gt;&quot;</span>]))  <span class="comment"># 0: &lt;pad&gt;</span></span><br><span class="line">   loss = tf.reduce_sum(ce * nonpadding) / (tf.reduce_sum(nonpadding) + <span class="number">1e-7</span>)</span><br><span class="line"></span><br><span class="line">   global_step = tf.train.get_or_create_global_step()</span><br><span class="line">   lr = noam_scheme(<span class="variable language_">self</span>.hp.lr, global_step, <span class="variable language_">self</span>.hp.warmup_steps)</span><br><span class="line">   optimizer = tf.train.AdamOptimizer(lr)</span><br><span class="line">   train_op = optimizer.minimize(loss, global_step=global_step)</span><br><span class="line"></span><br><span class="line">   tf.summary.scalar(<span class="string">&#x27;lr&#x27;</span>, lr)</span><br><span class="line">   tf.summary.scalar(<span class="string">&quot;loss&quot;</span>, loss)</span><br><span class="line">   tf.summary.scalar(<span class="string">&quot;global_step&quot;</span>, global_step)</span><br><span class="line"></span><br><span class="line">   summaries = tf.summary.merge_all()</span><br><span class="line"></span><br><span class="line">   <span class="keyword">return</span> loss, train_op, global_step, summaries</span><br></pre></td></tr></table></figure></p><p>进一步的，我们深入 <code>encode</code> 去看 <code>input</code> 在进入 <code>encoder</code> 前的一些预处理，如下代码所示。可以看到输入 <code>xs</code> 实际上包含三个部分：</p><ul><li><code>x</code>: 被补全的句子映射的 tokenid 序列</li><li><code>seqlens</code>: 句子的长度</li><li><code>sents</code>: 原始句子</li></ul><p>首先根据 <code>tokenid</code> 是否为0构建了 <code>src_masks</code> 源句掩码，接着将输入 <code>x</code> 进行词向量嵌入。</p><blockquote><p>这里需要注意，code 中作者将词向量进行了缩放，系数是 $d_{model}^{0.5}$。而这一部分原始论文中是没有提及的。</p></blockquote><p>之后，还进行了两步处理：</p><ul><li>加上 positional_encoding：为了融入位置信息；</li><li>接一层 dropout：为了防止过拟合。</li></ul><p>到此，输入的预处理便结束了，之后就如模型结构图所示，开始进入多个 <code>encoder</code> 进行编码了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, xs, training=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    memory: encoder outputs. (N, T1, d_model)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;encoder&quot;</span>, reuse=tf.AUTO_REUSE):</span><br><span class="line">        x, seqlens, sents1 = xs    <span class="comment"># 被补全的句子，句子长度，原句</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># src_masks 源句掩码</span></span><br><span class="line">        src_masks = tf.math.equal(x, <span class="number">0</span>) <span class="comment"># (N, T1) 掩码，标记补全位置</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># embedding 嵌入</span></span><br><span class="line">        enc = tf.nn.embedding_lookup(<span class="variable language_">self</span>.embeddings, x) <span class="comment"># (N, T1, d_model)    # 词嵌入 Input Embedding</span></span><br><span class="line">        enc *= <span class="variable language_">self</span>.hp.d_model**<span class="number">0.5</span> <span class="comment"># scale 对enc缩放，但是原论文中没有发现相关内容</span></span><br><span class="line"> </span><br><span class="line">        enc += positional_encoding(enc, <span class="variable language_">self</span>.hp.maxlen1)    <span class="comment"># 位置嵌入</span></span><br><span class="line">        enc = tf.layers.dropout(enc, <span class="variable language_">self</span>.hp.dropout_rate, training=training)     <span class="comment">#Dropout 防止过拟合</span></span><br><span class="line">        <span class="comment"># 截止现在输入已被嵌入完毕</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment">## Blocks Encoder 块</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.hp.num_blocks):    <span class="comment"># 设定的Encoder块</span></span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;num_blocks_&#123;&#125;&quot;</span>.<span class="built_in">format</span>(i), reuse=tf.AUTO_REUSE):    <span class="comment">#当前是第几个Encoder块</span></span><br><span class="line">                <span class="comment"># self-attention    多头注意力机制</span></span><br><span class="line">                enc = multihead_attention(queries=enc,</span><br><span class="line">                                          keys=enc,</span><br><span class="line">                                          values=enc,</span><br><span class="line">                                          key_masks=src_masks,</span><br><span class="line">                                          num_heads=<span class="variable language_">self</span>.hp.num_heads,</span><br><span class="line">                                          dropout_rate=<span class="variable language_">self</span>.hp.dropout_rate,</span><br><span class="line">                                          training=training,</span><br><span class="line">                                          causality=<span class="literal">False</span>)    <span class="comment"># 多头注意力机制</span></span><br><span class="line">                <span class="comment"># feed forward    前向传播</span></span><br><span class="line">                enc = ff(enc, num_units=[<span class="variable language_">self</span>.hp.d_ff, <span class="variable language_">self</span>.hp.d_model])</span><br><span class="line">    memory = enc <span class="comment"># 记住当前进度</span></span><br><span class="line">    <span class="keyword">return</span> memory, sents1, src_masks</span><br></pre></td></tr></table></figure><h3 id="3-2-Positional-encoding"><a href="#3-2-Positional-encoding" class="headerlink" title="3.2 Positional encoding"></a>3.2 Positional encoding</h3><p>前面提到为了融入位置信息，引入了 <code>positional_encoding</code> 的模块。而位置编码的需求：</p><ol><li>需要体现同一个单词在不同位置的区别；</li><li>需要体现一定的先后次序关系；</li><li>并且在一定范围内的编码差异不应该依赖于文本长度，具有一定不变性。</li></ol><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/transformer3.png" alt="transformer3"></p><p>官方的做法是：</p><script type="math/tex; mode=display">PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}})</script><script type="math/tex; mode=display">PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})</script><p>其中：</p><ul><li><code>pos</code> 是指词在句中的位置;</li><li><code>i</code> 是指位置嵌入 emb 的位置序号。</li></ul><p>整个模块的代码如下所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">positional_encoding</span>(<span class="params">inputs,</span></span><br><span class="line"><span class="params">                        maxlen,</span></span><br><span class="line"><span class="params">                        masking=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                        scope=<span class="string">&quot;positional_encoding&quot;</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Sinusoidal Positional_Encoding. See 3.5</span></span><br><span class="line"><span class="string">    inputs: 3d tensor. (N, T, E)</span></span><br><span class="line"><span class="string">    maxlen: scalar. Must be &gt;= T</span></span><br><span class="line"><span class="string">    masking: Boolean. If True, padding positions are set to zeros.</span></span><br><span class="line"><span class="string">    scope: Optional scope for `variable_scope`.</span></span><br><span class="line"><span class="string">    returns</span></span><br><span class="line"><span class="string">    3d tensor that has the same shape as inputs.</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"> </span><br><span class="line">    E = inputs.get_shape().as_list()[-<span class="number">1</span>] <span class="comment"># static 获取此向量维度 d_model</span></span><br><span class="line">    N, T = tf.shape(inputs)[<span class="number">0</span>], tf.shape(inputs)[<span class="number">1</span>] <span class="comment"># dynamic N为batch_size，T为最长句子长度</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope, reuse=tf.AUTO_REUSE):</span><br><span class="line">        <span class="comment"># position indices    位置索引</span></span><br><span class="line">        position_ind = tf.tile(tf.expand_dims(tf.<span class="built_in">range</span>(T), <span class="number">0</span>), [N, <span class="number">1</span>]) <span class="comment"># (N, T) 对张量进行扩展 1,T → N,T</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># First part of the PE function: sin and cos argument 位置嵌入方法</span></span><br><span class="line">        position_enc = np.array([</span><br><span class="line">            [pos / np.power(<span class="number">10000</span>, (i-i%<span class="number">2</span>)/E) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(E)]</span><br><span class="line">            <span class="keyword">for</span> pos <span class="keyword">in</span> <span class="built_in">range</span>(maxlen)])</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Second part, apply the cosine to even columns and sin to odds.  不同位置 使用sin和cos方法</span></span><br><span class="line">        position_enc[:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(position_enc[:, <span class="number">0</span>::<span class="number">2</span>])  <span class="comment"># dim 2i</span></span><br><span class="line">        position_enc[:, <span class="number">1</span>::<span class="number">2</span>] = np.cos(position_enc[:, <span class="number">1</span>::<span class="number">2</span>])  <span class="comment"># dim 2i+1</span></span><br><span class="line">        position_enc = tf.convert_to_tensor(position_enc, tf.float32) <span class="comment"># (maxlen, E)</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># lookup</span></span><br><span class="line">        outputs = tf.nn.embedding_lookup(position_enc, position_ind)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># masks</span></span><br><span class="line">        <span class="keyword">if</span> masking:    <span class="comment"># 是否需要掩码</span></span><br><span class="line">            outputs = tf.where(tf.equal(inputs, <span class="number">0</span>), inputs, outputs) </span><br><span class="line">        <span class="comment"># inputs中值为0的地方（为True的地方）保持值不变，其余元素替换为outputs结果。因为0的地方就是掩码的地方，不需要有所谓的位置嵌入。</span></span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> tf.to_float(outputs)</span><br></pre></td></tr></table></figure><p>论文中对该嵌入方法生成的 <code>embdding</code> 进行了可视化，如下图所示：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/transformer4.png" alt="transformer4"></p><p>为何如此设计呢？从公式看，<code>sin &amp; cos</code> 的交替使用只是为了使编码更丰富，在哪些维度上使用 sin，哪些使用 cos，不是很重要，都是模型可以调整适应的。而论文给的解释是：</p><blockquote><p>对任意确定的偏移 k，$PE<em>{pos+k}$ 可以表示为 $PE</em>{pos}$ 的函数。</p></blockquote><p>推导的结果是:</p><script type="math/tex; mode=display">PE(pos + k, 2i) = PE(pos, 2i) * constant^k_{2i + 1} + constant^k_i * PE(pos, 2i + 1)</script><p>需要指出的是：</p><ol><li>这个函数形式很可能是基于经验得到的，并且应该有不少可以替代的方法；</li><li>谷歌后期的作品 <code>BERT</code> 已经换用位置嵌入(positional embedding)来学习了。</li></ol><h3 id="3-3-Multi-Head-Attention"><a href="#3-3-Multi-Head-Attention" class="headerlink" title="3.3 Multi Head Attention"></a>3.3 Multi Head Attention</h3><h4 id="3-3-1-机制概述"><a href="#3-3-1-机制概述" class="headerlink" title="3.3.1 机制概述"></a>3.3.1 机制概述</h4><p>多头注意力机制是 <code>Transformer</code> 的核心，且这里的 <code>Attention</code> 被称为 <code>self-attention</code>，是为了区别另一种 <code>target-attention</code>。名字不是特别重要，重点是理解逻辑和实现。这里先抛出对此的看法：<strong>模型在理解句中某个词的时候，需要结合上下文，而 <code>Multihead Attention</code> 便是用来从不同角度度量句中单个词与上下文各个词之间关联性的机制。</strong></p><p>文字可能没有图片直观，这里以一个可视化的例子来呈现：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/transformer5.png" alt="transformer5"></p><p>如上图所示，当模型想要理解句子 “The animal didn’t cross the street because it was too tired” 中 it 含义的时候，attention 机制可以计算上下文中各个词与它的相关性，图中颜色的深浅便代表相关性大小。</p><p>所以，<code>Multihead Attention</code> 模块的任务就是<strong>将原本独立的词向量（维度d_k）经过一系列的计算过程，最终映射到一组新的向量(维度d_v)，新向量包含了上下文、位置等有助于词义理解的信息</strong>。</p><h4 id="3-3-2-Q、K、V变换"><a href="#3-3-2-Q、K、V变换" class="headerlink" title="3.3.2 Q、K、V变换"></a>3.3.2 Q、K、V变换</h4><p>模型 <code>Multihead Attention</code> 模块的输入是 embedding 后的一串词向量，而 Attention 机制中原始是对 Query 计算与 Key 的 Weight 后，叠加 Value 计算加权和，所以需要 $Query,Key,Value$ 三个矩阵。</p><p>作者便基于 Input 矩阵，通过矩阵变换来生成 Q、K、V，如下图所示，<strong>由于 Query 和 Key、Value 来源于同一个Input</strong>，故这种机制也称为 <code>self-attention</code>。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/transformer6.png" alt="transformer6"></p><p>如上图所示，假设 Input 是“Thinking Matchines”句子，只有2个词向量。假设每个词映射为图中的 $1 \times 4$ 的词向量，当我们使用图中所示的3个变换矩阵 $W^Q,W^K,W^V$ 来对 Input 进行变换 (即 $W \times X$) 后，便可以得到变换后的$Q,K,V$矩阵，即每个词向量转换成图中维度为 $1 \times 3$ 的 $q,k,v$。</p><p><strong>注意：这些新向量的维度比输入词向量的维度要小（原文 nlp 任务是 512–&gt;64，图中 case 是4-&gt;3），并不是必须要小的，是为了让多头 attention 的计算更稳定。</strong></p><p>对应的 code 如下所示，其中有一个 <code>Split and concat</code> 模块，这一块本节未提及，是模型中 <code>multi-head</code> 机制的体现，在后文将会详细介绍。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">multihead_attention</span>(<span class="params">queries, keys, values, key_masks,</span></span><br><span class="line"><span class="params">                        num_heads=<span class="number">8</span>, </span></span><br><span class="line"><span class="params">                        dropout_rate=<span class="number">0</span>,</span></span><br><span class="line"><span class="params">                        training=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                        causality=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                        scope=<span class="string">&quot;multihead_attention&quot;</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Applies multihead attention. See 3.2.2</span></span><br><span class="line"><span class="string">    queries: A 3d tensor with shape of [N, T_q, d_model].</span></span><br><span class="line"><span class="string">    keys: A 3d tensor with shape of [N, T_k, d_model].</span></span><br><span class="line"><span class="string">    values: A 3d tensor with shape of [N, T_k, d_model].</span></span><br><span class="line"><span class="string">    key_masks: A 2d tensor with shape of [N, key_seqlen]</span></span><br><span class="line"><span class="string">    num_heads: An int. Number of heads.</span></span><br><span class="line"><span class="string">    dropout_rate: A floating point number.</span></span><br><span class="line"><span class="string">    training: Boolean. Controller of mechanism for dropout.</span></span><br><span class="line"><span class="string">    causality: Boolean. If true, units that reference the future are masked.</span></span><br><span class="line"><span class="string">    scope: Optional scope for `variable_scope`.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">      A 3d tensor with shape of (N, T_q, C)  </span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    d_model = queries.get_shape().as_list()[-<span class="number">1</span>]    <span class="comment"># 获取词向量长度</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope, reuse=tf.AUTO_REUSE):</span><br><span class="line">        <span class="comment"># Linear projections    # 通过权重矩阵得出Q,K,V矩阵</span></span><br><span class="line">        Q = tf.layers.dense(queries, d_model, use_bias=<span class="literal">True</span>) <span class="comment"># (N, T_q, d_model)</span></span><br><span class="line">        K = tf.layers.dense(keys, d_model, use_bias=<span class="literal">True</span>) <span class="comment"># (N, T_k, d_model)</span></span><br><span class="line">        V = tf.layers.dense(values, d_model, use_bias=<span class="literal">True</span>) <span class="comment"># (N, T_k, d_model)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Split and concat    针对最后一个维度划分为多头，词向量长度512 → 每个头64</span></span><br><span class="line">        Q_ = tf.concat(tf.split(Q, num_heads, axis=<span class="number">2</span>), axis=<span class="number">0</span>) <span class="comment"># (h*N, T_q, d_model/h)</span></span><br><span class="line">        K_ = tf.concat(tf.split(K, num_heads, axis=<span class="number">2</span>), axis=<span class="number">0</span>) <span class="comment"># (h*N, T_k, d_model/h)</span></span><br><span class="line">        V_ = tf.concat(tf.split(V, num_heads, axis=<span class="number">2</span>), axis=<span class="number">0</span>) <span class="comment"># (h*N, T_k, d_model/h)</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Attention 计算自注意力</span></span><br><span class="line">        outputs = scaled_dot_product_attention(Q_, K_, V_, key_masks, causality, dropout_rate, training)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Restore shape 合并多头</span></span><br><span class="line">        outputs = tf.concat(tf.split(outputs, num_heads, axis=<span class="number">0</span>), axis=<span class="number">2</span> ) <span class="comment"># (N, T_q, d_model)</span></span><br><span class="line">              </span><br><span class="line">        <span class="comment"># Residual connection 残差链接</span></span><br><span class="line">        outputs += queries </span><br><span class="line">              </span><br><span class="line">        <span class="comment"># Layer Normalize </span></span><br><span class="line">        outputs = ln(outputs)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure><h4 id="3-3-3-Attention"><a href="#3-3-3-Attention" class="headerlink" title="3.3.3 Attention"></a>3.3.3 Attention</h4><p>在文中的全称是 <code>scaled_dot_product_attention</code>（缩放的点积注意力机制），这也是 <code>Transformer</code> 的计算核心。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/transformer7.png" alt="transformer7"></p><p>如上图所示，是 Attention 机制的一个计算过程示例。输入有2个词向量($x_1,x_2$)，分别映射成了对应的$q,k,v$向量。</p><p>作为 <code>scaled_dot_product_attention</code> 的输入后需要经过如下几步：</p><ol><li>计算每组 q, k 的点积，即图中的 Score；</li><li>对点积 Score 进行缩放（scaled），即图中的“除以8“，8由$\sqrt{d_k}$计算得到；</li><li>基于每个词维度，对其下所有的 scaled Score 计算 Softmax 得到对应的权重 Weight；</li><li>用3中的权重对所有向量 $v_i$ 做加权求和，得到最终的 Sum 向量作为 output。</li></ol><p>这里需要注意，在第 2 步中对点积的结果 Score 做了 scaled 的原因：</p><blockquote><p>作者提到，这样梯度会更稳定。然后加上softmax操作，归一化分值使得全为正数且加和为1。</p></blockquote><p>后半部分比较好理解，前半部分的原因可从如下角度考虑：假设 Q 和 K 的均值为0，方差为1，它们的矩阵乘积将有均值为0，方差为 $d_k$。因此，$d_k$ 的平方根被用于缩放（而非其他数值）后，因为，<strong>乘积的结果就变成了 0 均值和单位方差，这样会获得一个更平缓的 softmax，也即梯度更稳定不容易出现梯度消失</strong>。</p><p>以上是单个词向量在 Attention 中的计算过程，自然的，多个词向量可以叠加后进行矩阵运算，如下所示。实际上，就是将原来的单词向量$x_i$ ($1 \times d_k$)　堆叠到一起 $X$($N \times d_k$) 进行计算。</p><p>输入 $X$ 到 $Q,K,V$ 的矩阵变换过程：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/transformer8.png" alt="transformer8"></p><p>基于$Q,K,V$的 Attention 计算过程：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/transformer9.png" alt="transformer9"></p><h4 id="3-3-4-Multi-head"><a href="#3-3-4-Multi-head" class="headerlink" title="3.3.4 Multi-head"></a>3.3.4 Multi-head</h4><p>截止上述基本上就是 <code>self-attention</code> 的计算流程了，那么 <code>Multi Attention</code> 中的 <code>multi</code> 就体现在本节的 <code>Multi-head</code> 环节。</p><p>我们先看做法：</p><blockquote><p>使用多组 $W^Q,W^K,W^V$ 矩阵进行变换后进行 Attention 机制的计算，如此便可以得到多组输出向量 $Z$，整个流程如下所示。</p></blockquote><p>基于多组 $W^Q,W^K,W^V$ 矩阵映射成多组 $Q,K,V$：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/transformer10.png" alt="transformer10"></p><p>经过 Attention 多组 $Q,K,V$ 得到多个输出矩阵$Z$：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/transformer11.png" alt="transformer11"></p><p>多个输出矩阵$Z$进行 concat 后再线性变换成等嵌入维度($d_k$)的最终输出矩阵$Z$：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/transformer12.png" alt="transformer12"></p><h4 id="3-3-5-Attention-机制总结"><a href="#3-3-5-Attention-机制总结" class="headerlink" title="3.3.5 Attention 机制总结"></a>3.3.5 Attention 机制总结</h4><p>这里直接看整体流程图：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/transformer13.png" alt="transformer13"></p><p>如上图所示，是一个从左往右的计算流程：</p><ol><li>输入的句子，这里 case 是”Thinking Machines”;</li><li>词嵌入，将词嵌入为 embedding， 其中 R 表示非第 0 个 encoder 的 input 不需要词嵌入，而是上一个 encoder 的 ouput；</li><li>生成多组变换权重矩阵；</li><li>基于多组权重矩阵（多头）变换映射，得到多组 Q,K,V；</li><li>多组 Q,K,V 经过 Attention 后得到多个输出 z，将他们 concat 后进行线性变换得到最终的输出矩阵 Z。</li></ol><blockquote><p>至于为什么要用 Multi Head Attention ？作者提到：</p></blockquote><ol><li>多头机制扩展了模型集中于不同位置的能力。</li><li>多头机制赋予 attention 多种子表达方式。</li></ol><p>该模块的 code 如下所示，其中还有 <code>mask</code> 和 <code>dropout</code> 模块，前者是为了去除输入中 <code>padding</code> 的影响，后者则是为了提高模型稳健性。后者不过多介绍，mask 的 code 也附在了下方。</p><p><strong>方法就是使用一个很小的值，对指定位置进行覆盖填充</strong>。在之后计算 softmax 时，由于我们填充的值很小，所以计算出的概率也会很小，基本就忽略了。</p><p><strong>值得留意的是</strong>：</p><ul><li><code>type in (&quot;k&quot;, &quot;key&quot;, &quot;keys&quot;)</code>:  是 <code>padding mask</code>，因此全零的部分我们让 attention 的权重为一个很小的值 -4.2949673e+09。</li><li><code>type in (&quot;q&quot;, &quot;query&quot;, &quot;queries&quot;)</code>:  类似的，<code>query 序列</code>最后面也有可能是一堆 padding，不过对 queries 做 padding mask 不需要把 padding 加上一个很小的值，只要将其置零就行，因为 outputs 是先 key mask，再经过 softmax，再进行 query mask的。</li><li><code>type in (&quot;f&quot;, &quot;future&quot;, &quot;right&quot;)</code>:  是我们在做 <code>decoder</code> 的 self attention 时要用到的 <code>sequence mask</code>，也就是说在每一步，第 i 个 token 关注到的 attention 只有可能是在第 i 个单词之前的单词，因为它按理来说，看不到后面的单词, 作者用一个下三角矩阵来完成这个操作。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">scaled_dot_product_attention</span>(<span class="params">Q, K, V, key_masks,</span></span><br><span class="line"><span class="params">                                 causality=<span class="literal">False</span>, dropout_rate=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                                 training=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                                 scope=<span class="string">&quot;scaled_dot_product_attention&quot;</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;See 3.2.1.</span></span><br><span class="line"><span class="string">    Q: Packed queries. 3d tensor. [N, T_q, d_k].</span></span><br><span class="line"><span class="string">    K: Packed keys. 3d tensor. [N, T_k, d_k].</span></span><br><span class="line"><span class="string">    V: Packed values. 3d tensor. [N, T_k, d_v].</span></span><br><span class="line"><span class="string">    key_masks: A 2d tensor with shape of [N, key_seqlen]</span></span><br><span class="line"><span class="string">    causality: If True, applies masking for future blinding</span></span><br><span class="line"><span class="string">    dropout_rate: A floating point number of [0, 1].</span></span><br><span class="line"><span class="string">    training: boolean for controlling droput</span></span><br><span class="line"><span class="string">    scope: Optional scope for `variable_scope`.</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope, reuse=tf.AUTO_REUSE):</span><br><span class="line">        d_k = Q.get_shape().as_list()[-<span class="number">1</span>]</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># dot product</span></span><br><span class="line">        outputs = tf.matmul(Q, tf.transpose(K, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>]))  <span class="comment"># (N, T_q, T_k)</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># scale</span></span><br><span class="line">        outputs /= d_k ** <span class="number">0.5</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># key masking</span></span><br><span class="line">        outputs = mask(outputs, key_masks=key_masks, <span class="built_in">type</span>=<span class="string">&quot;key&quot;</span>)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># causality or future blinding masking</span></span><br><span class="line">        <span class="keyword">if</span> causality:</span><br><span class="line">            outputs = mask(outputs, <span class="built_in">type</span>=<span class="string">&quot;future&quot;</span>)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># softmax</span></span><br><span class="line">        outputs = tf.nn.softmax(outputs)</span><br><span class="line">        attention = tf.transpose(outputs, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line">        tf.summary.image(<span class="string">&quot;attention&quot;</span>, tf.expand_dims(attention[:<span class="number">1</span>], -<span class="number">1</span>))</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># # query masking</span></span><br><span class="line">        <span class="comment"># outputs = mask(outputs, Q, K, type=&quot;query&quot;)</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># dropout</span></span><br><span class="line">        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=training)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># weighted sum (context vectors)</span></span><br><span class="line">        outputs = tf.matmul(outputs, V)  <span class="comment"># (N, T_q, d_v)</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> outputs</span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mask</span>(<span class="params">inputs, key_masks=<span class="literal">None</span>, <span class="built_in">type</span>=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Masks paddings on keys or queries to inputs</span></span><br><span class="line"><span class="string">    inputs: 3d tensor. (h*N, T_q, T_k)</span></span><br><span class="line"><span class="string">    key_masks: 3d tensor. (N, 1, T_k)</span></span><br><span class="line"><span class="string">    type: string. &quot;key&quot; | &quot;future&quot; </span></span><br><span class="line"><span class="string">    e.g.,</span></span><br><span class="line"><span class="string">    &gt;&gt; inputs = tf.zeros([2, 2, 3], dtype=tf.float32)</span></span><br><span class="line"><span class="string">    &gt;&gt; key_masks = tf.constant([[0., 0., 1.],</span></span><br><span class="line"><span class="string">                                [0., 1., 1.]])</span></span><br><span class="line"><span class="string">    &gt;&gt; mask(inputs, key_masks=key_masks, type=&quot;key&quot;)</span></span><br><span class="line"><span class="string">    array([[[ 0.0000000e+00,  0.0000000e+00, -4.2949673e+09],</span></span><br><span class="line"><span class="string">        [ 0.0000000e+00,  0.0000000e+00, -4.2949673e+09]],</span></span><br><span class="line"><span class="string">       [[ 0.0000000e+00, -4.2949673e+09, -4.2949673e+09],</span></span><br><span class="line"><span class="string">        [ 0.0000000e+00, -4.2949673e+09, -4.2949673e+09]],</span></span><br><span class="line"><span class="string">       [[ 0.0000000e+00,  0.0000000e+00, -4.2949673e+09],</span></span><br><span class="line"><span class="string">        [ 0.0000000e+00,  0.0000000e+00, -4.2949673e+09]],</span></span><br><span class="line"><span class="string">       [[ 0.0000000e+00, -4.2949673e+09, -4.2949673e+09],</span></span><br><span class="line"><span class="string">        [ 0.0000000e+00, -4.2949673e+09, -4.2949673e+09]]], dtype=float32)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    padding_num = -<span class="number">2</span> ** <span class="number">32</span> + <span class="number">1</span> <span class="comment">#足够小的负数，保证被填充的位置进入softmax之后概率接近0</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span> <span class="keyword">in</span> (<span class="string">&quot;k&quot;</span>, <span class="string">&quot;key&quot;</span>, <span class="string">&quot;keys&quot;</span>): <span class="comment"># padding mask</span></span><br><span class="line">        key_masks = tf.to_float(key_masks)</span><br><span class="line">        key_masks = tf.tile(key_masks, [tf.shape(inputs)[<span class="number">0</span>] // tf.shape(key_masks)[<span class="number">0</span>], <span class="number">1</span>]) <span class="comment"># (h*N, seqlen)</span></span><br><span class="line">        key_masks = tf.expand_dims(key_masks, <span class="number">1</span>)  <span class="comment"># (h*N, 1, seqlen)</span></span><br><span class="line">        outputs = inputs + key_masks * padding_num</span><br><span class="line">    <span class="comment"># elif type in (&quot;q&quot;, &quot;query&quot;, &quot;queries&quot;):</span></span><br><span class="line">    <span class="comment">#     # Generate masks</span></span><br><span class="line">    <span class="comment">#     masks = tf.sign(tf.reduce_sum(tf.abs(queries), axis=-1))  # (N, T_q)</span></span><br><span class="line">    <span class="comment">#     masks = tf.expand_dims(masks, -1)  # (N, T_q, 1)</span></span><br><span class="line">    <span class="comment">#     masks = tf.tile(masks, [1, 1, tf.shape(keys)[1]])  # (N, T_q, T_k)</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#     # Apply masks to inputs</span></span><br><span class="line">    <span class="comment">#     outputs = inputs*masks</span></span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">type</span> <span class="keyword">in</span> (<span class="string">&quot;f&quot;</span>, <span class="string">&quot;future&quot;</span>, <span class="string">&quot;right&quot;</span>):    <span class="comment"># future mask</span></span><br><span class="line">        diag_vals = tf.ones_like(inputs[<span class="number">0</span>, :, :])  <span class="comment"># (T_q, T_k)    </span></span><br><span class="line">        tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()  <span class="comment"># (T_q, T_k)    # 上三角皆为0</span></span><br><span class="line">        future_masks = tf.tile(tf.expand_dims(tril, <span class="number">0</span>), [tf.shape(inputs)[<span class="number">0</span>], <span class="number">1</span>, <span class="number">1</span>])  <span class="comment"># (N, T_q, T_k)    # N batch size</span></span><br><span class="line"> </span><br><span class="line">        paddings = tf.ones_like(future_masks) * padding_num</span><br><span class="line">        outputs = tf.where(tf.equal(future_masks, <span class="number">0</span>), paddings, inputs)     <span class="comment"># 上三角中用padding值代替 </span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Check if you entered type correctly!&quot;</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure><h3 id="3-4-Add-amp-Norm"><a href="#3-4-Add-amp-Norm" class="headerlink" title="3.4 Add &amp; Norm"></a>3.4 Add &amp; Norm</h3><p>在 <code>multihead_attention</code> 模块的代码中有以下2行代码，这边对应着模型结构图 <code>encoder</code> 中的 <code>Add &amp; Norm</code> 模块，如下图所示。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Residual connection</span></span><br><span class="line">outputs += queries </span><br><span class="line"></span><br><span class="line"><span class="comment"># Layer Normalize </span></span><br><span class="line">outputs = ln(outputs)</span><br></pre></td></tr></table></figure></p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/transformer14.png" alt="transformer14"></p><p>其中 <code>Add</code> 是类似残差的操作，但与残差不同的是，不是用输入减去输出，而是用输入加上输出。</p><p>而对于 <code>Norm</code>，这里则用的是 <code>Layer Norm</code>，其代码如后文所示。不论是哪一种实际上都是对输入的分布进行调整，调整的通常方式是：</p><script type="math/tex; mode=display">Norm(x_i) = \alpha \times \frac{x_i - u}{\sqrt{\sigma^2_L + \epsilon}} + \beta</script><p>其中，不同的 Norm 方法便对应着不同的 $u,\sigma$ 计算方式。</p><p>这里之所以使用 <code>Layer Norm</code> 而不是 <code>Batch Norm</code> 的原因是：</p><ol><li>BN 比较依赖 BatchSize，偏小不适合，过大耗费 GPU 显存；</li><li>BN 需要 batch 内 features 的维度一致；</li><li>BN 只在训练的时候用，inference 的时候不会用到，因为 inference 的输入不是批量输入；</li><li>每条样本的 token 是同一类型特征，LN 擅长处理，与其他样本不关联，通信成本更少；</li><li>embedding 和 layer size 大，且长度不统一，LN 可以处理且保持分布稳定。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">ln</span>(<span class="params">inputs, epsilon = <span class="number">1e-8</span>, scope=<span class="string">&quot;ln&quot;</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Applies layer normalization. See https://arxiv.org/abs/1607.06450.</span></span><br><span class="line"><span class="string">    inputs: A tensor with 2 or more dimensions, where the first dimension has `batch_size`.</span></span><br><span class="line"><span class="string">    epsilon: A floating number. A very small number for preventing ZeroDivision Error.</span></span><br><span class="line"><span class="string">    scope: Optional scope for `variable_scope`.</span></span><br><span class="line"><span class="string">      </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      A tensor with the same shape and data dtype as `inputs`.</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope, reuse=tf.AUTO_REUSE):</span><br><span class="line">        inputs_shape = inputs.get_shape()    <span class="comment"># 输入形状</span></span><br><span class="line">        params_shape = inputs_shape[-<span class="number">1</span>:]    <span class="comment"># </span></span><br><span class="line">    </span><br><span class="line">        mean, variance = tf.nn.moments(inputs, [-<span class="number">1</span>], keep_dims=<span class="literal">True</span>)    <span class="comment"># 求均值和方差</span></span><br><span class="line">        beta= tf.get_variable(<span class="string">&quot;beta&quot;</span>, params_shape, initializer=tf.zeros_initializer())</span><br><span class="line">        gamma = tf.get_variable(<span class="string">&quot;gamma&quot;</span>, params_shape, initializer=tf.ones_initializer())</span><br><span class="line">        normalized = (inputs - mean) / ( (variance + epsilon) ** (<span class="number">.5</span>) )</span><br><span class="line">        outputs = gamma * normalized + beta</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure><h3 id="3-5-Feed-Forward"><a href="#3-5-Feed-Forward" class="headerlink" title="3.5 Feed Forward"></a>3.5 Feed Forward</h3><p>承接上述，encoder 中只剩下最后一个环节了，也就是 <code>ff</code> 层（Feed Forward），对比模型图，实际上 <code>ff</code> 后还有一层 <code>Add &amp; Norm</code>，但是一般将其二者合并在一个模块中，统称为 <code>ff</code> 层。</p><p>该模块的 code 如下所示，相对比较清晰，2 层 dense 网络后紧接一个 <code>Residual connection</code> 即将输入直接相加，最后再过一层 <code>Layer Normalization</code> 即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">ff</span>(<span class="params">inputs, num_units, scope=<span class="string">&quot;positionwise_feedforward&quot;</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;position-wise feed forward net. See 3.3</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    inputs: A 3d tensor with shape of [N, T, C].</span></span><br><span class="line"><span class="string">    num_units: A list of two integers.  </span></span><br><span class="line"><span class="string">                num_units[0]=d_ff: 隐藏层大小（2048）</span></span><br><span class="line"><span class="string">                num_units[1]=d_model: 词向量长度（512）</span></span><br><span class="line"><span class="string">    scope: Optional scope for `variable_scope`.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      A 3d tensor with the same shape and dtype as inputs</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope, reuse=tf.AUTO_REUSE):</span><br><span class="line">        <span class="comment"># Inner layer</span></span><br><span class="line">        outputs = tf.layers.dense(inputs, num_units[<span class="number">0</span>], activation=tf.nn.relu)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Outer layer </span></span><br><span class="line">        outputs = tf.layers.dense(outputs, num_units[<span class="number">1</span>])</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Residual connection</span></span><br><span class="line">        outputs += inputs</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Layer Normalize</span></span><br><span class="line">        outputs = ln(outputs)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure><h3 id="3-6-decoder"><a href="#3-6-decoder" class="headerlink" title="3.6 decoder"></a>3.6 decoder</h3><p>截止上述是完成了模型的 encoder 模块，本节重点介绍 decoder 模块，其在应用形式上与 encoder 略有不同，整体结构如前文模型结构图中已有展示，容易发现有几个特殊之处：</p><ol><li>输入是经过 <code>Sequence Mask</code> 的，也就是掩去未出现的词；</li><li>每个 decoder 有 2 个 <code>multihead_attention</code> 层；</li><li>首层 <code>multihead_attention</code> 的 $Q,K,V$都是来源输入向量，第二层输入中的 $K,V$ 则是来自 encoder 模块的输出作为 memory 来输入。</li></ol><p>整个 decoder 侧的工作原理可以如下动画展示：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/transformer15.gif" alt="transformer15"></p><p>其中在最后一层 <code>Linear+Softmax</code> 后是怎么得到单词的，想必了解 nlp 的同学也不会陌生，一般就是转化为对应词表大小的概率分布，取最大的位置词即可，如下图所示：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/transformer16.png" alt="transformer16"></p><p>整个 decode 的 code 如下所示，可以清晰的看到 decoder 前的处理与 encoder 几乎一致，唯独 mask 模块走的是 <code>Sequence Mask</code>，在前面的 mask 代码有涉及。每个 decoder 中的 2 层 <code>multihead_attention</code> 的输入差异也比较清晰，重点就是将 encode 模块的输出应用在每个 decoder 的第二层 <code>multihead_attention</code> 中。输出的时候，实际上利用了 <code>softmax</code> 的单调性，直接使用 <code>tf.argmax</code> 来获取最大值位置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, ys, memory, src_masks, training=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    memory: encoder outputs. (N, T1, d_model)</span></span><br><span class="line"><span class="string">    src_masks: (N, T1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    logits: (N, T2, V). float32.</span></span><br><span class="line"><span class="string">    y_hat: (N, T2). int32</span></span><br><span class="line"><span class="string">    y: (N, T2). int32</span></span><br><span class="line"><span class="string">    sents2: (N,). string.</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;decoder&quot;</span>, reuse=tf.AUTO_REUSE):</span><br><span class="line">        decoder_inputs, y, seqlens, sents2 = ys</span><br><span class="line"></span><br><span class="line">        <span class="comment"># tgt_masks</span></span><br><span class="line">        tgt_masks = tf.math.equal(decoder_inputs, <span class="number">0</span>)  <span class="comment"># (N, T2)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># embedding</span></span><br><span class="line">        dec = tf.nn.embedding_lookup(<span class="variable language_">self</span>.embeddings, decoder_inputs)  <span class="comment"># (N, T2, d_model)</span></span><br><span class="line">        dec *= <span class="variable language_">self</span>.hp.d_model ** <span class="number">0.5</span>  <span class="comment"># scale</span></span><br><span class="line"></span><br><span class="line">        dec += positional_encoding(dec, <span class="variable language_">self</span>.hp.maxlen2)</span><br><span class="line">        dec = tf.layers.dropout(dec, <span class="variable language_">self</span>.hp.dropout_rate, training=training)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Blocks</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.hp.num_blocks):</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;num_blocks_&#123;&#125;&quot;</span>.<span class="built_in">format</span>(i), reuse=tf.AUTO_REUSE):</span><br><span class="line">                <span class="comment"># Masked self-attention (Note that causality is True at this time)</span></span><br><span class="line">                dec = multihead_attention(queries=dec,</span><br><span class="line">                                          keys=dec,</span><br><span class="line">                                          values=dec,</span><br><span class="line">                                          key_masks=tgt_masks,</span><br><span class="line">                                          num_heads=<span class="variable language_">self</span>.hp.num_heads,</span><br><span class="line">                                          dropout_rate=<span class="variable language_">self</span>.hp.dropout_rate,</span><br><span class="line">                                          training=training,</span><br><span class="line">                                          causality=<span class="literal">True</span>,</span><br><span class="line">                                          scope=<span class="string">&quot;self_attention&quot;</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Vanilla attention</span></span><br><span class="line">                dec = multihead_attention(queries=dec,</span><br><span class="line">                                          keys=memory,</span><br><span class="line">                                          values=memory,</span><br><span class="line">                                          key_masks=src_masks,</span><br><span class="line">                                          num_heads=<span class="variable language_">self</span>.hp.num_heads,</span><br><span class="line">                                          dropout_rate=<span class="variable language_">self</span>.hp.dropout_rate,</span><br><span class="line">                                          training=training,</span><br><span class="line">                                          causality=<span class="literal">False</span>,</span><br><span class="line">                                          scope=<span class="string">&quot;vanilla_attention&quot;</span>)</span><br><span class="line">                <span class="comment">### Feed Forward</span></span><br><span class="line">                dec = ff(dec, num_units=[<span class="variable language_">self</span>.hp.d_ff, <span class="variable language_">self</span>.hp.d_model])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Final linear projection (embedding weights are shared)</span></span><br><span class="line">    weights = tf.transpose(<span class="variable language_">self</span>.embeddings) <span class="comment"># (d_model, vocab_size)</span></span><br><span class="line">    logits = tf.einsum(<span class="string">&#x27;ntd,dk-&gt;ntk&#x27;</span>, dec, weights) <span class="comment"># (N, T2, vocab_size)</span></span><br><span class="line">    y_hat = tf.to_int32(tf.argmax(logits, axis=-<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> logits, y_hat, y, sents2</span><br></pre></td></tr></table></figure><h3 id="3-7-特殊模块"><a href="#3-7-特殊模块" class="headerlink" title="3.7 特殊模块"></a>3.7 特殊模块</h3><h4 id="3-7-1-label-smoothing"><a href="#3-7-1-label-smoothing" class="headerlink" title="3.7.1 label_smoothing"></a>3.7.1 label_smoothing</h4><p>如前文提到的 <code>train</code> 模块代码，在 decode 后，紧接的便是 <code>label_smoothing</code> 模块。其作用就是：</p><blockquote><p>平滑一下标签值，比如 <code>ground truth</code> 标签是 1 的，改到 0.9333，本来是 0 的，他改到 0.0333，这是一个比较经典的平滑技术了。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">label_smoothing</span>(<span class="params">inputs, epsilon=<span class="number">0.1</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Applies label smoothing. See 5.4 and https://arxiv.org/abs/1512.00567.</span></span><br><span class="line"><span class="string">    inputs: 3d tensor. [N, T, V], where V is the number of vocabulary.</span></span><br><span class="line"><span class="string">    epsilon: Smoothing rate.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    For example,</span></span><br><span class="line"><span class="string">    </span></span><br></pre></td></tr></table></figure><pre><code>import tensorflow as tfinputs = tf.convert_to_tensor([[[0, 0, 1],    [0, 1, 0],   [1, 0, 0]],  [[1, 0, 0],   [1, 0, 0],   [0, 1, 0]]], tf.float32)outputs = label_smoothing(inputs)with tf.Session() as sess:    print(sess.run([outputs]))&gt;&gt;[array([[[ 0.03333334,  0.03333334,  0.93333334],    [ 0.03333334,  0.93333334,  0.03333334],    [ 0.93333334,  0.03333334,  0.03333334]],   [[ 0.93333334,  0.03333334,  0.03333334],    [ 0.93333334,  0.03333334,  0.03333334],    [ 0.03333334,  0.93333334,  0.03333334]]], dtype=float32)]   <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">V = inputs.get_shape().as_list()[-1] # number of channels</span><br><span class="line">return ((1-epsilon) * inputs) + (epsilon / V)</span><br></pre></td></tr></table></figure></code></pre><h4 id="3-7-2-noam-scheme"><a href="#3-7-2-noam-scheme" class="headerlink" title="3.7.2 noam_scheme"></a>3.7.2 noam_scheme</h4><p>在模型的学习了上，作者使用了 <code>noam_scheme</code> 这样一个机制来处理。代码如后文所示，使用的学习率递减公式为：</p><script type="math/tex; mode=display">Lr = init_lr * warm_step^{0.5} * min(s * warm_step^{-1.5}, s^{-0.5})</script><p>其中，$init_lr$ 是指<code>初始学习率</code>，$warm_step$ 是<code>指预热步数</code>，而 $s$ 则是代表全局步数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">noam_scheme</span>(<span class="params">init_lr, global_step, warmup_steps=<span class="number">4000.</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Noam scheme learning rate decay</span></span><br><span class="line"><span class="string">    init_lr: initial learning rate. scalar.</span></span><br><span class="line"><span class="string">    global_step: scalar.</span></span><br><span class="line"><span class="string">    warmup_steps: scalar. During warmup_steps, learning rate increases</span></span><br><span class="line"><span class="string">        until it reaches init_lr.</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    step = tf.cast(global_step + <span class="number">1</span>, dtype=tf.float32)</span><br><span class="line">    <span class="keyword">return</span> init_lr * warmup_steps ** <span class="number">0.5</span> * tf.minimum(step * warmup_steps ** -<span class="number">1.5</span>, step ** -<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure><h3 id="3-8-其他"><a href="#3-8-其他" class="headerlink" title="3.8 其他"></a>3.8 其他</h3><h4 id="3-8-1-项目运行"><a href="#3-8-1-项目运行" class="headerlink" title="3.8.1 项目运行"></a>3.8.1 项目运行</h4><p>该项目运行需要 <code>sentencepiece</code>，其安装的时候留意是否关了 VPN，否则安装会失败，然后可以使用如下代码直接安装：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install sentencepiece</span><br></pre></td></tr></table></figure><h4 id="3-8-2-uitls模块"><a href="#3-8-2-uitls模块" class="headerlink" title="3.8.2 uitls模块"></a>3.8.2 uitls模块</h4><p><code>Transformer</code> 项目中 utils 模块是训练中使用到的工具算子集合，这里简单较少一下各个算子的作用。</p><ul><li><code>calc_num_batches</code>: 计算样本的 num_batch，就是 total_num/batch_size 取整，再加1；</li><li><code>convert_idx_to_token_tensor</code>: 将 int32 转为字符串张量（string tensor）;</li><li><code>postprocess</code>: 做翻译后的处理，输入一个是翻译的预测列表，还有一个是 id2token 的表，就是用查表的方式把数字序列转化成字符序列，从而形成一句可以理解的话。(如果做中文数据这个就要改一下了，中文不适用BPE等word piece算法)。</li><li><code>save_hparams</code>: 保存超参数。</li><li><code>load_hparams</code>: 加载超参数并覆写parser对象。</li><li><code>save_variable_specs</code>: 保存一些变量的信息，包括变量名，shape，总参数量等等。</li><li><code>get_hypotheses</code>: 得到预测序列。这个方法就是结合前面的 postprocess 方法，来生成 num_samples 个数的有意义的自然语言输出。</li><li><code>calc_bleu</code>: 计算BLEU值。</li></ul><h4 id="3-8-3-data-load模块"><a href="#3-8-3-data-load模块" class="headerlink" title="3.8.3 data_load模块"></a>3.8.3 data_load模块</h4><p>在数据加载中有不少预处理环节，我们重点介绍一下相关算子。</p><ul><li><code>load_vocab</code>: 加载词汇表。参数  vocab_fpath表示词文件的地址，会返回两个字典，一个是 id-&gt;token，一个是 token-&gt;id；</li><li><code>load_data</code>: 加载数据。加载源语和目标语数据，筛除过长的数据，注意是筛除，也就是长度超过maxlen的数据直接丢掉了，没加载进去。</li><li><code>encode</code>: 将字符串转化为数字，这里具体方法是输入的是一个字符序列，然后根据空格切分，然后如果是源语言，则每一句话后面加上“\&lt;/s&gt;”，如果是目标语言，则在每一句话前面加上”\<S>“，后面加上“\&lt;/s&gt;”，然后再转化成数字序列。如果是中文，这里很显然要改。</li><li><code>generator_fn</code>: 生成训练和评估集数据。对于每一个sent1，sent2（源句子，目标句子），sent1经过前面的encode函数转化成x，sent2经过前面的encode函数转化成y之后，decoder的输入decoder_input是y[:-1]，预期输出y是y[1:]。</li><li><code>input_fn</code>: 生成Batch数据。</li><li><code>get_batch</code>: 获取batch数据。</li></ul><p><strong>参考文章</strong><br><a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a><br><a href="https://github.com/Kyubyong/transformer">transformer 源码</a><br><a href="https://zhuanlan.zhihu.com/p/149634836">Transformer和Bert相关知识解</a><br><a href="https://blog.csdn.net/nocml/article/details/110920221">Transformer(二)—论文理解：transformer 结构详解</a><br><a href="https://blog.csdn.net/caroline_wendy/article/details/109337216">Python - 安装sentencepiece异常</a><br><a href="https://blog.csdn.net/yujianmin1990/article/details/85221271">The Illustrated Transformer【译】</a><br><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a><br><a href="https://blog.csdn.net/u012759262/article/details/103999959?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-4.control&amp;dist_request_id=58280678-ea4e-4d7f-a2c2-38bd90ab3bda&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-4.control">Attention专场——（2）Self-Attention 代码解析</a><br><a href="https://www.zhihu.com/question/347678607">如何理解Transformer论文中的positional encoding，和三角函数有什么关系？</a></p><hr>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-背景&quot;&gt;&lt;a href=&quot;#1-背景&quot; class=&quot;headerlink&quot; title=&quot;1 背景&quot;&gt;&lt;/a&gt;1 背景&lt;/h2&gt;&lt;p&gt;算法工程师在成长道路上基本绕不开深度学习，而 &lt;code&gt;Transformer&lt;/code&gt; 模型更是其中的经典，它在2017年的&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;《Attention is All You Need》&lt;/a&gt;论文中被提出，直接掀起了 &lt;code&gt;Attention&lt;/code&gt; 机制在深度模型中的广泛应用潮流。&lt;/p&gt;
&lt;p&gt;在该模型中有许多奇妙的想法启发了诸多算法工程师的学习创造，为了让自己回顾复习更加方便，亦或让在学习的读者更轻松地理解，便写了这篇文章。形式上，在参考诸多优秀文章和博客后，这里还是采用结构与代码并行阐述的模式。&lt;/p&gt;</summary>
    
    
    
    <category term="算法总结" scheme="https://www.xiemingzhao.com/categories/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="Transformer" scheme="https://www.xiemingzhao.com/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>PID 调控算法</title>
    <link href="https://www.xiemingzhao.com/posts/pidcontrol.html"/>
    <id>https://www.xiemingzhao.com/posts/pidcontrol.html</id>
    <published>2022-07-08T16:00:00.000Z</published>
    <updated>2025-04-04T17:48:51.554Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p><code>PID</code> 全称 <code>Proportional Integral Derivative</code>，拆分项分别是 <strong>比例（Proportional）、积分（Integral）和微分（Derivative）</strong>。是应用最为广泛的控制模型，有 100 余年的历史了，应用场景有四轴飞行器，汽车的定速巡航等。<br>官方流程图：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/pidcontrol0.png" alt="pidcontrol0"></p><span id="more"></span><p>官方公式：</p><script type="math/tex; mode=display">u(t)=K_pe(t)+K_i\int_0^te(\tau)d\tau+K_d\frac{d}{dt}e(t)=K_p[e(t)+\frac{1}{T_i}\int_0^te(\tau)d\tau+T_d\frac{d}{dt}e(t)]</script><p>其中：</p><ul><li>$K_p,K_i,K_d$ 分别是比例、积分、微分项的参数；</li><li>$T_i,T_d$ 分别是积分、微分的时间常数；</li><li>$e$ 为误差项=目标值(SP)-当前值(PV)；</li><li>$t$ 为当前时间，$\tau$ 积分变数；</li></ul><p>看上去很复杂，实际上比较简单，下面我们通过实例仿真的方式介绍下原理和效果。</p><h2 id="2-算法详解"><a href="#2-算法详解" class="headerlink" title="2 算法详解"></a>2 算法详解</h2><p><strong>示例场景</strong>：我们以汽车的ACC巡航功能为例，假设起始速度为0，目标巡航车速为60。<br><strong>最朴素的想法</strong>：以固定的加速度 a 加速到60后停止。<br><strong>问题</strong>：实际上很难做到上述，因为控制器、传感器的输入、输出量是有延迟的，并且还有惯性的存在（比如，加速度并不能够直接从某个值骤降到0）。所以，比如当车速为58的时候，加速度不变，很容易超过60，超过后减速又很容易低于60，如此稳定性极差。</p><h3 id="改进1-PID-中的-P-比例（Proportional）"><a href="#改进1-PID-中的-P-比例（Proportional）" class="headerlink" title="改进1: PID 中的 P-比例（Proportional）"></a>改进1: PID 中的 P-比例（Proportional）</h3><p>既然有上述问题的存在，那么一个简单的缓解办法就是油门（加速度）不能一直不变，需要时刻监控车速，根据车速来调整，越接近目标值的时候，加速或者减速幅度越小，<strong>以便于车速稳定</strong>。</p><p><strong>算法</strong>：当前时刻车速 $V_t$，目标车速 $V_a$，那么误差项 $e_t = V_a - V_t$，那么输出量为 $u_t = K_p * e_t$，即下一个单位时间提速 $u_t$。<br>通过代码模拟实际加速情况如下（$V_a = 60, K_p = 0.8$）：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/pidcontrol1.jpg" alt="pidcontrol1"></p><p>可以发现很快就趋近于目标值了。但实际上还是会存在<strong>问题</strong>：</p><blockquote><p>实际中汽车会收到风阻、地面摩擦力等各种阻力，会使汽车自燃状态下速度逐渐减小，我们假设单位时间汽车车速收到的阻力综合效果会减速 $V_p$。</p></blockquote><p>当我们把模拟代码加入此项后，情况如下（$V_P = 6$）：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/pidcontrol2.jpg" alt="pidcontrol2"></p><p>看上去最终车速停在了目标车速的下方，这个还是比较好证明的。因汽车最终的车速达到稳态后，则会有<strong>加速=阻力损失</strong>的状态，那么就有：</p><script type="math/tex; mode=display">K_p (V_a - V_t) = V_p</script><p>代入参数可以解得最重的稳态速度</p><script type="math/tex; mode=display">V_t = V_a - V_p/K_p = 52.5</script><blockquote><p>问题：这一差距称为<code>稳态误差</code>，因此需要想办法来克服这一误差。</p></blockquote><h3 id="改进2-PID-中的-I-积分（Proportional）"><a href="#改进2-PID-中的-I-积分（Proportional）" class="headerlink" title="改进2: PID 中的 I-积分（Proportional）"></a>改进2: PID 中的 I-积分（Proportional）</h3><p>积分项能够在比例单元的基础上，消除由比例调控造成的余差，能够对含有累计误差的系统进行误差修正，<strong>减小稳态误差</strong>。</p><p>我们来看实际情况中是怎么生效的，I项离散化后就是历史所有 $e<em>t$ 的累计和。$I_t = \sum</em>{t=0}^T e_t$当我们把模拟代码加入此项后，情况如下（$K_i = 0.2$）：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/pidcontrol3.jpg" alt="pidcontrol3"></p><p>可以发现，最终速度可以很好的收敛到目标。</p><p>为什么能够做到这一点呢？相对也比较好证明，基于前面的稳态条件，此处需要达到稳态的话，需要满足：</p><script type="math/tex; mode=display">K_i I_t + K_p \cdot e_t = V_p</script><p>一般 $K_i,K_p$ 都是正数，那么要想达到稳态，必须 $e_t = 0$，否则 $I_t$ 一直处于变化状态。而 $e_t = 0$ 则意味着 $V_t = V_a$，即在稳态达到的时候，车速最终也将在目标速度。</p><h3 id="改进3-PID中的-D-微分（Derivative）"><a href="#改进3-PID中的-D-微分（Derivative）" class="headerlink" title="改进3: PID中的 D-微分（Derivative）"></a>改进3: PID中的 D-微分（Derivative）</h3><p>看似拥有P和I项之后，整个系统效果已经不错了，那么为什么还需要D项呢？</p><p>实际上，在现实工业系统中，大多数控制通道都是有一定延迟之后的。这时候就需要D这一<code>微分项</code>，它具有<code>超前调节</code>的作用，合适的值能够有效减少系统的超调量，<strong>减缓振荡以提高稳定性</strong>。</p><p>我们来对比一下，假设引入系统滞后性参数 $delay = 0.1$:</p><ul><li>下左图为仅有P和I项，可见收敛前有一个比较大的峰值震荡；</li><li>相应的，在此基础上我们引入D项（$K_d=0.1$），结果如下右图，平缓了许多。</li></ul><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/pidcontrol4.jpg" alt="pidcontrol4"></p><p>可以见到，PID中的三项分别是针对实际系统中的情况进行设计的，有时候D项对应的问题不明显的时候（例如系统延迟很低），确实P和I就够用了。</p><p>另一方面，就是各项超参数的设定，虽然也有一些<a href="https://chem.jgvogel.cn/c/1156/1156348.shtml">参数调整的经验</a>，但在实际应用中更多还是靠实际应用效果为准。</p><h3 id="code"><a href="#code" class="headerlink" title="code"></a>code</h3><p>这里展示的是为了介绍构建的最简单的模型 code，实际工业应用远比此复杂，但底层逻辑相通，仅供参考。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.interpolate <span class="keyword">import</span> make_interp_spline</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PID</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, P, I, D</span>):</span><br><span class="line">        <span class="variable language_">self</span>.Kp = P</span><br><span class="line">        <span class="variable language_">self</span>.Ki = I</span><br><span class="line">        <span class="variable language_">self</span>.Kd = D</span><br><span class="line">        <span class="variable language_">self</span>.sample_time = <span class="number">0.00</span></span><br><span class="line">        <span class="variable language_">self</span>.current_time = <span class="number">0</span></span><br><span class="line">        <span class="variable language_">self</span>.last_time = <span class="variable language_">self</span>.current_time</span><br><span class="line">        <span class="variable language_">self</span>.upper = <span class="number">0</span></span><br><span class="line">        <span class="variable language_">self</span>.lower = <span class="number">0</span></span><br><span class="line">        <span class="variable language_">self</span>.last_error = <span class="number">0</span></span><br><span class="line">        <span class="variable language_">self</span>.pre_error = <span class="number">0</span></span><br><span class="line">        <span class="variable language_">self</span>.inc = []</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">set_bound</span>(<span class="params">self, upper, lower</span>):</span><br><span class="line">        <span class="variable language_">self</span>.upper = upper</span><br><span class="line">        <span class="variable language_">self</span>.lower = lower</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">set_target</span>(<span class="params">self, target</span>):</span><br><span class="line">        <span class="variable language_">self</span>.target = target</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self, feedback_value</span>):</span><br><span class="line">        error = <span class="variable language_">self</span>.target - feedback_value</span><br><span class="line">        delta_error = error - <span class="variable language_">self</span>.last_error</span><br><span class="line">        <span class="comment"># inc_error = error - 2 * self.last_error + self.pre_error</span></span><br><span class="line">        <span class="variable language_">self</span>.inc.append(error)</span><br><span class="line">        PTerm = <span class="variable language_">self</span>.Kp * error<span class="comment">#比例</span></span><br><span class="line">        ITerm = <span class="variable language_">self</span>.Ki * <span class="built_in">sum</span>(<span class="variable language_">self</span>.inc) <span class="comment">#积分</span></span><br><span class="line">        DTerm = <span class="variable language_">self</span>.Kd * delta_error <span class="comment">#微分</span></span><br><span class="line">        <span class="variable language_">self</span>.output = PTerm + ITerm + DTerm</span><br><span class="line">        <span class="comment"># self.output = min(self.upper,max(self.output, self.lower))</span></span><br><span class="line">        <span class="variable language_">self</span>.pre_error = <span class="variable language_">self</span>.last_error</span><br><span class="line">        <span class="variable language_">self</span>.last_error = error</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_pid</span>(<span class="params">P, I , D, L, isdelay = <span class="literal">True</span></span>):</span><br><span class="line">    pid = PID(P, I, D)</span><br><span class="line">    T = <span class="number">60.0</span></span><br><span class="line">    pid.set_target(T)</span><br><span class="line">    pid.set_bound(<span class="number">0.4</span>,-<span class="number">0.4</span>)</span><br><span class="line"></span><br><span class="line">    END = L</span><br><span class="line">    feedback = <span class="number">0</span></span><br><span class="line">    damper =  <span class="number">0.1</span> * T <span class="comment"># 系统阻力</span></span><br><span class="line">    feedback_list = []</span><br><span class="line">    feedback_list.append(feedback)</span><br><span class="line">    time_list = []</span><br><span class="line">    time_list.append(<span class="number">0</span>)</span><br><span class="line">    setpoint_list = []</span><br><span class="line">    setpoint_list.append(pid.target)</span><br><span class="line">    output_last = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, END):</span><br><span class="line">        pid.update(feedback)</span><br><span class="line">        output = pid.output</span><br><span class="line">        delay = <span class="number">0</span> <span class="keyword">if</span> isdelay <span class="keyword">else</span> output_last * <span class="number">0.1</span></span><br><span class="line">        feedback += output - damper + delay <span class="comment">#PID控制系统的函数</span></span><br><span class="line">        feedback_list.append(feedback)</span><br><span class="line">        setpoint_list.append(pid.target)</span><br><span class="line">        time_list.append(i)</span><br><span class="line">        output_last = output</span><br><span class="line"></span><br><span class="line">    time_sm = np.array(time_list)</span><br><span class="line">    time_smooth = np.linspace(time_sm.<span class="built_in">min</span>(), time_sm.<span class="built_in">max</span>(), <span class="number">300</span>)</span><br><span class="line">    feedback_smooth = make_interp_spline(time_list, feedback_list)(time_smooth)</span><br><span class="line">    plt.figure(<span class="number">0</span>)</span><br><span class="line">    plt.grid(<span class="literal">True</span>)</span><br><span class="line">    plt.plot(time_smooth, feedback_smooth,<span class="string">&#x27;b-&#x27;</span>)</span><br><span class="line">    plt.plot(time_list, setpoint_list,<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    plt.xlim((<span class="number">0</span>, L))</span><br><span class="line">    plt.ylim((<span class="built_in">min</span>(feedback_list)-<span class="number">0.5</span>, <span class="built_in">max</span>(feedback_list)+<span class="number">0.5</span>))</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;time (s)&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;PID (PV)&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;PID simulation by python&#x27;</span>,fontsize=<span class="number">15</span>)</span><br><span class="line"></span><br><span class="line">    plt.ylim((<span class="number">0</span>, <span class="number">2</span>*T))</span><br><span class="line"></span><br><span class="line">    plt.grid(<span class="literal">True</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># P</span></span><br><span class="line">    test_pid(<span class="number">0.8</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, L=<span class="number">30</span>)</span><br><span class="line">    <span class="comment"># P + I</span></span><br><span class="line">    test_pid(<span class="number">0.8</span>, <span class="number">0.2</span>, <span class="number">0.0</span>, L=<span class="number">30</span>)</span><br><span class="line">    <span class="comment"># p + I + D</span></span><br><span class="line">    test_pid(<span class="number">0.8</span>, <span class="number">0.2</span>, <span class="number">0.0</span>, L=<span class="number">30</span>, isdelay=<span class="literal">False</span>)</span><br><span class="line">    test_pid(<span class="number">0.8</span>, <span class="number">0.2</span>, <span class="number">0.1</span>, L=<span class="number">30</span>, isdelay=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h2 id="3-流量调控应用"><a href="#3-流量调控应用" class="headerlink" title="3 流量调控应用"></a>3 流量调控应用</h2><p><strong>场景</strong>：在互联网推荐中，经常需要针对一些物料定制分发量。比如新品保最低量，特殊品定量分发等。<br><strong>差异</strong>：不同于ACC越早稳定到目标越好，需要的可能是尽量保留高效的pv，且逐步缓慢式的在规定时间结束前达到目标流量值。</p><p><strong>参数设定：</strong></p><ul><li>某物料日内的流量目标值 $pv_a=2880$；</li><li>跳出条件点击目标 $clk_a=3$;</li><li>分片时间窗口 $P=1h$（平滑pv_a日内波动）；</li><li>调控时间窗口 $W=5m$（p是w的整数倍）；</li></ul><p>那么W将是整个系统的更新频率，每个P内会更新12次；</p><p>假设当前时刻$t$，在某个分片$p$（8-9点）的某个调控窗口$w$（8:20-8:25）内。</p><p><strong>算法步骤：</strong></p><ol><li><p>统计实时流量：</p><ul><li>实时累计曝光$exp_t = 1000$，累计点击$clk_t=1$；</li><li>p和前一p的初始累计曝光$exp<em>p=970,exp</em>{p-1}=840$;</li></ul></li><li><p>判断是否跳出，即$(clk_t&gt;=clk_a)=False$;</p></li><li><p>计算目标：</p><ul><li>$t$所在窗口$p$内的总目标 $target_p=(2880/24)=120$；（简化为均分$pv_a/24$）</li><li>p开始到当前t的累积目标 $target_t = target_p \cdot (25/60) = 50$;</li><li>假设上一p的累积目标 $starget<em>{p-1} = 960$,那么当前 $starget_t = starget</em>{p-1}+ target_t=1010$；</li></ul></li><li><p>计算误差：</p><ul><li>$p$开始到当前$t$实际曝光 $pexp_t = exp_t - exp_p = 30$;</li><li>当前$t$误差 $e_t = target_t - pexp_t = 20$;</li><li>假设 $exp<em>{t-1} = 980$,那么 $e</em>{t-1} = target<em>{t-1} - pexp</em>{t-1} = 25$;</li><li>积分误差 $ie_t = starget_t - exp_t = 10$;</li><li>微分误差 $de<em>t = e_t - e</em>{t-1} = -5$;</li></ul></li><li><p>计算调控输出：</p><ul><li>$u_t = K_p \cdot e_t + K_i \cdot ie_t + K_d \cdot de_t$;</li><li>$u_t = max(min_u, min(u_t, max_u))$ 控制调控上下限；</li><li>应用方式，可以基于$u_t$做插入分发量，或者转为权重进行调控。</li></ul></li></ol><p>当然实际中有很多可以优化点的，比如：</p><ul><li>每个 $p$ 内的 $target_p$ 可以按照日内流量分布来加权计算更准确；</li><li>$u_t$ 应用的时候可以考虑物料具体的效率。</li></ul><p>整体来说，这是一个比较经典的PID算法应用示例，当然也可以看得出，我们还是需要从实际问题出发对算法做一定的调整以便于更好的服务于业务。</p><p><strong>参考文献:</strong><br><a href="https://zhuanlan.zhihu.com/p/448979690">什么是PID？讲个故事，秒懂！</a><br><a href="https://zhuanlan.zhihu.com/p/39573490">PID控制算法原理</a><br><a href="https://chem.jgvogel.cn/c/1156/1156348.shtml">PID算法的一般形式、原理、公式等</a></p><hr>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-引言&quot;&gt;&lt;a href=&quot;#1-引言&quot; class=&quot;headerlink&quot; title=&quot;1 引言&quot;&gt;&lt;/a&gt;1 引言&lt;/h2&gt;&lt;p&gt;&lt;code&gt;PID&lt;/code&gt; 全称 &lt;code&gt;Proportional Integral Derivative&lt;/code&gt;，拆分项分别是 &lt;strong&gt;比例（Proportional）、积分（Integral）和微分（Derivative）&lt;/strong&gt;。是应用最为广泛的控制模型，有 100 余年的历史了，应用场景有四轴飞行器，汽车的定速巡航等。&lt;br&gt;官方流程图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/pidcontrol0.png&quot; alt=&quot;pidcontrol0&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="算法总结" scheme="https://www.xiemingzhao.com/categories/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="调控算法" scheme="https://www.xiemingzhao.com/tags/%E8%B0%83%E6%8E%A7%E7%AE%97%E6%B3%95/"/>
    
    <category term="PID" scheme="https://www.xiemingzhao.com/tags/PID/"/>
    
  </entry>
  
  <entry>
    <title>MIND（多兴趣）召回模型</title>
    <link href="https://www.xiemingzhao.com/posts/mindmodel.html"/>
    <id>https://www.xiemingzhao.com/posts/mindmodel.html</id>
    <published>2022-06-15T16:00:00.000Z</published>
    <updated>2025-04-04T17:48:35.407Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>在深度学习召回算法领域，比较经典的包括了以下2大类：</p><ul><li>基于 <code>item2vec</code> 模型构建在线的i2i召回；</li><li>基于 <code>user2item</code> 泛双塔模型构建在线的u2i召回；</li></ul><blockquote><p>当然还有2阶以上的召回，<code>i2u2i</code>、<code>u2u2i</code>等，在这里不做重点介绍，最终目的都是为了召回 item。</p></blockquote><p>对于第一种，相信大家比较熟知的有从 <code>word2vec</code> 衍生出的<code>item2vec</code>、阿里的<code>deepwalk</code>以及<code>FM</code>等，核心方式都是离线构建出 item 的 Embedding，<strong>在online侧基于用户的行为序列，取其中的 item 作为 trigger 来进行倒排/近邻召回</strong>。</p><span id="more"></span><p>对于第二种，一般比较常用的有微软的 <code>DSSM</code>、<code>Airbnb</code> 的向量召回的以及 <code>YouTubeDNN</code> 模型。他们的核心原理都是构建 user 和 item 的泛化双塔结构，使得 user 和 item 侧的独立生成各自的 Embedding，之后一般进行点积计算余弦相关性来构建 logloss 的优化目标。<strong>online 侧一般基于 user 画像特征，结合 user 侧模型结构实时 infer 出 userEmbedding，并从 item 集合中进行近邻召回 TopK</strong>。</p><p>本文重点介绍的就是2019年阿里团队在 CIKM 上发表的论文<a href="https://arxiv.org/pdf/1904.08030.pdf">《Multi-Interest Network with Dynamic Routing for Recommendation at Tmal》</a>中提出的 <code>MIND（多兴趣）</code>召回模型。</p><h2 id="2-动机"><a href="#2-动机" class="headerlink" title="2 动机"></a>2 动机</h2><blockquote><p>在 u2i 召回领域，最重要便是建立合适的用户<code>兴趣模型</code>，以构建用户兴趣的<code>有效表示</code>。</p></blockquote><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/recall/mind0.png" alt="mind0"></p><p>如上图所示，便是<strong>经典的电商推荐场景</strong>，在召回阶段需要快速召回数千个与用户相关的候选物品。在文章的业务场景中，每日uv量大约在10亿级别，每个 user 会与上百量级的 item 进行互动，而整个物品池在千万甚至亿级别。所以作者发现<code>用户的兴趣具有显著的多样性</code>。</p><p>那么如何有效地表示这种多样的用户兴趣是最关键的问题，在此之前已有不少方案：</p><ul><li><code>协同过滤</code>(itemcf, usercf)召回，是通过历史交互过的物品或隐藏因子直接表示用户兴趣， 但会遇到<strong>稀疏或计算问题</strong></li><li>基于<code>深度学习</code>的召回，将user表示成 dense embedding，例如 DSSM、YouTubeDNN。但是这种<code>单一embedding表示有局限性</code>，对用户兴趣<strong>多样性表示欠佳，而增加 embedding 维度又会带来计算成本，并且也无法解决信息混合的问题</strong>。</li><li>基于 <code>Attention 机制</code>的兴趣表示，例如经典的 DIN 模型。但是，此结构为了有效提取与 item 的信息，需要针对每一个候选 item 应用 attention 来计算 user 的 embedding，<strong>主要应用场景是精排模块</strong>。当然，self-attention 可以避开候选 item 侧，但是其也就退化成了上一种 u2i 模型。</li></ul><p>为了更好的表示用户多样的兴趣，同时又尽量避开上述方法的弊端，作者提出了 MIND（多兴趣）网络模型。其<code>核心思想</code>便是：</p><blockquote><p><strong>基于胶囊网络的动态路由算法来将用户兴趣表示成多个向量</strong></p></blockquote><h2 id="3-胶囊网络与动态路由"><a href="#3-胶囊网络与动态路由" class="headerlink" title="3 胶囊网络与动态路由"></a>3 胶囊网络与动态路由</h2><p>在介绍 <code>MIND</code> 之前，我们需要介绍一下<code>胶囊网络</code>和<code>动态路由</code>这两个知识点，主要是因为它们是MIND模型作者的借鉴来源，熟悉它们有助于对MIND的理解，当然我们只捡其中最核心相关部分来详解。</p><h3 id="3-1-模型起源"><a href="#3-1-模型起源" class="headerlink" title="3.1 模型起源"></a>3.1 模型起源</h3><p>胶囊网络模型是2017年大名鼎鼎的 Hinton 在文章<a href="https://arxiv.org/pdf/1710.09829.pdf">《Dynamic Routing Between Capsule》</a>中提出的。</p><p>实际上，胶囊网络是为了解决CNN在图像识别上的问题。彼时，CNN识别效果很显著，其具有下面两个特性：</p><ul><li><code>平移不变性（translation invariance ）</code>：即不管图片的内容如何进行平移，CNN还能输出与之前一样的结果。这个性质由全局共享权值和 Pooling 共同得到的；</li><li><code>平移等变性（translation equivariance）</code>：即如果你对其输入施加的变换也会同样反应在输出上。这由局部连接和权值共享决定。</li></ul><p>但是其依然具有与一些问题，那就是<strong>对同一个图像的旋转版本会识别错误</strong>，学术上称为不具有<code>旋转不变性</code>。所以为了缓解这一问题，常常会做<code>数据增强</code>以及<code>pooling</code>的操作去增加鲁棒程度：</p><ul><li><code>数据增强</code>：给图片经过旋转，裁剪，变换等操作，让CNN能学习同一张图片不同的这些形态；</li><li><code>pooling</code>：使得网络减少对特征出现的原始位置的依赖；</li></ul><p>以上两种方式往往可以提高模型的泛化能力，但同时丢失了对位置信息的捕获能力。<strong>胶囊网络就是为了赋予模型理解图像中所发生变化的能力，从而可以更好地概括所感知的内容</strong>。</p><h3 id="3-2-胶囊网络"><a href="#3-2-胶囊网络" class="headerlink" title="3.2 胶囊网络"></a>3.2 胶囊网络</h3><p>接下来重点了解一下<code>Capsule</code>（胶囊网络）的结构，我们将其与传统的神经元结构做一个对比，如下图所示。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/recall/mind1.png" alt="mind1"></p><ul><li>上图左侧是标准的神经元结构，其 input 与 output 都是标量，即 <code>scalar to scalar</code> 形式；</li><li>上图右侧便是一个胶囊结构，其 input 与 output 都是 vector，即 <code>vector to vector</code> 形式；</li></ul><p>进一步解析 <code>Capsule</code> 结构，实际上这里的是不包含路由结构的单次胶囊结构。其输入是两个 vector，即 $v_1,v_2$，经过 $W_i$ 线性映射（矩阵乘）后得到新向量 $u_1,u_2$。之后，经过一组 $c_i$ 进行加权和得到汇总向量 $s$，$c_i$ 的计算方式后面会详细介绍。最后将 $s$ 经过<code>Squashing</code>算子便得到了输出向量 $v$。整体计算过程可以汇总如下公式组：</p><script type="math/tex; mode=display">\begin{array}{l}u_i = W_i v_i \\s = \sum c_i u_i \\v = Squashing(s) = \frac{||s||^2}{1 + ||s||^2} \frac{s}{||s||}\end{array}</script><p>对于<code>Squashing</code>算子，我们可以发现:</p><ul><li>其右边的项就是为了做 <code>norm</code>，来<strong>归一化量纲，同时保留了向量的方向</strong>。</li><li>而左侧项则是根据 $s$ 的模 $||s||$ 的大小来对结果进行<strong>压缩，越大，该项约趋于1，相反则趋于0</strong>。</li></ul><p>如此便会有：</p><blockquote><p>当$||s||$比较大的时候，一般是具有大值的长向量，则有$v \approx \frac{s}{||s||}$；<br>当$||s||$比较小的时候，一般是具有小值的短向量，则有$v \approx s||s||$；</p></blockquote><p>为了进一步了解该函数的性质，我们基于标量构建<code>Squashing</code>算子的函数图如下。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/recall/mind2.png" alt="mind2"></p><p>值得注意的是，实际上$W_i$是需要学习的变量，而$c_i$并不是，其为迭代计算的超参数，重点将在下一节介绍。</p><h3 id="3-3-动态路由"><a href="#3-3-动态路由" class="headerlink" title="3.3 动态路由"></a>3.3 动态路由</h3><p>基于前面的胶囊结构，动态路由实际上就是其中叠加一个迭代计算的过程，如下图所示的是原始论文对该算法的描述。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/recall/mind3.png" alt="mind3"></p><p>可以看到，先对每个胶囊初始化一个迭代参数$b_i$，并通过其生成权重$c_i$，在每一次迭代完成之后，更新迭代参数$b_i$。</p><p>这样看不够清晰，由于其基于CNN介绍的，包括了多个 Layer。所以我们基于前一节的单层 <code>Capsule</code>（胶囊网络）转化成如下的计算公式：</p><script type="math/tex; mode=display">\begin{array}{l}b_1^0 = 0, b_2^0 = 0 \\for \quad r = 1 \quad to \quad R \\\quad c_1^r, c_2^r = softmax(b_1^r,b_2^r) \\\quad s^r = c_1^r u_1 + c_2^r u_2 \\\quad a^r = Squashing(s^r) \\\quad b_i^r = b_i^{r-1} +a^r u_i\end{array}</script><p>我们来简要说明一下<strong>整个流程</strong>：</p><ul><li>先对每个 capsule 初始化一个$b_i=0$；</li><li>开始R轮迭代，每轮迭代做以下步骤：<blockquote><ol><li>对所有的$b_i$取 softmax，如此使得权重$c_i$总和为1</li><li>基于$c_i$对所有$u_i$进行加权求和得到$s$</li><li>对$s$应用 Squashing 算子，得到结果向量$a$</li><li>按照公式更新所有$b_i$，并开始下一轮迭代</li></ol></blockquote></li></ul><p>可以看到，实际上权重$c_i$与 attention 中的 weights 生成机制很像，只不过在这里经过$b_i$作为迭代的中间参数，$b_i$实际上称为 <code>routing logit</code>。其初始化为0，就使得$c_i$初始值都一样，对每一个 capsule 的关注度一致，没有偏差，在后面经过学习进行迭代。</p><p>我们将这一迭代过程可视化出来更助于理解。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/recall/mind4.png" alt="mind4"></p><p>实际上，$v_i$<strong>可以称为 Capsule 网络的 input 向量</strong>，首先通过 $W_i$ 将其线性映射为 $u_i$，在这里 $v_i,u_i$ 的维度可能不同，前者是输入维度，后者是胶囊维度。并且，<strong>这一映射过程只在迭代前进行</strong>，迭代中只会用到映射后的 $u_i$。</p><p>在上图中，实际上有2个 capsule 向量，即 $u_1,u_2$，所以对应的会有 $b_1,b_2$ 两个初始参数以及其对应的迭代权重 $c_1,c_2$。<strong>他们的右上角标是指迭代的轮数 r</strong>。</p><p>例如，</p><ol><li>r=0 的时候，$b_1^0=1,b_2^2=0$是初始化参数；</li><li>然后经过 softmax 得到第1轮的 $c_1^1,c_2^1$ 权重；</li><li>经过胶囊网络得到第1轮的结果向量 $a^1$；</li><li>按照公式 $b_i^r = b_i^{r-1} + a^r u_i$ 便可迭代得到第2轮的 $b_1^1,b_2^1$ 参数;</li><li>与是便得到更新后的第2轮的权重 $c_1^2,c_2^2$。</li></ol><p>以此类推，直到最后一步迭代结束将 $a^3$ 最为最终结果向量输出。</p><p>既然 $b_i$ 不是学习得到的，而是迭代得到的，那么这里重点关注一下其更新公式。我们可以发现：</p><blockquote><p>$b_i$ 在第r轮的变化项是 $a^r u_i$，如果该内积项值很大，则说明本轮的结果向量 $a^r$ 与此 <code>capsule</code> 向量 $u_i$ 很相似，那么参数 $b_i^r$ 便会增加，下一轮的权重 $c_i$ 同样变大，那么对应的 $a$ 中包含的 $u_i$ 的成分就会更大，二者向量就更近。<strong>实际上，这个 <code>dynamic routing</code> 的过程被看成是<code>软聚类</code>（soft-clustering）</strong>。</p></blockquote><h3 id="3-4-有效的原因"><a href="#3-4-有效的原因" class="headerlink" title="3.4 有效的原因"></a>3.4 有效的原因</h3><p>我们还以该技术的起源CNN图像识别为例，如下图所示，CNN实际上属于左侧结果，即对于图像的旋转是不变的，前面提过主要是通过一些手段加强训练的。而我们期望能够做到右侧的等变性，即能够感知到图像的变化，但又不影响结果。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/recall/mind5.png" alt="mind5"></p><p>那为什么融入的 <code>Capsule</code> 网络结构就能够做到呢，我们举一个例子，如下图所示。</p><ul><li>左侧是一个经典的 <code>maxpooling 结构</code>，其仅仅能做到 <code>Invariance</code>（不变性），即对于位置的变化无法感知，但能够做到结果一致。</li><li>右侧是一个 <code>capsule 结构</code>，首先其在结果上能够做到 <code>Invariance</code>（不变性），同时其过程中产生的 <code>capsule</code> 向量是不同的，即能够感知到图像旋转的变化，所以同时做到了 <code>Equivariance</code>（等变性）。</li></ul><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/recall/mind6.png" alt="mind6"></p><h2 id="4-MIND模型"><a href="#4-MIND模型" class="headerlink" title="4 MIND模型"></a>4 MIND模型</h2><h3 id="4-1-模型概述"><a href="#4-1-模型概述" class="headerlink" title="4.1 模型概述"></a>4.1 模型概述</h3><p>经过前面的介绍，接下来理解 MIND 模型的结构就会简单的多。我们首先将其网络架构展示出来:</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/recall/mind7.png" alt="mind7"></p><p>整个图大部分都是比较清晰的。</p><ol><li>底部是输入特征的 Embedding Layer，包括：<ul><li><code>用户属性特征</code>（user ID，Age，Gender等，最左侧，<code>concat 操作</code>）</li><li><code>行为序列特征</code>（item ID，Brand ID，Category等，中间部分，<code>pooling 操作</code>）</li><li><code>物品特征</code>（item ID，Brand ID，Category等，最右侧，<code>pooling 操作</code>）</li></ul></li><li>用户行为序列特征会经过 <code>Multi-Interest Extractor Layer（多兴趣提取层）</code>抽取 <code>Interest Capsules</code>，即多个胶囊兴趣向量；</li><li>将生成的 <code>Interest Capsules</code> 与用户属性特征 <strong>concat 到一起，经过两层 ReLU 激活函数的全连接网络</strong>；</li><li>在 <code>training</code> 阶段，继续经过 <code>Label-aware Attention（标签意识注意力）</code>层，最终结合 <code>Sampled Softmax Loss</code>（负采样损失函数）即可完成训练；</li><li>而在左上角表示的是 <code>Serving</code> 的时候，线上直接使用<strong>步骤3的结果（多兴趣向量）进行 TopK 的近邻召回即可</strong>。</li></ol><p>需要注意的是，博主在工作中发现其中<strong>步骤3容易引起很多人误解</strong>：</p><blockquote><p>也就是 <code>Interest Capsules</code> 抽取完之后的紧接着的两层全连接，这里<em>很容易误解成将所有的兴趣向量与用户属性全部打平concat到到一起</em>，然后经过两层FC，那结果不就是一个向量了吗？难道说这里还需要重新再把结果的长向量slice成多个Interest Capsules？<strong>答案显然NO！</strong></p></blockquote><p>仔细研究后文或者 code，便可以知道：<strong>这里的FC（全连接）是应用在 Interest Capsules 与用户属性特征 concat 后的最后一维上</strong>。</p><p>这里列举相关变量维度可能更容易理解：</p><ul><li>假设 用户属性特征 <code>concat</code> 后维度是 (b, 1, n)，b 是 <code>Batch Size</code>，扩展出第二维的1是为了对齐</li><li>而提取的 <code>Interest Capsules</code> 层维度为 (b, k, m), k 是胶囊个数</li><li>全连接层 FC 的 Input 应该是上述二者的 concat 结果，即 (b, k, n+m)</li><li>FC 层是应用在上述结果的最后一层进行线性映射，故其结果维度 (b, k, d)，d 是最终的 capsule 维度，其应该和 item 侧 的embedding pooling 结果一致，如此才能做 Attention。</li></ul><p>接下来我们按照论文结构，介绍其中核心部分。</p><h3 id="4-2-问题定义"><a href="#4-2-问题定义" class="headerlink" title="4.2 问题定义"></a>4.2 问题定义</h3><p>这是一个召回问题，其任务目标毋庸置疑：</p><blockquote><p>根据用户行为和属性等特征抽取多个用户兴趣的向量表示，然后利用其从 item 池子中进行TopK的近邻召回。</p></blockquote><p>模型的输入在前一节已经介绍，主要是一个 <code>user&amp;item</code> 的信息三元组 $(I_u,P_u,F_i)$，其中：</p><ul><li>$I_u$ 代表与用户u交互过的物品集，即用户的历史行为;</li><li>$P_u$ 表示用户的属性，例如性别、年龄等；</li><li>$F_i$ 表示为目标物品i的一些特征，例如 item id 和 category id 等。</li></ul><p>基于上述，模型的<code>核心任务</code>：<br>将用户的属性$P_u$和行为特征$I_u$有效地映射成用户多兴趣 Embedding 向量集合，即</p><script type="math/tex; mode=display">V_u = f_u(I_u, P_u) = (v_u^1, \dots , v_u^k) \in R^{d \times k}</script><p>其中，<strong>d 是用户最终的兴趣向量 Embedding 维度，k 表示兴趣向量的个数。</strong></p><p>如此容易发现：</p><blockquote><p>如果 $k=1$，即只有一个兴趣向量的话，模型本身就退化成传统的召回模型结构了，例如 YouTube DNN 这样。</p></blockquote><p>而目标物品侧的映射方式:</p><script type="math/tex; mode=display">\vec e_i = f_{item}(F_i)</script><p>其中 $\vec e<em>i \in R^{d \times 1}$，于是其维度就和兴趣向量对其了，就支持后面的 <code>Label-aware Attention</code> 操作，而 $f</em>{item}( \cdot )$ 是一个 <code>Embedding &amp; Pooling</code> 层，即<strong>目标 item 的不同属性特征过 Embedding Layer 层后直接进行 sum/avg pooling。</strong></p><p>最后也是将每个兴趣向量通过内积做相似度进行 TopK 的 item 召回：</p><script type="math/tex; mode=display">f_{score} (V_i, \vec e_i) = \max_{1 \le k \le K} \vec e_i \vec V_u^k</script><h3 id="4-3-Multi-Interest-Extractor-Layer（多兴趣提取层）"><a href="#4-3-Multi-Interest-Extractor-Layer（多兴趣提取层）" class="headerlink" title="4.3 Multi-Interest Extractor Layer（多兴趣提取层）"></a>4.3 Multi-Interest Extractor Layer（多兴趣提取层）</h3><h4 id="4-3-1-Dynamic-Routing-Revisit（动态路由）"><a href="#4-3-1-Dynamic-Routing-Revisit（动态路由）" class="headerlink" title="4.3.1 Dynamic Routing Revisit（动态路由）"></a>4.3.1 Dynamic Routing Revisit（动态路由）</h4><p>在胶囊网络内，不管迭代多少次，实际上可以把整个网络看成2层，一个是 input 的低阶胶囊记为 $\vec c<em>i^l \in R^{N_l \times 1}, i \in {1, \cdots , m}$，另一层便是 output 的高阶胶囊记为 $\vec c_j^h \in R^{N_h \times 1}, i \in {1, \cdots , n}$。其中 m, n 表示胶囊的个数，在 MIND 中<strong>m 那就是输入时序列的长度，n便是要抽取的兴趣向量个数</strong>，$N_l, N_h$ 表示两层胶囊的维度。 那么从低阶胶囊抽取高阶胶囊过程中的路由对数$b</em>{ij}$一般如下计算：</p><script type="math/tex; mode=display">b_{ij} = (\vec c_j^h)^T S_{ij} \vec c_I^l</script><p>其中，$S<em>{ij} \in R^{N_j \times N_l}$ 是待学习的转换矩阵。接下来便可由 $b</em>{ij}$ 计算出高低阶胶囊之间的加权权重 $w<em>{ij}$（又称耦合系数），即直接对 $b</em>{ij}$ 进行 softmax 计算即可：</p><script type="math/tex; mode=display">w_{ij} = \frac{\exp{b_{ij}}}{\sum_{k = 1}^m \exp{b_{ik}}}</script><p><strong>注意：这里计算的是某个低阶向量在不同胶囊之间的权重分配（总和为1），而不是某个胶囊里面不同低阶向量的权重分配</strong></p><p>然后，便可以基于上述的权重来计算高阶胶囊j的中间过渡向量$\vec z_j^h$：</p><script type="math/tex; mode=display">\vec z_j^h = \sum_{i=1}^m w_{ij} S_{ij} \vec c_i^l</script><p>最后，便是通过 <code>Squashing</code> 算子对中间变量进行压缩来得到结果的高阶胶囊向量 $\vec c_j^h$：</p><script type="math/tex; mode=display">\vec c_j^h = Squashing(\vec z_j^h) = \frac{||\vec z_j^h||^2}{1 +||\vec z_j^h||^2 } \frac{\vec z_j^h}{||\vec z_j^h||}</script><p>上述是一次迭代的整个过程，看上去貌似与前述的胶囊网络不一样，实则不然。为了进一步促进理解，依然跟上一节一样，我们将<code>单个高阶胶囊</code>$\vec c_j^h$的2轮迭代的动态路由可视化出来，如下图所示。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/recall/mind8.png" alt="mind8"></p><p>将需要注意的是：<strong>原论文中的符号与前文和图示有些区别，且无高阶胶囊维度j</strong>：</p><ul><li>论文的 $w<em>{ij}$ -&gt; 图示的胶囊加权权重$c</em>{ir}$</li><li>论文的低阶和高阶胶囊 $\vec c_i^l, \vec c_j^h$ -&gt; 图示的输入和输出向量 $v_i, v$</li><li>论文的聚合向量 $\vec z_j^h$ -&gt; 图示的聚合向量$s$</li><li>论文的转化系数 $S<em>{ij}$ -&gt; 图示的转化矩阵 $W</em>{i}$</li></ul><h4 id="4-3-2-B2I-Dynamic-Routing（B2I动态路由）"><a href="#4-3-2-B2I-Dynamic-Routing（B2I动态路由）" class="headerlink" title="4.3.2 B2I Dynamic Routing（B2I动态路由）"></a>4.3.2 B2I Dynamic Routing（B2I动态路由）</h4><p>MIND 的作者实际上没有使用最原始的动态路由机制，而是使用了做了些许改造的<code>B2I动态路由</code>。它和原始的路由主要有3出处区别：(<strong>本部分以原文符号为主</strong>)</p><ol><li><strong>共享映射矩阵</strong>。<blockquote><p>即所有的$S<em>{ij}$（图中的$W</em>{i}$）使用同一个S，主要原因是：</p></blockquote></li></ol><ul><li>input 胶囊（用户行为序列）的<strong>长度是不等的</strong>，统一映射矩阵利于减少参数提高泛化；</li><li>统一的映射矩阵可将商品映射的<strong>向量统一到同一空间</strong>；</li></ul><ol><li><strong>随机初始化陆游对数 $b_{ij}$</strong><blockquote><p>由于共享了映射矩阵S，那么如果$b<em>{ij}$初始化为 0，那么 softmax 后产生的所有的加权权重 $w</em>{ij}$ 边都是相等的，之后各个兴趣胶囊在迭代中将会始终保持一致。作者实际上采用高斯分布来初始化 $b_{ij}$，<strong>这样使得每个胶囊（用户兴趣聚类中心）差异较大</strong>，从而度量多样的兴趣。实际上与<code>K-means思想</code>有点类似。</p></blockquote></li></ol><p>$b_{ij}$这一点可以从论文中的实验结果看到，使用<strong>方差更大的高斯函数来初始化routing logits</strong>效果更好:</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/recall/mind9.png" alt="mind9"></p><p><strong>但是，这里需要注意！！！</strong><br>上面的设计并不一定是最优的，博主在实际应用中，发现参数不共享时，$b_{ij}$ 可以初始化为 0，效果反而更好，更有利于兴趣向量差异化，与业界其他业务交流也有类似的。</p><ol><li><strong>动态的兴趣胶囊数量</strong><blockquote><p>作者出发点是<strong>行为个数不一样的用户兴趣向量应该也有差异</strong>，行为越丰富兴趣像两个数相对给多一些，具体兴趣向量个数通过下面公式来确定。</p></blockquote></li></ol><script type="math/tex; mode=display">{K_u}' = max(1, min(K, log_{2}{(|L_u|)}))</script><h3 id="4-4-Label-aware-Attention-Layer（标签意识注意力层）"><a href="#4-4-Label-aware-Attention-Layer（标签意识注意力层）" class="headerlink" title="4.4 Label-aware Attention Layer（标签意识注意力层）"></a>4.4 Label-aware Attention Layer（标签意识注意力层）</h3><p>实际上在多兴趣提取层和标签意识注意力层之间还夹杂着两个步骤：</p><ol><li>将用户的属性 Embedding 分别 concat 到每一个兴趣向量上；</li><li>再经过两层激活函数为 ReLU 的全连接层来对其维度；</li></ol><p>上述两部在前面部分已经介绍过，那么在此之后变得到了可以 feed 进入 Label-aware Attention Layer 的多兴趣向量。该层内的计算结构比较熟知，其实就是传统的 QKV 形式的 <code>Attention 结构</code>：</p><script type="math/tex; mode=display">\vec v_u = Attention(\vec e_i, V_u, V_u) = V_u \quad softmax(pow(V_u^T \vec e_i, p))</script><p>其中，$\vec e_i$表示的目标商品向量，$V_u$就是用户的多兴趣向量组合，里面会有${K_u}’$个有效的兴趣向量。唯一的<strong>区别是，在做完内积操作后进行了一个幂次操作，$p$就是幂次的超参数</strong>。如此便会发现p是一个可调节的参数来调整注意力分布：</p><blockquote><p>当 $p \longrightarrow 0$ 时，不同兴趣胶囊的注意力权重趋于相同；<br>当 $p &gt;&gt; 0$ 时，较大注意力权重的胶囊将会拉大这个优势，极端情况 $p \longrightarrow \infty$ 时，就变成了 <code>hard-attention</code>，即只有一个兴趣胶囊会生效；</p></blockquote><p><strong>值得注意的是，实际应用中（本人也有同样经验），p 小会使得胶囊之间差距缩小，反之可以使得兴趣胶囊差异性增加，实际线上效果也是 <code>hard-attention</code> 模式效果最优（如下图）</strong></p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/recall/mind10.png" alt="mind10"></p><h3 id="4-6-离线训练和线上服务"><a href="#4-6-离线训练和线上服务" class="headerlink" title="4.6 离线训练和线上服务"></a>4.6 离线训练和线上服务</h3><p>听过前面介绍的 Label-aware Attention Layer 生成用户u的聚合兴趣向量之 $\vec v_u$ 后，用户与物品i的<code>交互的概率</code>可以如下计算：</p><script type="math/tex; mode=display">Pr(i|u) = Pr(\vec e_i | \vec v_u) = \frac{exp{(\vec v_u^T \vec e_i)}}{\sum_{j \in I} exp{(\vec v_u^T \vec e_j)}}</script><p><strong>实际上就是一个对有所物品应用 softmax 算子</strong></p><p>整体的<code>目标函数</code>是：</p><script type="math/tex; mode=display">L = \sum_{(u,i) \in D} log{Pr(i|u)}</script><p>其中，D是训练数据包含用户物品交互的集合。</p><blockquote><p>这里与 word2vec 类似，由于最后一层需要对所有物品应用 softmax 算子来计算概率。而有效物品的量一般很大，所以为了简化计算就转化成 <code>SampledSoftmax</code> 的方式，即只保留正样本，通过负采样生成负样本来做 <code>binary task</code>。</p></blockquote><p><strong>线上 serving 的时候</strong>，去除 label-aware 层，仅需要得到一个用户多兴趣向量表示的映射 $f_{user}$ 即可。通过 feed 用户画像信息，得到多个有效的兴趣表示向量，然后分别从物品集合中近邻检索 TopN 个物品即可（总共KN个物品）。</p><p>最后，作者实验了不同兴趣个数K的效果，发现<strong>最大兴趣个数K控制在5-7的时候表现较好</strong>。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/recall/mind11.png" alt="mind11"></p><h2 id="5-code"><a href="#5-code" class="headerlink" title="5 code"></a>5 code</h2><p>这里给出一版自己实现的模型结构，篇幅原因，这里重点展示模型核心结构部分，其他模块省略，仅供参考。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.python.ops <span class="keyword">import</span> partitioned_variables</span><br><span class="line"><span class="keyword">from</span> .recModelOpt <span class="keyword">import</span> recModelOpt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> modules <span class="keyword">import</span> dnn, capsuleLayer</span><br><span class="line"><span class="keyword">import</span> modules.featProcessor <span class="keyword">as</span> fp</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> tf.__version__ &gt;= <span class="string">&#x27;2.0&#x27;</span>:</span><br><span class="line">    tf = tf.compat.v1</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">mindSampled</span>(<span class="title class_ inherited__">recModelOpt</span>):</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build_graph</span>(<span class="params">self, features, mode, params</span>):</span><br><span class="line">            ......</span><br><span class="line">            <span class="comment"># build high_capsules</span></span><br><span class="line">            seqFeats = tf.concat(seqFeatList, axis=<span class="number">2</span>, name=<span class="string">&quot;seqFeats&quot;</span>)</span><br><span class="line">            seqFeats = tf.layers.dense(seqFeats, units=<span class="variable language_">self</span>.high_dim, activation=tf.nn.selu, name=<span class="string">&quot;seqFeatsDim&quot;</span>)</span><br><span class="line">            capsuleNet = capsuleLayer(capsule_config=capsule_config, is_training=is_training)</span><br><span class="line">            high_capsules, num_capsules = capsuleNet(seqFeats, seqLen)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># concatenate with user features</span></span><br><span class="line">            userFeats = tf.tile(tf.expand_dims(user_inputs, axis=<span class="number">1</span>),</span><br><span class="line">                [<span class="number">1</span>, tf.shape(high_capsules)[<span class="number">1</span>], <span class="number">1</span>])</span><br><span class="line">            interest_capsule = tf.concat([high_capsules, userFeats], axis=<span class="number">2</span>, name=<span class="string">&quot;cap_concat&quot;</span>)</span><br><span class="line">            tf.logging.info(<span class="string">&quot;=&quot;</span> * <span class="number">8</span> + <span class="string">&quot;interest_capsule shape is %s&quot;</span> % <span class="built_in">str</span>(interest_capsule.shape) + <span class="string">&quot;=&quot;</span> * <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">            interest_capsule = dnn(<span class="built_in">input</span>=interest_capsule, dnnDims=<span class="variable language_">self</span>.userDnn, is_training = is_training,</span><br><span class="line">                            usebn = <span class="literal">False</span>, l2_reg = <span class="variable language_">self</span>.l2_reg, name = <span class="string">&quot;userDnn&quot;</span>)</span><br><span class="line">            <span class="comment"># cap_norm = self.norm(interest_capsule, axis = 2, name = &quot;user_norm&quot;)</span></span><br><span class="line">            <span class="comment"># item_norm = self.norm(self.item_vec, axis = 1, name = &quot;item_norm&quot;)</span></span><br><span class="line"></span><br><span class="line">            cap_att = tf.matmul(interest_capsule, tf.reshape(<span class="variable language_">self</span>.item_vec, [-<span class="number">1</span>, <span class="variable language_">self</span>.high_dim, <span class="number">1</span>]))</span><br><span class="line">            cap_att = tf.reshape(tf.<span class="built_in">pow</span>(cap_att, <span class="variable language_">self</span>.sim_pow), [-<span class="number">1</span>, <span class="variable language_">self</span>.num_interest])</span><br><span class="line">            capsules_mask = tf.sequence_mask(num_capsules, <span class="variable language_">self</span>.num_interest)</span><br><span class="line">            user_capsules = tf.multiply(interest_capsule, tf.to_float(capsules_mask[:, :, <span class="literal">None</span>]), name=<span class="string">&quot;user_capsules&quot;</span>)</span><br><span class="line">            padding = tf.ones_like(cap_att) * (-<span class="number">1e9</span>)</span><br><span class="line">            cap_att = tf.where(capsules_mask, cap_att, padding)</span><br><span class="line">            cap_att = tf.nn.softmax(cap_att, axis=<span class="number">1</span>)</span><br><span class="line">            cap_att_stop = tf.stop_gradient(cap_att)</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.hardAtt:</span><br><span class="line">                user_vec = tf.gather(tf.reshape(interest_capsule, [-<span class="number">1</span>, <span class="variable language_">self</span>.high_dim]),</span><br><span class="line">                                        tf.argmax(cap_att_stop, axis=<span class="number">1</span>, output_type=tf.int32) + tf.<span class="built_in">range</span>(</span><br><span class="line">                                        tf.shape(cap_att_stop)[<span class="number">0</span>]) * <span class="variable language_">self</span>.num_interest)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                user_vec = tf.matmul(tf.reshape(cap_att_stop, [tf.shape(cap_att_stop)[<span class="number">0</span>], <span class="number">1</span>, <span class="variable language_">self</span>.num_interest]),</span><br><span class="line">                                     interest_capsule)</span><br><span class="line">            <span class="variable language_">self</span>.user_vec = tf.reshape(user_vec, [-<span class="number">1</span>, <span class="variable language_">self</span>.high_dim], name=<span class="string">&quot;user_embed&quot;</span>)</span><br><span class="line">            <span class="variable language_">self</span>.user_emb = tf.reduce_join(</span><br><span class="line">                tf.reduce_join(tf.as_string(user_capsules), axis=-<span class="number">1</span>, separator=<span class="string">&#x27;,&#x27;</span>),</span><br><span class="line">                axis=-<span class="number">1</span>, separator=<span class="string">&#x27;|&#x27;</span>)</span><br><span class="line">            <span class="variable language_">self</span>.item_emb = tf.reduce_join(tf.as_string(<span class="variable language_">self</span>.item_vec), axis=-<span class="number">1</span>, separator=<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">            </span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">capsuleLayer</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, capsule_config, is_training, name = <span class="string">&quot;capsuleNet&quot;</span></span>):</span><br><span class="line">        <span class="comment"># max_seq_len: max behaviour sequence length(history length)</span></span><br><span class="line">        <span class="variable language_">self</span>._max_seq_len = capsule_config.get(<span class="string">&quot;max_seq_len&quot;</span>, <span class="number">10</span>)</span><br><span class="line">        <span class="comment"># max_k: max high capsule number</span></span><br><span class="line">        <span class="variable language_">self</span>._num_interest = capsule_config.get(<span class="string">&quot;num_interest&quot;</span>, <span class="number">3</span>)</span><br><span class="line">        <span class="comment"># high_dim: high capsule vector dimension</span></span><br><span class="line">        <span class="variable language_">self</span>._high_dim = capsule_config.get(<span class="string">&quot;high_dim&quot;</span>, <span class="number">32</span>)</span><br><span class="line">        <span class="comment"># number of Expectation-Maximization iterations</span></span><br><span class="line">        <span class="variable language_">self</span>._num_iters = capsule_config.get(<span class="string">&quot;num_iters&quot;</span>, <span class="number">3</span>)</span><br><span class="line">        <span class="comment"># routing_logits_scale</span></span><br><span class="line">        <span class="variable language_">self</span>._routing_logits_scale = capsule_config.get(<span class="string">&quot;routing_logits_scale&quot;</span>, <span class="number">1.0</span>)</span><br><span class="line">        <span class="comment"># routing_logits_stddev</span></span><br><span class="line">        <span class="variable language_">self</span>._routing_logits_stddev = capsule_config.get(<span class="string">&quot;routing_logits_stddev&quot;</span>, <span class="number">1.0</span>)</span><br><span class="line">        <span class="variable language_">self</span>.bilinear_type = capsule_config.get(<span class="string">&quot;bilinear_type&quot;</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>._is_training = is_training</span><br><span class="line">        <span class="variable language_">self</span>.name = name</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">squash</span>(<span class="params">self, cap_interest</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Squash cap_interest over the last dimension.&quot;&quot;&quot;</span></span><br><span class="line">        cap_norm = tf.reduce_sum(tf.square(cap_interest), axis=-<span class="number">1</span>, keep_dims=<span class="literal">True</span>)</span><br><span class="line">        scalar_factor = cap_norm / (<span class="number">1</span> + cap_norm) / tf.sqrt(cap_norm + <span class="number">1e-8</span>)</span><br><span class="line">        <span class="keyword">return</span> scalar_factor * cap_interest</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">seq_feat_high_builder</span>(<span class="params">self, seq_feat</span>):</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="variable language_">self</span>.name + <span class="string">&#x27;/bilinear&#x27;</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.bilinear_type == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># 复用转换矩阵，后面路由对数可高斯初始化</span></span><br><span class="line">                seq_high = tf.layers.dense(seq_feat, <span class="variable language_">self</span>._high_dim, activation=<span class="literal">None</span>, bias_initializer=<span class="literal">None</span>)</span><br><span class="line">                seq_high = tf.tile(seq_high, [<span class="number">1</span>, <span class="number">1</span>, <span class="variable language_">self</span>._num_interest])</span><br><span class="line">            <span class="keyword">elif</span> <span class="variable language_">self</span>.bilinear_type == <span class="number">1</span>:</span><br><span class="line">                <span class="comment"># seq_feat_high</span></span><br><span class="line">                seq_high = tf.layers.dense(seq_feat, <span class="variable language_">self</span>._num_interest * <span class="variable language_">self</span>._high_dim, activation=<span class="literal">None</span>,</span><br><span class="line">                                               bias_initializer=<span class="literal">None</span>)</span><br><span class="line">            <span class="keyword">elif</span> <span class="variable language_">self</span>.bilinear_type == <span class="number">2</span>:</span><br><span class="line">                <span class="comment"># seq_feat_high</span></span><br><span class="line">                seq_feat =  tf.reshape(seq_feat, [-<span class="number">1</span>, <span class="variable language_">self</span>._max_seq_len, <span class="variable language_">self</span>._high_dim])</span><br><span class="line">                seq_high = tf.layers.dense(seq_feat, <span class="variable language_">self</span>._max_seq_len * <span class="variable language_">self</span>._num_interest * <span class="variable language_">self</span>._high_dim, activation=<span class="literal">None</span>,</span><br><span class="line">                                               bias_initializer=<span class="literal">None</span>)</span><br><span class="line">                seq_high = tf.reshape(seq_high, [-<span class="number">1</span>, <span class="variable language_">self</span>._max_seq_len, <span class="variable language_">self</span>._num_interest, <span class="variable language_">self</span>._high_dim])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 扩增一维trans矩阵</span></span><br><span class="line">                w = tf.get_variable(</span><br><span class="line">                    <span class="variable language_">self</span>.name + <span class="string">&#x27;/transWeight&#x27;</span>, shape=[<span class="number">1</span>, <span class="variable language_">self</span>._max_seq_len, <span class="variable language_">self</span>._num_interest * <span class="variable language_">self</span>._high_dim, <span class="variable language_">self</span>._high_dim],</span><br><span class="line">                    initializer=tf.random_normal_initializer())</span><br><span class="line">                <span class="comment"># [N, T, 1, C]</span></span><br><span class="line">                u = tf.expand_dims(seq_feat, axis=<span class="number">2</span>)</span><br><span class="line">                <span class="comment"># [N, T, num_caps * dim_caps]</span></span><br><span class="line">                seq_high = tf.reduce_sum(w[:, :<span class="variable language_">self</span>._max_seq_len, :, :] * u, axis=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">        seq_high = tf.reshape(seq_high, [-<span class="number">1</span>, <span class="variable language_">self</span>._max_seq_len, <span class="variable language_">self</span>._num_interest, <span class="variable language_">self</span>._high_dim])</span><br><span class="line">        seq_high = tf.transpose(seq_high, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line">        seq_high = tf.reshape(seq_high, [-<span class="number">1</span>, <span class="variable language_">self</span>._num_interest, <span class="variable language_">self</span>._max_seq_len, <span class="variable language_">self</span>._high_dim])</span><br><span class="line">        <span class="keyword">return</span> seq_high</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">routing_logits_builder</span>(<span class="params">self, batch_size</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.bilinear_type &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 非共享转换矩阵，0初始化路由对数</span></span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>._is_training:</span><br><span class="line">                <span class="comment"># training的时候全部初始化</span></span><br><span class="line">                routing_logits = tf.stop_gradient(tf.zeros([batch_size, <span class="variable language_">self</span>._num_interest, <span class="variable language_">self</span>._max_seq_len]))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 否则就是预估的时候同用户需要tile</span></span><br><span class="line">                routing_logits = tf.zeros([<span class="variable language_">self</span>._num_interest, <span class="variable language_">self</span>._max_seq_len])</span><br><span class="line">                routing_logits = tf.stop_gradient(tf.tile(routing_logits[<span class="literal">None</span>, :, :], [batch_size, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>._is_training:</span><br><span class="line">                routing_logits = tf.stop_gradient(tf.truncated_normal(</span><br><span class="line">                    [batch_size, <span class="variable language_">self</span>._num_interest, <span class="variable language_">self</span>._max_seq_len],</span><br><span class="line">                    stddev=<span class="variable language_">self</span>._routing_logits_stddev))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                routing_logits = tf.constant(</span><br><span class="line">                    np.random.uniform(</span><br><span class="line">                        high=<span class="variable language_">self</span>._routing_logits_stddev,</span><br><span class="line">                        size=[<span class="variable language_">self</span>._num_interest, <span class="variable language_">self</span>._max_seq_len]),</span><br><span class="line">                    dtype=tf.float32)</span><br><span class="line">                routing_logits = tf.stop_gradient(tf.tile(routing_logits[<span class="literal">None</span>, :, :], [batch_size, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        <span class="keyword">return</span> routing_logits</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, seq_feat, seq_lens</span>):</span><br><span class="line">        <span class="comment"># seq_feat padding</span></span><br><span class="line">        cur_batch_max_seq_len = tf.shape(seq_feat)[<span class="number">1</span>]</span><br><span class="line">        seq_feat = tf.cond(</span><br><span class="line">            tf.greater(<span class="variable language_">self</span>._max_seq_len, cur_batch_max_seq_len),</span><br><span class="line">            <span class="keyword">lambda</span>: tf.pad(tensor=seq_feat,</span><br><span class="line">                paddings=[[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="variable language_">self</span>._max_seq_len - cur_batch_max_seq_len], [<span class="number">0</span>, <span class="number">0</span>]],</span><br><span class="line">                name=<span class="string">&#x27;%s/CONSTANT&#x27;</span> % <span class="variable language_">self</span>.name),</span><br><span class="line">            <span class="keyword">lambda</span>: tf.<span class="built_in">slice</span>(seq_feat, [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [-<span class="number">1</span>, <span class="variable language_">self</span>._max_seq_len, -<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">        seq_feat_high = <span class="variable language_">self</span>.seq_feat_high_builder(seq_feat)</span><br><span class="line">        seq_feat_high_stop = tf.stop_gradient(seq_feat_high, name = <span class="string">&quot;%s/seq_feat_high_stop&quot;</span> % <span class="variable language_">self</span>.name)</span><br><span class="line"></span><br><span class="line">        batch_size = tf.shape(seq_lens)[<span class="number">0</span>]</span><br><span class="line">        routing_logits = <span class="variable language_">self</span>.routing_logits_builder(batch_size)</span><br><span class="line"></span><br><span class="line">        num_capsules = tf.maximum(</span><br><span class="line">            <span class="number">1</span>, tf.minimum(<span class="variable language_">self</span>._num_interest, tf.to_int32(tf.log(tf.to_float(seq_lens)))))</span><br><span class="line">        mask = tf.sequence_mask(seq_lens, <span class="variable language_">self</span>._max_seq_len)</span><br><span class="line">        atten_mask = tf.tile(tf.expand_dims(mask, axis=<span class="number">1</span>), [<span class="number">1</span>, <span class="variable language_">self</span>._num_interest, <span class="number">1</span>])</span><br><span class="line">        paddings = tf.zeros_like(atten_mask, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>._num_iters):</span><br><span class="line">            capsule_softmax_weight = tf.nn.softmax(routing_logits, axis=<span class="number">1</span>)</span><br><span class="line">            capsule_softmax_weight = tf.where(tf.equal(atten_mask, <span class="number">0</span>), paddings, capsule_softmax_weight)</span><br><span class="line">            capsule_softmax_weight = tf.expand_dims(capsule_softmax_weight, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i + <span class="number">1</span> &lt; <span class="variable language_">self</span>._num_iters:</span><br><span class="line">                <span class="comment"># stop_gradient内迭代</span></span><br><span class="line">                interest_capsule = tf.matmul(capsule_softmax_weight, seq_feat_high_stop)</span><br><span class="line">                high_capsules = <span class="variable language_">self</span>.squash(interest_capsule)</span><br><span class="line">                delta_routing = tf.matmul(seq_feat_high_stop, tf.transpose(high_capsules, [<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>]))</span><br><span class="line">                delta_routing = tf.reshape(delta_routing, [-<span class="number">1</span>, <span class="variable language_">self</span>._num_interest, <span class="variable language_">self</span>._max_seq_len])</span><br><span class="line">                routing_logits = routing_logits + delta_routing</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                interest_capsule = tf.matmul(capsule_softmax_weight, seq_feat_high)</span><br><span class="line">                high_capsules = <span class="variable language_">self</span>.squash(interest_capsule)</span><br><span class="line">        high_capsules = tf.reshape(high_capsules, [-<span class="number">1</span>, <span class="variable language_">self</span>._num_interest, <span class="variable language_">self</span>._high_dim])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> high_capsules, num_capsules</span><br></pre></td></tr></table></figure></p><p><strong>参考文献</strong><br><a href="https://www.cnblogs.com/DjangoBlog/articles/11777366.html">MIND召回介绍</a><br><a href="https://mp.weixin.qq.com/s?__biz=MzIwODA4NTIxMQ%3D%3D&amp;chksm=9709cd40a07e445669cedc192ae1a17a40604a7701393dd04f77e1b695aba775af7e46d0293c&amp;idx=1&amp;mid=2247484660&amp;scene=21&amp;sn=90a9b07594d3f5cbfef83dfc003a4eff#wechat_redirect">浅谈胶囊网络与动态路由算法</a><br><a href="https://blog.csdn.net/wuzhongqiang/article/details/123696462">AI上推荐 之 MIND(动态路由与胶囊网络的奇光异彩)</a><br><a href="https://www.jianshu.com/p/88e5f4fc3fd7">召回阶段的多兴趣模型——MIND</a><br><a href="https://zhuanlan.zhihu.com/p/497962651">MIND模型(多兴趣)</a></p><hr>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-引言&quot;&gt;&lt;a href=&quot;#1-引言&quot; class=&quot;headerlink&quot; title=&quot;1 引言&quot;&gt;&lt;/a&gt;1 引言&lt;/h2&gt;&lt;p&gt;在深度学习召回算法领域，比较经典的包括了以下2大类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于 &lt;code&gt;item2vec&lt;/code&gt; 模型构建在线的i2i召回；&lt;/li&gt;
&lt;li&gt;基于 &lt;code&gt;user2item&lt;/code&gt; 泛双塔模型构建在线的u2i召回；&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;当然还有2阶以上的召回，&lt;code&gt;i2u2i&lt;/code&gt;、&lt;code&gt;u2u2i&lt;/code&gt;等，在这里不做重点介绍，最终目的都是为了召回 item。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;对于第一种，相信大家比较熟知的有从 &lt;code&gt;word2vec&lt;/code&gt; 衍生出的&lt;code&gt;item2vec&lt;/code&gt;、阿里的&lt;code&gt;deepwalk&lt;/code&gt;以及&lt;code&gt;FM&lt;/code&gt;等，核心方式都是离线构建出 item 的 Embedding，&lt;strong&gt;在online侧基于用户的行为序列，取其中的 item 作为 trigger 来进行倒排/近邻召回&lt;/strong&gt;。&lt;/p&gt;</summary>
    
    
    
    <category term="召回模型" scheme="https://www.xiemingzhao.com/categories/%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="算法总结" scheme="https://www.xiemingzhao.com/categories/%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="召回" scheme="https://www.xiemingzhao.com/tags/%E5%8F%AC%E5%9B%9E/"/>
    
    <category term="MIND" scheme="https://www.xiemingzhao.com/tags/MIND/"/>
    
  </entry>
  
  <entry>
    <title>RankI2I 召回简述</title>
    <link href="https://www.xiemingzhao.com/posts/ranki2imodel.html"/>
    <id>https://www.xiemingzhao.com/posts/ranki2imodel.html</id>
    <published>2022-05-20T16:00:00.000Z</published>
    <updated>2025-04-04T17:46:16.427Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1 背景"></a>1 背景</h2><p>在推荐系统中，<code>i2i</code>类型的召回往往在多路召回中扮演中重要的角色，具有<strong>效率高、覆盖广、可解释、易调控</strong>等优势。常用的算法一般有<code>swing</code>,<code>icf</code>,<code>wbcos</code>以及<code>item2vec</code>等，虽然不同算法逻辑不同，实际上构建的倒排结果往往具有一定的重复，并且多路 i2i 在线上并存往往也会带来维护成本高，迭代效率低等问题。那么，<code>ranki2i</code> 是较为通用的将各种 i2i 有效整合到一起的一种方案。</p><h2 id="2-算法逻辑"><a href="#2-算法逻辑" class="headerlink" title="2 算法逻辑"></a>2 算法逻辑</h2><p><code>ranki2i</code> 算法承载着两个目标：</p><ul><li>合并分散的 i2i 召回；</li><li>提高 i2i 召回效率。</li></ul><p>为了完成上述 2 个目标，相应的转化成以下两个方案：</p><ul><li>构建一个 i2i 预估模型；</li><li>对所有的 i2i 候选 pair 对进行预估构建截断倒排结果。</li></ul><span id="more"></span><h3 id="2-1-样本"><a href="#2-1-样本" class="headerlink" title="2.1 样本"></a>2.1 样本</h3><p>因为最终是构建 i2i 的预估模型，那么重点的数据来源便是 i2i 后验数据，主要有以下 2 种：</p><ol><li>推荐中已有的 i2i 行为日志数据（线上已有 i2i 类召回）；</li><li>相关推荐等场景的 i2i 行为日志（线上无 i2i 召回）。</li></ol><p><strong>一般第 1 类样本更重要，效果要更好，这也符合训练和预估任务一致性的要求</strong>。</p><p>但无论哪一种，我们都可以从对应日志中提取 <code>user-trigger_item-target_item-label</code> 结构的归因样本。也即某个用户看过某个 trigger_item 后，对于其相关的 target_item 的偏好结果 label 是什么。</p><blockquote><p>如此便有了 ranki2i 正样本（曝光点击）和负样本（曝光未点击），但是和其他召回模型类似，只使用曝光的数据来构建样本往往是有偏的。对于负样本我们还需要大量的负采样，一般可以是全局负采样或 in-batch 负采样，具体 hard neg 和 easy neg 占比需要通过实验来调整。</p></blockquote><p>样本除了归因表之外，另一个要素便是特征体系，由于线上是 i2i 召回，那么主要就是构建 item 特征体系，只要是 item 维度的即可，一般可以从以下几个方面着手：</p><ol><li>统计类特征：曝光pv、点击pv、ctr、cvr、price等；</li><li>属性类特征：类目id，品牌id，适用人群，颜色等；</li><li>上下文特征：召回 channel（swing 等），召回分数（i2i 算法 score）等；</li><li>多模态特征：item 的 nlp 文本向量，图片的 cv 预训练向量等。</li></ol><h3 id="2-2-模型"><a href="#2-2-模型" class="headerlink" title="2.2 模型"></a>2.2 模型</h3><p>因为 i2i 线上往往是使用用户的行为序列中的 item 作为 trigger 去倒排中召回 item。那么结合上述的样本结构，我们就需要构建一个由 <code>trigger_item vs target_item</code> 组成的 pair 对样本，模型结构可以选择<code>双塔模型</code>，两边的特征体系往往一模一样，仅仅是为了训练出<strong>用户一般在看了某个 trigger_item 后最有可能还看哪些 target_item</strong>。</p><p>所以模型的结构往往比较简单：</p><ul><li><code>input</code>：trigger_item 和 target_item 的 features；</li><li><code>forward</code>：input 在 concat 后喂入 NN 即可；</li><li><code>loss</code>：构建 ctr loss，多目标的话也可构建多头网络。</li></ul><h3 id="2-3-预估倒排"><a href="#2-3-预估倒排" class="headerlink" title="2.3 预估倒排"></a>2.3 预估倒排</h3><p>在 <code>ranki2i</code> 样本和模型中，主要为了提高 i2i 的效率，那如何将模型的能力转为 i2i 的效果，并起到对分散的各路 i2i 进行合并的作用呢？</p><blockquote><p>一个比较自然的想法：为了确保分散的各路 i2i 候选集都有参与的机会，那么就将 swing 等所有 i2i 的倒排合并去重后，对所有的 pair 对应用上述 i2i 模型进行预估，然后根据预估结果（ctr预估分或各个多目标分）倒序排列后截断。</p></blockquote><p>如此便得到了融合的 i2i 倒排，由于以下2点，往往此路 i2i 能够替换所有分散的 i2i 的效果：</p><ol><li>基于场景内真实 i2i 样本数据训练得到；</li><li>将所有 i2i 倒排融合后进行排序的结果。</li></ol><h3 id="2-4-其他调优"><a href="#2-4-其他调优" class="headerlink" title="2.4 其他调优"></a>2.4 其他调优</h3><p>在实际应用中，一般效率上是有比较明显的效果。并且能够降低 i2i 线上的复杂度，提高优化效率，否则 i2i 分散在多路中，提升效果难以撬动整体。</p><p>针对 ranki2i 本身依然有不少调优策略，这里简单列举几个博主自己实践中的经验：</p><ol><li>模型需要增量更新一段时间，尽可能覆盖 pair 对；</li><li>负采样上，尽量达到曝光负样本的量级；</li><li>预估长尾 pair 对由于训练不充分（甚至没出现过），需要考虑置信度，比如长度阶段后需要进一步分数截断，或者类目调权。</li><li>在线召回的策略迭代就和单路的差不多，例如拆分长短兴趣召回等。</li></ol><hr>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-背景&quot;&gt;&lt;a href=&quot;#1-背景&quot; class=&quot;headerlink&quot; title=&quot;1 背景&quot;&gt;&lt;/a&gt;1 背景&lt;/h2&gt;&lt;p&gt;在推荐系统中，&lt;code&gt;i2i&lt;/code&gt;类型的召回往往在多路召回中扮演中重要的角色，具有&lt;strong&gt;效率高、覆盖广、可解释、易调控&lt;/strong&gt;等优势。常用的算法一般有&lt;code&gt;swing&lt;/code&gt;,&lt;code&gt;icf&lt;/code&gt;,&lt;code&gt;wbcos&lt;/code&gt;以及&lt;code&gt;item2vec&lt;/code&gt;等，虽然不同算法逻辑不同，实际上构建的倒排结果往往具有一定的重复，并且多路 i2i 在线上并存往往也会带来维护成本高，迭代效率低等问题。那么，&lt;code&gt;ranki2i&lt;/code&gt; 是较为通用的将各种 i2i 有效整合到一起的一种方案。&lt;/p&gt;
&lt;h2 id=&quot;2-算法逻辑&quot;&gt;&lt;a href=&quot;#2-算法逻辑&quot; class=&quot;headerlink&quot; title=&quot;2 算法逻辑&quot;&gt;&lt;/a&gt;2 算法逻辑&lt;/h2&gt;&lt;p&gt;&lt;code&gt;ranki2i&lt;/code&gt; 算法承载着两个目标：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;合并分散的 i2i 召回；&lt;/li&gt;
&lt;li&gt;提高 i2i 召回效率。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了完成上述 2 个目标，相应的转化成以下两个方案：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;构建一个 i2i 预估模型；&lt;/li&gt;
&lt;li&gt;对所有的 i2i 候选 pair 对进行预估构建截断倒排结果。&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="召回模型" scheme="https://www.xiemingzhao.com/categories/%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="算法总结" scheme="https://www.xiemingzhao.com/categories/%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="召回" scheme="https://www.xiemingzhao.com/tags/%E5%8F%AC%E5%9B%9E/"/>
    
    <category term="RankI2I" scheme="https://www.xiemingzhao.com/tags/RankI2I/"/>
    
  </entry>
  
  <entry>
    <title>wbcos 召回</title>
    <link href="https://www.xiemingzhao.com/posts/wbcosrecall.html"/>
    <id>https://www.xiemingzhao.com/posts/wbcosrecall.html</id>
    <published>2022-05-08T16:00:00.000Z</published>
    <updated>2025-04-04T17:48:13.089Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1 背景"></a>1 背景</h2><blockquote><p><code>wb</code> 意为 <code>weight base</code>，wbcos 即加权式的 cos。</p></blockquote><p><strong>思想：其实就是改进的 itemcos 来计算相似度。</strong></p><p>核心在于两点：</p><ul><li>user+session 内的 pair 重复出现的时候如何聚合，主要就是时间衰减和类目等维度加权；</li><li>user+session 间的 pair 如何聚合，主要是 session 丰富度加权；</li></ul><span id="more"></span><h2 id="2-步骤"><a href="#2-步骤" class="headerlink" title="2 步骤"></a>2 步骤</h2><h3 id="step0-样本构造"><a href="#step0-样本构造" class="headerlink" title="step0: 样本构造"></a>step0: 样本构造</h3><p>将用户在 app 全场景的正行为汇总到一起，作为底表 <code>user_action_database</code>。</p><p><em>注意可筛选行为数较多或者较少的，例如：正反馈item个数在[a,b]之间；以及高质量用户，例如经验：在近 n 天内有 order 的用户，以及用户当天点击数不少于 k 等等</em></p><p>保留 <code>user,event,time,session,item,cate,brand</code> 等等维度。</p><h3 id="step1-计算bw"><a href="#step1-计算bw" class="headerlink" title="step1: 计算bw"></a>step1: 计算<code>bw</code></h3><p>在 <code>user+session</code> 维度下，计算：</p><script type="math/tex; mode=display">userBw = \frac{1}{log_2 (3 + itemNum)}</script><p>其中 <code>itemNum</code> 指的是 user 在 session 内的正反馈 item 的去重个数。</p><p><strong>这里的思想很简单：即一个 user 在一个 session 维度下，看的 item 越多，理论上兴趣分布越广，则权重越小；从概率学角度理解集合元素越多，产生某 pair 对的概率越大，分得的权重也越小</strong></p><h3 id="step2-计算-item-的-wb"><a href="#step2-计算-item-的-wb" class="headerlink" title="step2: 计算 item 的 wb"></a>step2: 计算 item 的 <code>wb</code></h3><p>在 user+session 维度下，计算同一 item 的出现次数 <code>itemCnt</code>，截断后作为 <code>itemWb</code>：</p><script type="math/tex; mode=display">itemWb = min(m, itemCnt)</script><p><strong>注意，这里截断 m 是为了后续取 pair 对 topk 时间相近，思想就是：在找出与当前 itemA 行为最近的 itemB 的时候，后者有多次出现的话最多取 m 个（时间最近的）来构建 pair，m 具体需要根据业务数据来确定</strong></p><p>同时计算类目等维度的权重系数 <code>ratio</code>，这里以类目 cate 为例。</p><blockquote><p>即 <code>user_id,session_id</code> 维度下，每个 item_id 对应的类目权重。每个类目的权重可以参考：</p></blockquote><script type="math/tex; mode=display">ratio_k = \frac{cnt(cate_k)}{\sum_i cnt(cate_i)}</script><h3 id="step3-计算-item-pair-相关参数"><a href="#step3-计算-item-pair-相关参数" class="headerlink" title="step3: 计算 item pair 相关参数"></a>step3: 计算 item pair 相关参数</h3><p>在 <code>user+session</code> 维度下，构建 item 的 pair 对，可以设置 item 不同的时候才成 pair，于是每个 pair 就有两个 item，我们记为<code>(litem, ritem)</code>。</p><p>紧接着对每个pair对计算参数 <code>timeGap</code> 和 <code>matchRatio</code>。</p><script type="math/tex; mode=display">timeGap = e^{- \alpha * abs(tsDiff)}</script><p>其中:</p><ul><li>$\alpha$ 是时间衰减的超参数，经验上可取 0.75。</li><li><code>tsDiff</code> 表示的是pair对中的两个正反馈 item 的行为发生时间差，建议使用 hour 的精度。</li></ul><p><code>matchRatio</code>的计算需要融入先验信息，我们以简单的cate维度为例：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">matchRatio = </span><br><span class="line">case </span><br><span class="line">when 叶子类目相同 then 1</span><br><span class="line">when 二级类目相同 then 0.9</span><br><span class="line">when 一级类目相同 then 0.8</span><br><span class="line">else 0.3</span><br></pre></td></tr></table></figure><br>可以看得出来，此处是 pair 对中两个 item 的 cate 维度越相同，则先验相关性越高。此处，可以融入其他的先验信息，例如 brand，price 等。</p><h3 id="step4-统计-pair-对的两种频次"><a href="#step4-统计-pair-对的两种频次" class="headerlink" title="step4: 统计 pair 对的两种频次"></a>step4: 统计 pair 对的两种频次</h3><p>首先，统计每一个pair对<code>(litem, ritem)</code>全局的频次，记为<code>pairCnt</code>，并且可以以此筛选除去总出现次数较少的 pair 对，例如<code>pairCnt&gt;=5</code>。</p><p>其次，计算每一个pair对<code>(litem, ritem)</code>在全局有多少个 <code>user+session</code> 出现了，即以 <code>user+session</code> 为 key 去 groupby，来计算 <code>count(distinct user,session)</code>，我们记为 <code>pairUserSessionCnt</code>。</p><blockquote><p><em>这里有些 tf-idf 的思想。</em></p></blockquote><h3 id="step5-计算innerProduct参数。"><a href="#step5-计算innerProduct参数。" class="headerlink" title="step5: 计算innerProduct参数。"></a>step5: 计算<code>innerProduct</code>参数。</h3><p>以上三个参数的计算都是在<code>user+session+pair(litem, ritem)</code>维度之下的，我们记为<code>基准维度</code>。</p><h4 id="首先，计算timeGapWeight"><a href="#首先，计算timeGapWeight" class="headerlink" title="首先，计算timeGapWeight"></a>首先，计算<code>timeGapWeight</code></h4><p>我们知道在<code>基准维度</code>下，<code>matchRatio,pairCnt,pairUserSessionCnt</code>都是一致的，但是同一 pair 对会出现多次，每个 pair 对我们在前面计算过<code>timeGap</code>。而每个 pair 对<code>(litem,ritem)</code>都有自己的<code>itemWb</code>.</p><p>于是我们可以如下计算：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">k = litemWb * ritemWb;</span><br><span class="line">在基准维度下，对重复出现的 pair 对的 timeGap（记为timeGaps）取前k个，即：</span><br><span class="line">timeGaps.sort(reverse = True);</span><br><span class="line">最终，timeGapWeight = sum(timeGaps[:k])</span><br></pre></td></tr></table></figure></p><h4 id="然后，我们计算innerProduct"><a href="#然后，我们计算innerProduct" class="headerlink" title="然后，我们计算innerProduct"></a>然后，我们计算<code>innerProduct</code></h4><p>这个就比较简单了，也是在去重后的<code>基准维度</code>上进行计算：</p><script type="math/tex; mode=display">innerProduct = matchRatio * timeGapWeight * lratio * rratio</script><p>其中，<code>lratio</code> 和 <code>rratio</code> 分别是 step3 中计算的左右 item 的类目权重（可选）。</p><p><strong>这里其实可以理解为，同一 user+session 下，多次共现的 pair 对（matchRatio一致），按照其时间间隔权重来加权，两个 ratio 是按照对应类目集中度来加权（可选）</strong>。</p><p>到这里，我们应该在<code>基准维度</code>（<code>user+session+pair(litem, ritem)</code>）下获得了以下特征数据(<strong>此处已经去重了</strong>)：</p><blockquote><p>matchRatio: 匹配度<br>timeGapWeight: 时间间隔权重<br>innerProduct: 内积权重<br>pairCnt: 全局pair的计数<br>pairUserSessionCnt: 出现对应pair的UserSession计数<br>litemWb: pair对中左item的wb<br>ritemWb: pair对右左item的wb<br>lratio: pair对中左item的类目权重ratio<br>rratio: pair对右左item的类目权重ratio<br>userBw: user+session级别的bw</p></blockquote><h3 id="step6-计算pair对的三个参数"><a href="#step6-计算pair对的三个参数" class="headerlink" title="step6: 计算pair对的三个参数"></a>step6: 计算pair对的三个参数</h3><p>首先，我们基于 step5 计算 <code>pairBw</code>：</p><script type="math/tex; mode=display">pairBw = innerProduct * userBw^2</script><p>接着计算 pair 对中左右 item 的加权 wb（item 对应的 userBw）：</p><script type="math/tex; mode=display">leftWb = litemWb * userBw</script><script type="math/tex; mode=display">rightWb = ritemWb * userBw</script><h3 id="step7-计算itemWbLen并聚合-pair-对"><a href="#step7-计算itemWbLen并聚合-pair-对" class="headerlink" title="step7: 计算itemWbLen并聚合 pair 对"></a>step7: 计算<code>itemWbLen</code>并聚合 pair 对</h3><p>首先，我们之前通过截断<code>itemCnt</code>，作为<code>itemWb</code>，在这里我们不再需要<code>基准维度</code>，我们聚合到 pair 维度，以左 item 为 key 聚合出左右 item 的<code>itemWbLen</code>:</p><script type="math/tex; mode=display">litemWbLen = \sqrt {sum(leftWb^2)}</script><script type="math/tex; mode=display">ritemWbLen = \sqrt {sum(rightWb^2)}</script><p>其次，聚合pair对。<br>我们聚合全局的 pair 对，并计算下列参数：</p><script type="math/tex; mode=display">pairBwScore = sum(pairBw)</script><h3 id="step8-计算最终wbScore"><a href="#step8-计算最终wbScore" class="headerlink" title="step8: 计算最终wbScore"></a>step8: 计算最终<code>wbScore</code></h3><p>最后我们将基于pair对维度计算：</p><script type="math/tex; mode=display">wbScore = \frac{pairBwScore}{litemWbLen * ritemWbLen}</script><p>其中，<code>litemWbLen</code>和<code>ritemWbLen</code>分别是 pair 对中左右 item 的<code>itemWbLen</code>值。</p><h2 id="3-后记"><a href="#3-后记" class="headerlink" title="3 后记"></a>3 后记</h2><p><strong>1. 聚合</strong><br>在实际中应用的时候，往往每个分区生产一张 wbcos 分区结果表，我们可以进行多分区维度的<strong>聚合</strong>来减少方差从而提高准确度：<br>一般就是采用如下更新方式</p><script type="math/tex; mode=display">wbScore = \frac{prePairBw + curPairBw}{(prelitemWbLen + curlitemWbLen) + (preritemWbLen + curritemWbLen)}</script><p>其实就是利用多窗口的数据进行指标平滑的思想。或者可以进行滑动平均，比如：</p><script type="math/tex; mode=display">wbScore = \alpha * wbScore_{pre} + (1 - \alpha) * wbScore_{cur}</script><p><strong>2. 计算</strong><br>因为 i2i 召回逻辑上具有对称性，在构建 pair 时，只需要构建单向 pair 对 $(i1, i2)$ 即可。最终构建倒排时，反向 pair 对 $(i2, i1)$ 可以使用同样的相似度分，以减少计算量。</p><p><strong>3. 不同行为的融合</strong><br>在操作中，如何考虑所有的正行为，除<code>clk</code>之外，例如<code>fav</code>和<code>buy</code>等。那么对于不同行为之间的 pair 对就可以采取不一样的操作。主要是 session 内合并的时候所用的权重，在计算<code>innerProduct</code>的时候，<code>timeGapWeight</code>可以在不同行为对之间使用不用的权重。</p><hr>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-背景&quot;&gt;&lt;a href=&quot;#1-背景&quot; class=&quot;headerlink&quot; title=&quot;1 背景&quot;&gt;&lt;/a&gt;1 背景&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;wb&lt;/code&gt; 意为 &lt;code&gt;weight base&lt;/code&gt;，wbcos 即加权式的 cos。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;思想：其实就是改进的 itemcos 来计算相似度。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;核心在于两点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;user+session 内的 pair 重复出现的时候如何聚合，主要就是时间衰减和类目等维度加权；&lt;/li&gt;
&lt;li&gt;user+session 间的 pair 如何聚合，主要是 session 丰富度加权；&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="召回模型" scheme="https://www.xiemingzhao.com/categories/%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="算法总结" scheme="https://www.xiemingzhao.com/categories/%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="召回" scheme="https://www.xiemingzhao.com/tags/%E5%8F%AC%E5%9B%9E/"/>
    
    <category term="wbcos" scheme="https://www.xiemingzhao.com/tags/wbcos/"/>
    
  </entry>
  
  <entry>
    <title>Swing 召回</title>
    <link href="https://www.xiemingzhao.com/posts/swingrecall.html"/>
    <id>https://www.xiemingzhao.com/posts/swingrecall.html</id>
    <published>2022-04-23T16:00:00.000Z</published>
    <updated>2025-04-04T17:47:47.447Z</updated>
    
    <content type="html"><![CDATA[<p><strong>思想：来源于传统的 CF：</strong></p><ul><li>如果多个 user 都只共同点了 i1 和 i2，那么其一定是强关联的，这种关联是通过用户来传递的；</li><li>如果两个 user pair 对之间构成的 swing 结构越多，则每个结构越弱，在这个 pair 对上每个节点分到的权重越低。</li></ul><h2 id="1-原理"><a href="#1-原理" class="headerlink" title="1 原理"></a>1 原理</h2><p><code>Swing</code>意为摇摆或者秋千，它是基于图结构的一种实时推荐算法。主要公式为：</p><script type="math/tex; mode=display">Sim(i, j) = \sum_{u \in U_i \cup U_j} \sum_{v \in U_i \cup U_j} \frac{1}{\alpha + |I_u \cup I_v|}</script><p>结合前面的思想，公式表达的就是为了衡量物品 i 和 j 的<code>相似性</code>：<strong>考察都购买了物品 i 和 j 的用户 u 和 v， 如果这两个用户共同购买的物品越少，则物品 i 和 j 的相似性越高</strong>。</p><span id="more"></span><p>极端情况下，两个用户都购买了某个物品，且两个用户所有购买的物品中，共同购买的物品只有这两个，说明这两个用户兴趣差异非常大，然而却同时购买了这两个物品，则说明这两个物品相似性非常大！</p><p><strong>区别</strong>：<br><code>icf</code>：如果喜欢两个物品的交集用户越多，那么这两个物品间的相似度越高。<br><code>swing</code>：如果同时喜欢两个物品的用户越多，且这些用户之间的重合度越低，那么这两个物品间的相似度越高。</p><!--more--><h2 id="2-计算步骤"><a href="#2-计算步骤" class="headerlink" title="2 计算步骤"></a>2 计算步骤</h2><p>在实际中计算的时候主要分为以下3步：</p><h3 id="step0-计算用户的权重-w-u"><a href="#step0-计算用户的权重-w-u" class="headerlink" title="step0:计算用户的权重$w_u$"></a>step0:计算用户的权重$w_u$</h3><script type="math/tex; mode=display">w_u = \frac{1}{(clkcnt + k)^{\alpha}}</script><p>其中：</p><ul><li>$k$：平滑作用的超参数，可根据具体业务效果确定，比如 5；</li><li>$\alpha$：权重因子，越大对高活用户降权越狠，常用 0.35。</li></ul><p>主要为了通过用户的行为数来衡量兴趣的分散度，从而给定用户行为 item 的权重。</p><blockquote><p><strong>注意在统计 clkcnt 的时候是不去重的。</strong></p></blockquote><h3 id="step1-计算用户pair的权重"><a href="#step1-计算用户pair的权重" class="headerlink" title="step1:计算用户pair的权重"></a>step1:计算用户pair的权重</h3><p>对于点击同 item 的每个 <code>(u1,u2)</code> 的 pair 对，其权重系数：</p><script type="math/tex; mode=display">w_{pair} = w_{u1} * w_{u2}</script><h3 id="stpe2-计算相似度"><a href="#stpe2-计算相似度" class="headerlink" title="stpe2:计算相似度"></a>stpe2:计算相似度</h3><p>对于有被 <code>(u1,u2)</code> 用户 pair 对共同正反馈的 item pair 对<code>(i,j)</code>来说，其相似分：</p><script type="math/tex; mode=display">Sim(i, j) = \sum_{pair}\frac{w_{pair}}{1+intersection}</script><p>其中 <code>intersection</code> 代表每一个用户 pair 对<code>(u1,u2)</code>的共同正行为 item 个数。</p><blockquote><p>此处，分母中的 1 也是一个可调参数。</p></blockquote><p>并且，分母中可以再引入一些权重参数，例如：</p><ol><li>两个用户对左右 item pair 的类目权重；（session内 当前类目点击数/全部类目点击数）</li><li>user 与 item 之间的连接数作为权重；</li></ol><h2 id="3-相关经验"><a href="#3-相关经验" class="headerlink" title="3 相关经验"></a>3 相关经验</h2><p>这里披露部分实践中可能需要注意的模块。</p><p><strong>1. 用户的筛选</strong><br>主要为了保障数据的置信度，需要选择相对正常且行为有参考价值的用户。比如，行为数过少/过多的剔除，当然具体操作还是要结合业务逻辑。</p><p><strong>2. 物料的筛选</strong><br>也是出于同样的目的，当然这里是否操作的影响可能小一些。比如，物料的 pair 对过少的是否排除。</p><p><strong>3. 简化计算</strong><br>因为这里的 pair 是双向等价的，即 $(i1,i2)$ 和 $(i2,i1)$ 的相似度应该的相同。所以不论是 user 还是 item 的 pair 对，往往只需要构建单向的，最后构建倒排的时候，所有 $(i2,i1)$ 复用 $(i1,i2)$ 的相似度即可。</p><p><strong>4. 增量更新</strong><br>与很多召回算法一样，不论更新频率是多少（by day/hour 等），都需要考虑和历史数据合并的问题。这里也是类似的，可以将所有的 pair 先做增量合并，然后按照时间步数衰减合并新老相似度分，分数过低的可以选择截断淘汰。</p><p><strong>参考文章：</strong><br><a href="https://www.jianshu.com/p/a5d46cdc2b4e">【召回】swing 算法</a><br><a href="https://blog.csdn.net/weixin_46838716/article/details/126138597">一文看懂推荐系统：召回02：Swing 模型</a></p><hr>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;思想：来源于传统的 CF：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果多个 user 都只共同点了 i1 和 i2，那么其一定是强关联的，这种关联是通过用户来传递的；&lt;/li&gt;
&lt;li&gt;如果两个 user pair 对之间构成的 swing 结构越多，则每个结构越弱，在这个 pair 对上每个节点分到的权重越低。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;1-原理&quot;&gt;&lt;a href=&quot;#1-原理&quot; class=&quot;headerlink&quot; title=&quot;1 原理&quot;&gt;&lt;/a&gt;1 原理&lt;/h2&gt;&lt;p&gt;&lt;code&gt;Swing&lt;/code&gt;意为摇摆或者秋千，它是基于图结构的一种实时推荐算法。主要公式为：&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;Sim(i, j) = &#92;sum_{u &#92;in U_i &#92;cup U_j} &#92;sum_{v &#92;in U_i &#92;cup U_j} &#92;frac{1}{&#92;alpha + |I_u &#92;cup I_v|}&lt;/script&gt;&lt;p&gt;结合前面的思想，公式表达的就是为了衡量物品 i 和 j 的&lt;code&gt;相似性&lt;/code&gt;：&lt;strong&gt;考察都购买了物品 i 和 j 的用户 u 和 v， 如果这两个用户共同购买的物品越少，则物品 i 和 j 的相似性越高&lt;/strong&gt;。&lt;/p&gt;</summary>
    
    
    
    <category term="召回模型" scheme="https://www.xiemingzhao.com/categories/%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="算法总结" scheme="https://www.xiemingzhao.com/categories/%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="召回" scheme="https://www.xiemingzhao.com/tags/%E5%8F%AC%E5%9B%9E/"/>
    
    <category term="swing" scheme="https://www.xiemingzhao.com/tags/swing/"/>
    
  </entry>
  
  <entry>
    <title>推荐模型中的 position bias 和 debias</title>
    <link href="https://www.xiemingzhao.com/posts/biasnet.html"/>
    <id>https://www.xiemingzhao.com/posts/biasnet.html</id>
    <published>2022-03-26T16:00:00.000Z</published>
    <updated>2025-04-02T17:11:05.137Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>在推荐系统中一个重要的任务就是 CTR 建模，其本质的思想便是<strong>预估 user 对 item 的点击率</strong>。但是实际中获取的样本往往是在一定条件（时间、机型、位置等）下的后验结果，所以使得建模的 Label 往往是夹杂了这些因素的结果。</p><p>这些影响后验结果的因素一般称为 <code>偏置（bias）项</code>，而去除这些偏置项的过程就称为 <code>消偏（debias）</code>。在这其中最重要的便是 <code>位置偏置（position bias）</code>，即 item 展示在不同位置会有不同的影响，且用户往往更偏向点击靠前的位置。本文将重点介绍业界在 <code>position bias</code> 消除上的一般做法和相关经验。</p><h2 id="2-Position-Bias"><a href="#2-Position-Bias" class="headerlink" title="2 Position Bias"></a>2 Position Bias</h2><p>看下面的图，是笔者实际工作场景中部分位置的 CTR 趋势图。可以明显地看到：</p><ul><li>呈现每 20 个 position 位一个周期；每刷请求的个数是 20.</li><li>周期内位置越靠前，CTR 越大；靠前效率高，用户更偏好点靠前的。</li></ul><span id="more"></span><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/biasnet0.png" alt="biasnet0"></p><p>在华为的研究中也论证了用户对 position 靠前的偏好。固定 item 在不同 position 的 CTR 和不固定 item 的趋势差别较为显著。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/biasnet1.png" alt="biasnet1"></p><h2 id="3-Position-Debias—特征法"><a href="#3-Position-Debias—特征法" class="headerlink" title="3 Position Debias—特征法"></a>3 Position Debias—特征法</h2><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/biasnet2.png" alt="biasnet2"></p><p>比较朴素的想法，便是在特征体系中引入 position, 如上图所示。</p><ul><li>模型 <code>offline training</code> 的时候，把 position 作为特征输入模型，让模型学习 position 带来的后验影响。</li><li>而在 <code>online infer</code> 的时候，并没有 position 这样后验的信息，往往可以选择填充一个默认值，比如 0。</li></ul><p><strong>注意：具体填什么也需要测试，不同默认值的结果差别还不小。</strong></p><h2 id="4-Position-Debias—Shallow-Tower"><a href="#4-Position-Debias—Shallow-Tower" class="headerlink" title="4 Position Debias—Shallow Tower"></a>4 Position Debias—Shallow Tower</h2><p>此方法核心是：<strong>构建一个 Shallow Tower 来预估 Position Debias。</strong></p><p>方法来源是 Youtube 发表在 RecSys 2019上的文章：<a href="https://daiwk.github.io/assets/youtube-multitask.pdf">Recommending What Video to Watch Next: A Multitask Ranking System</a></p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/biasnet3.png" alt="biasnet3"></p><p>如上图所示，文章阐述在 <code>position debias</code> 上的做法是:</p><ul><li>保持原来的主模型(main model)不变</li><li>新增一个专门拟合 position bias 的浅层网络(shallow tower)</li><li>将 main model 和 shallow tower 的 logit 相加再过 sigmoid 层后构建 loss。</li></ul><p>其中，<code>shallow tower</code> 的输入主要包含 <code>position feature</code>, <code>device info</code> 等会带来 bias 的特征，而加入 device info 的原因是<em>在不同的设备上会观察到不同的位置偏差</em>。</p><blockquote><p>注意：文章提到在 training 的时候，<strong>position 的特征会应用 10% 的 drop-out</strong>，目的是为了防止模型过度依赖 position 特征。在 online infer 的时候，由于没有后验的 position 特征，<strong>直接丢掉 shallow tower 即可</strong>。</p></blockquote><p>在文章中，披露了模型训练结果提取出的 position bias，如下图所示，可以看到随之位置的增长，bias 越大。因为越靠后，用户更有可能看不到。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/biasnet4.png" alt="biasnet4"><br><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/biasnet5.png" alt="biasnet5"></p><blockquote><p>实际上，bias 还可以拓展更多的特征，包括 user 和 item 侧的属性，具体如何还需依赖对业务的理解和实验。</p></blockquote><h2 id="5-Position-Debias—PAL"><a href="#5-Position-Debias—PAL" class="headerlink" title="5 Position Debias—PAL"></a>5 Position Debias—PAL</h2><p>此方法核心是：<strong>将 position bias 从后验点击概率中拆出来，看作是用户看到的概率。</strong></p><p>方法来源是华为发表在 RecSys 2019上的文章：<a href="https://dl.acm.org/doi/abs/10.1145/3298689.3347033">PAL: A Position-bias Aware Learning Framework for CTR Prediction in Live Recommender Systems</a></p><script type="math/tex; mode=display">p(y = 1|x, pos) = p(seen|x, pos) p(y = 1|x, pos, seen)</script><p>如上公式，作者将后验点击概率拆成了2个条件概率的乘积：</p><ul><li>Item 被用户看到的概率</li><li>用户看到 item 后，再点击的概率</li></ul><p>那么可以进一步假设：</p><ul><li>用户是否看到 item 只跟位置有关系</li><li>用户看到 item 后，是否点击 item 与位置无关</li></ul><script type="math/tex; mode=display">p(y = 1|x, pos) = p(seen|pos) p(y = 1|x, seen)</script><p>基于上述假设，就可以建模如下：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/biasnet6.png" alt="biasnet6"></p><p>如上图所示，其中：</p><ul><li><code>ProbSeen</code>： 是预估广告被用户看到的概率</li><li><code>pCTR</code>：是用户看到 item 后，点击的概率</li></ul><p>可以看到与 YouTube 做法的<strong>区别主要有2点：bias net 和 main model 都先过激活层；然后两边的值再相乘。</strong></p><p>最后 loss 是两者的结合：</p><script type="math/tex; mode=display">L(\theta_{ps},\theta_{pCTR}) = \frac{1}{N}\sum_{i = 1}^Nl(y_i,vCTR_i)) = \frac{1}{N}\sum_{i = 1}^N l(y_i,ProbSeen_i \times pCTR_i))</script><p>在 online infer 的时候，也是类似地丢掉 position 相关的 ProbSeen 的网络，只保留 pCTR 部分即可。</p><h2 id="6-拓展思考"><a href="#6-拓展思考" class="headerlink" title="6 拓展思考"></a>6 拓展思考</h2><h3 id="6-1-假设是否成立？"><a href="#6-1-假设是否成立？" class="headerlink" title="6.1 假设是否成立？"></a>6.1 假设是否成立？</h3><p>两种主流的做法都是将 position 等可能造成 bias 影响的信息单独构建 <code>bias net</code>，然后与 <code>main model</code> 进行融合。<br>但是，</p><blockquote><p><em>Position 带来的 bias 是否可以独立于 main model 进行建模？</em><br><em>用户是否看到是否可以简化为只与 position 相关？</em><br><em>Bias net 的作用是否可以简化为与主塔结果的相加再激活/先激活再乘积？</em></p></blockquote><p>上述问题也许没有标准答案。实际上，笔者在实际中还做了另一种方案，即真的只将结果看成 bias 项，那么就简单的与主网络相加即可，实际上结果也不差。为了控制值域依然在 (0,1) 从而不影响 loss 的构建，最终输出变成：</p><script type="math/tex; mode=display">p(y=1|x,pos) = \frac{1}{2}(p(seen|pos) + p(y = 1|x, seen))</script><h3 id="6-2-pCTR-的分布问题"><a href="#6-2-pCTR-的分布问题" class="headerlink" title="6.2 pCTR 的分布问题"></a>6.2 pCTR 的分布问题</h3><p>容易发现，无论哪种 bias net 的融合方式，最后 loss 所使用的 pCTR 已经发生了变化，而在 online 阶段去除 bias net 部分后，保留的 main tower 对应的输出 pCTR 的分布必然会发生变化。最明显的表现就是 <strong>pcoc（sum(clk)/sum(pCTR）将会偏离 1 附近</strong>。</p><p>而这带来的影响就是：</p><blockquote><p>如果后排和重排中使用到 pCTR 的时候，就会出现含义偏离，会带来一些连锁效应，并且不利于数据分析。当然，有的系统可能没有这个要求。</p></blockquote><p>对于这个问题，笔者试过一些缓解方案：</p><ul><li>增加辅助 loss：比如主网络的 pCTR 也增加一个 logloss 来修正齐 pcoc</li><li>增加 pcoc 正则：针对主网络的 pCTR 新增一个 pcoc 偏离 1 的惩罚项，类似于正则的思想</li><li>矫正结果分值：统计主网络输出偏离期望的比例，直接将输出结果根据该值进行矫正即可</li></ul><p>从效果上来说：</p><ul><li>辅助 loss 和正则的方式确实有助于改善 pcoc，但往往也会影响效果，毕竟梯度被分散了；</li><li>矫正的方式最明显，但是会面临校正系数变化的问题。</li></ul><h3 id="6-3-业务效果"><a href="#6-3-业务效果" class="headerlink" title="6.3 业务效果"></a>6.3 业务效果</h3><p>在推荐系统中，一切脱离实验业务效果的优化，往往都不够坚挺。笔者主要在电商推荐领域内，那么这里给出的经验也仅仅针对此类做参考，不一定具有普适性。</p><p>笔者主要实验了 PAL 和 $\frac{1}{2}(p(seen|pos) + p(y = 1|x, seen))$ 的方式，并且都做了预估分值矫正。离线上 auc 往往会有微幅提升。线上的 CTR 和 IPV 一般会有一定涨幅，在笔者的实验中 +1% 左右。<br>但是，一些体验指标变差了，比如负反馈率、猎奇品的占比等。综合分析下来，click 主要涨在猎奇、标题党等低质品的流量增加上，是不利于系统健康的，于是最终实验没有推全。当然，如果是 UGC 或者 Ads 类业务可能会是另一个逻辑，所以仅供参考。</p><p><strong>参考文章</strong><br><a href="https://zhuanlan.zhihu.com/p/342905546">推荐生态中的bias和debias</a><br><a href="https://tech.meituan.com/2021/06/10/deep-position-wise-interaction-network-for-ctr-prediction.html">SIGIR 2021 | 广告系统位置偏差的CTR模型优化方案</a><br><a href="https://zhuanlan.zhihu.com/p/420373594">推荐系统中的bias&amp;&amp;debias(二)：position bias的消偏</a><br><a href="https://daiwk.github.io/assets/youtube-multitask.pdf">Recommending What Video to Watch Next: A Multitask Ranking System</a><br><a href="https://dl.acm.org/doi/abs/10.1145/3298689.3347033">PAL: a position-bias aware learning framework for CTR prediction in live recommender systems</a><br><a href="https://arxiv.org/pdf/2010.03240.pdf">Bias and Debias in Recommender System: A Survey and Future Directions</a></p><hr>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-引言&quot;&gt;&lt;a href=&quot;#1-引言&quot; class=&quot;headerlink&quot; title=&quot;1 引言&quot;&gt;&lt;/a&gt;1 引言&lt;/h2&gt;&lt;p&gt;在推荐系统中一个重要的任务就是 CTR 建模，其本质的思想便是&lt;strong&gt;预估 user 对 item 的点击率&lt;/strong&gt;。但是实际中获取的样本往往是在一定条件（时间、机型、位置等）下的后验结果，所以使得建模的 Label 往往是夹杂了这些因素的结果。&lt;/p&gt;
&lt;p&gt;这些影响后验结果的因素一般称为 &lt;code&gt;偏置（bias）项&lt;/code&gt;，而去除这些偏置项的过程就称为 &lt;code&gt;消偏（debias）&lt;/code&gt;。在这其中最重要的便是 &lt;code&gt;位置偏置（position bias）&lt;/code&gt;，即 item 展示在不同位置会有不同的影响，且用户往往更偏向点击靠前的位置。本文将重点介绍业界在 &lt;code&gt;position bias&lt;/code&gt; 消除上的一般做法和相关经验。&lt;/p&gt;
&lt;h2 id=&quot;2-Position-Bias&quot;&gt;&lt;a href=&quot;#2-Position-Bias&quot; class=&quot;headerlink&quot; title=&quot;2 Position Bias&quot;&gt;&lt;/a&gt;2 Position Bias&lt;/h2&gt;&lt;p&gt;看下面的图，是笔者实际工作场景中部分位置的 CTR 趋势图。可以明显地看到：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;呈现每 20 个 position 位一个周期；每刷请求的个数是 20.&lt;/li&gt;
&lt;li&gt;周期内位置越靠前，CTR 越大；靠前效率高，用户更偏好点靠前的。&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="精排模型" scheme="https://www.xiemingzhao.com/categories/%E7%B2%BE%E6%8E%92%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="算法总结" scheme="https://www.xiemingzhao.com/categories/%E7%B2%BE%E6%8E%92%E6%A8%A1%E5%9E%8B/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="精排" scheme="https://www.xiemingzhao.com/tags/%E7%B2%BE%E6%8E%92/"/>
    
    <category term="debias" scheme="https://www.xiemingzhao.com/tags/debias/"/>
    
  </entry>
  
  <entry>
    <title>YouTubeDNN 和 WCE</title>
    <link href="https://www.xiemingzhao.com/posts/youtubednn.html"/>
    <id>https://www.xiemingzhao.com/posts/youtubednn.html</id>
    <published>2021-12-17T16:00:00.000Z</published>
    <updated>2025-04-04T10:13:21.906Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1 背景"></a>1 背景</h2><p>这是一篇推荐算法领域经典的论文，它由 YouTube 在2016年发表在 RecSys 上的文章<a href="https://dl.acm.org/doi/pdf/10.1145/2959100.2959190">Deep Neural Networks for YouTube Recommendations</a>。<br>这篇文章是诸多推荐算法工程师的必学经典，可能很多人多次重读都会有新的思考，本文也重点总结文章的核心内容与一些实战经验的思考。</p><h2 id="2-原理"><a href="#2-原理" class="headerlink" title="2 原理"></a>2 原理</h2><p>首先便是其展示的系统链路示意图，这块与大多主流方案没有什么区别。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/youtubednn0.png" alt="youtubednn0"></p><p>论文分别介绍了在 <code>recall</code> 和 <code>ranking</code> 两个模块的方案，但可以说，recall 部分的重要性远大于 ranking。就此文章发表后的几年而言，<em>recall 往往还在工业界主流召回的候选方案中，但 ranking 的方案基本已经成为历史，很少再使用了</em>，不过其思想还是值得学习的。</p><span id="more"></span><h3 id="2-1-recall"><a href="#2-1-recall" class="headerlink" title="2.1 recall"></a>2.1 recall</h3><blockquote><p>任务目标：预测用户下一个观看的视频（next watch），一个多分类问题。</p></blockquote><script type="math/tex; mode=display">P(w_t=i|U,C)=\frac{e^{v_i,u}}{\sum_{j \in V} e^{v_j,u}}</script><p>这里先上模型结构，如下图所示。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/youtubednn1.png" alt="youtubednn1"></p><p><strong>特征</strong></p><ul><li>用户历史看的视频序列，取 embedding 做 <code>average pooling</code>；</li><li>用户历史搜索的 token 序列，也做 <code>average pooling</code>；</li><li>用户的地理位置、性别、年龄等；</li><li>样本年龄（后续单独介绍）。</li></ul><p>之后便是把所有 embedding 进行 concat 拼接，过3层 DNN 以得到 user vector 即 user embedding。</p><p><strong>注意：这里只有 user 的特征输入。</strong></p><blockquote><p>这是召回模型的通用方法，类似于双塔模型。主要是先构建 user embedding 的网络，利于后续线上服务。而与 item 的交互，往往放在最后一个环节。</p></blockquote><p>可以看到，在 user vector 生成后，被分成了 <code>training</code> 和 <code>serving</code> 两个分支。</p><p>先看 <code>training</code> 部分，看上去2步：</p><ol><li>先经过 softmax 层预估 video 的多分类概率；</li><li>然后产出 video vector 供 serving 使用。</li></ol><p>我们假设 3 层的 DNN 后得到的 user vector 是 K 维的，而需要进行多分类的候选集有 M 个 video，那么 training 侧的结构便是：</p><script type="math/tex; mode=display">output = softmax(user_{vec} \cdot W)</script><p>如果 $user_{vec}$ 是 $1 \times K$ 的，那么 $W$ 便是 $K \times M$ 的，如此输出就是 M 维的 softmax 结果。<strong>那么 W 的 M 列 K 维向量即可作为候选集 M 个 video 的 vector。</strong></p><p>其实不用陌生：让我们再次联想召回的双塔模型，是不是就相当于将候选 M 个 video 先过 embedding 层，之后与user vector 做点积，这也是召回模型的经典做法。</p><p>再看 <code>serving</code> 环节，也是经典的召回方案。即：</p><ol><li>离线模型中训练好的 video vector 保存下来；</li><li>将 video vector 构建到 ANN 等向量索引库中；</li><li>线上 serving 的时候，user vector 通过模型实时 infer 得到；</li><li>用 user vector 和索引库进行近邻召回。</li></ol><p><strong>如此的优势</strong>：</p><ul><li>因为每次 serving 需要处理的 video 很多，其 vector 不适合实时生成；</li><li>每次 serving 时 user vector 只需要 infer 一条样本，性能可控，捕捉 user 实时偏好就更重要。</li></ul><p><strong>样本年龄</strong><br>针对论文提到的 <code>Example Age</code> 特征，可能很多人（包括我本人），一开始对此理解是；</p><blockquote><p>视频上架距离 log 的时间差，比如曝光的视频已经上架 48h，那么该特征便是 48。即文中的 <code>Days Since Upload</code>。</p></blockquote><p>然而，结合其他观点和重读论文，应该是：</p><blockquote><p><code>sample log</code> 距离当前的时间作为 <code>example age</code>，比如一条曝光/点击日志 ，发生在 2h 前，在被 training 的时候，也许其 video 已经上架 48h 了，但 example age 特征的取值是前者 2。</p></blockquote><p>作者提到，在加入该特征后，<strong>模型能够较好地学习到视频的 freshness 程度对 popularity 的影响</strong>。体现在下图的 <code>Days Since Upload</code> 后验分布的变化，新模型比 baseline 表现得更接近真实分布。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/youtubednn2.png" alt="youtubednn2"></p><h3 id="2-2-ranking"><a href="#2-2-ranking" class="headerlink" title="2.2 ranking"></a>2.2 ranking</h3><blockquote><p>优化目标：expected watch time，即视频期望观看时长。</p></blockquote><p>这里我们重点介绍一些核心，先上模型结构图：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/youtubednn3.png" alt="youtubednn3"></p><p><strong>特征：</strong></p><ul><li><code>video embedding</code>：<ul><li><code>impression video ID</code>: 当前待评估的video的embedding</li><li><code>watched video IDs</code>: 用户最近观看的N个 video 的 embedding 的 average pooling</li></ul></li><li><code>language embedding</code>:<ul><li><code>user language</code>: 用户语言的 embedding</li><li><code>video language</code>: 当前视频语言的 embedding</li></ul></li><li><code>time since last watch</code>: 用户上次观看同 channel 视频距今的时间</li><li><code>#previous impressions</code>: 该 video 已经被曝光给该用户的次数</li></ul><p>披露的特征设计非常经典且贴合实际业务，也许真实的特征体系比这要更丰富，但论文披露的更多是特征设计的思想。</p><ul><li><code>video embedding</code> 代表了捕捉用户历史行为序列关于当前视频的相关度；</li><li><code>language</code> 非常具有 youtube 全球视频网站的特色，捕捉用户与视频语言差异。</li><li>后面的2个统计值度量了一些时间因素，用户看同 <code>channel</code> 隔了多久以捕捉兴趣衰减，已经曝光的次数代表了用户忽视程度。</li></ul><p>此外，论文提到了一些<code>trick</code>：</p><ul><li>连续型特征做 <code>normalization</code>，利于模型收敛；</li><li>部分统计特征进行了 <code>box-cox 变化</code>，<strong>是一种增加特征非线性输入的办法，辅助模型训练</strong>；</li><li>长尾 video，其 embedding 用 0 来代替，降低长尾影响。</li></ul><p>模型将输入 embedding 进行 concat 后过了一个 3 层 DNN，之后类似 recall 环节，又分成了 training 和 serving 这2个分支，实际上这里是<strong>巧妙地将回归问题转分类了</strong>。</p><ul><li><code>training</code> 时，Weighted LR 方式，label 为是否观看，weight 是观看时长，作用在 loss 上；</li><li><code>serving</code> 时，使用 $e^{wx+b}$ 作为观看时长的预估值，其中指数部分是训练时 sigmoid 的 input 部分。</li></ul><h2 id="3-实践经验"><a href="#3-实践经验" class="headerlink" title="3 实践经验"></a>3 实践经验</h2><p>结合王哲老师的工程10问，这里总结和补充一下个人认为比较重要的实战经验，供自己复盘和其他读者批评。</p><h3 id="3-1-Recall-model-的性能问题"><a href="#3-1-Recall-model-的性能问题" class="headerlink" title="3.1 Recall model 的性能问题"></a>3.1 Recall model 的性能问题</h3><p><code>next watch</code> 的目标下，候选 video 有数百万量级，这在使用 softmax 进行多分类更低效。论文有提到这块，类似于 word2vec 的解决方案，<strong>负采样（negative sampling）或者 分层处理（hierarchical softmax）。</strong>效果是没有太大差异，一般负采样使用更广泛。</p><blockquote><p>其原理是：每次不用预估所有候选，而只采样一定数量（超参数）的样本作为负样本，甚至进一步可以转化成基于点积的二分类。</p></blockquote><h3 id="3-2-Ranking-Model-为什么选择用分类而不是回归？"><a href="#3-2-Ranking-Model-为什么选择用分类而不是回归？" class="headerlink" title="3.2 Ranking Model 为什么选择用分类而不是回归？"></a>3.2 Ranking Model 为什么选择用分类而不是回归？</h3><p>我认为在该问题上主要有2点。</p><ol><li>是业务目标的决策。如果是点击等目标天然满足，这里这不满足此。</li><li>实际工业应用中，以时长等连续型数据作为 Label 时，因其<strong>具有严重的长尾分布特性，这会使得回归模型在拟合过程中容易欠佳</strong>。一般体现在对 Label 值过低和过高的两端样本拟合偏差，MSE、PCOC等预估统计量偏差很大。因而一般会转成分类任务来处理。<br><em>具体原因则和回归模型特性以及样本梯度分布有关系，不过多赘述。相对地，分类模型则在这方面稳健性会高一些。</em></li></ol><h3 id="3-3-Ranking-model-使用-weighted-LR-的原理"><a href="#3-3-Ranking-model-使用-weighted-LR-的原理" class="headerlink" title="3.3 Ranking model 使用 weighted LR 的原理"></a>3.3 Ranking model 使用 weighted LR 的原理</h3><p>我们来理解一下为什么论文中做法能生效？这里阐述一下个人的理解。</p><script type="math/tex; mode=display">p = sigmoid(z)=\frac{e^z}{1 + e^{z}}</script><p>其中 z 是最后一层，即 $z = wx+b$。</p><p>那么 LR 模型的交叉熵 loss 为：</p><script type="math/tex; mode=display">loss = \sum -(\log{p} + \log(1-p))</script><p>那么，如果我们将 label 由“是否观看”变成 $\frac{t}{1+t}$ ，其中 t 是观看时长，那么 loss 就变成：</p><script type="math/tex; mode=display">loss = \sum -(\frac{t}{1+t} \cdot \log{p} + \frac{1}{1+t} \cdot \log(1-p))</script><p>注意！这时候，$p$ 拟合的就是 $\frac{t}{1+t}$，当其不断逼近的时候，就有：</p><script type="math/tex; mode=display">e^{z} \to t</script><p>故，<strong>在 serving 的时候就使用 $e^{z}=e^{wx+b}$作为观看时长 t 的预估值。</strong></p><p>进一步，因大多时候 1+t 等于或接近1，那么 loss 近似等价于：</p><script type="math/tex; mode=display">loss = \sum -(t \cdot \log{p} + 1 \cdot \log(1-p))</script><blockquote><p>注：这里类似王哲老师提到的“概率p往往是一个很小的值”来近似上一个道理。</p></blockquote><p>这便是一个<strong>目标是否观看的 weighted LR 的 loss，且 weight 为观看时间 t。</strong></p><p><strong>补充：</strong><br><code>Weighted LR</code> 实际上就是 <code>WCE(weighted cross entropy) Loss</code>，一般来说有两种方法：</p><ul><li>将正样本按照 weight 做重复 sampling；</li><li>通过改变梯度的 weight 来得到 Weighted LR （论文方法）。</li></ul><p>但是 <code>WCE</code> 有2个缺点：</p><ul><li>其实假设了样本分布服从几何分布，否则可能导致效果不好；</li><li>在低估时（$\hat y &lt; y$）梯度很大，高估时（$\hat y &gt; y$）梯度很小，很容易导致模型高估。</li></ul><h3 id="3-4-如何生成-user-和-video-的-embedding？"><a href="#3-4-如何生成-user-和-video-的-embedding？" class="headerlink" title="3.4 如何生成 user 和 video 的 embedding？"></a>3.4 如何生成 user 和 video 的 embedding？</h3><p>文中介绍用 word2vec 预训练得到。当然，我们知道，也可以使用 embedding layer 去联合训练，且往往这种实践更好，也是现如今的主流做法。</p><h3 id="3-5-example-age-的处理和原因？"><a href="#3-5-example-age-的处理和原因？" class="headerlink" title="3.5 example age 的处理和原因？"></a>3.5 example age 的处理和原因？</h3><p><strong>猜想：可能是从特征维度区分开历史样本的重要度，越新的样本（不是 video）可能对模型参考价值越大。有点类似于 biasnet 的修正作用。</strong></p><blockquote><p>比如有一个 video（不一定新上架） 1h 前有很多正样本，且越来越少，那么模型通过此特征可能能够感知到这种热度的时序变化趋势。</p></blockquote><p>但，对于 youtube 而言，其模型训练应该是：</p><ul><li>实时（至少日内）的；</li><li>批次增量的（即不会回溯更久远的样本）；</li></ul><p>假设样本的切分窗口为 T（比如2h），那么就一定 $0&lt;example age&lt;T$，那么问题来了：</p><blockquote><p>样本只能学习到该特征在（0，T）的分布，但论文图中却展示 Days Since Upload 特征的后验分布改善了。</p></blockquote><p>serving 的时候，虽然说通过置0来消除 example age 的bias，但实际上样本距离当下的时间又发生了变化，即 example age 信息发生了偏移。</p><p><strong>总结</strong>：也许这两个特征都在模型中，且交叉效应大于边际效应。描述 video fresh 程度交给 Days Since Upload 特征，再加上描述 sample fresh 程度的 example age。能够使得前者后验预估的更准确，也能够通过后者修正历史样本的 times bias。</p><h3 id="3-6-为什么对每个用户提取等数量的训练样本？"><a href="#3-6-为什么对每个用户提取等数量的训练样本？" class="headerlink" title="3.6 为什么对每个用户提取等数量的训练样本？"></a>3.6 为什么对每个用户提取等数量的训练样本？</h3><p>原文中提到“这是为了减少高度活跃用户对于loss的过度影响。”但实际上个人觉得这个方法可能不一定最适合。结合逻辑和个人经验，个人认为主要有2点：</p><ul><li>模型学习了一个有偏于真实分布的样本，预估会有偏差；</li><li>本末倒置，使得低活的影响力反而增强，线上 ABtest 的时候，其贡献往往不足，因线上业务收益往往也是高活主导，二八法则；</li></ul><p>虽如此，<strong>倒不是说高活不应该抑制</strong>，对于比较异常的高活，可以针对他们样本欠采样或者加正则，而不是只有通过提高重要性更低的非高活人群的作用。</p><h3 id="3-7-为什么不采取类似RNN的Sequence-model？"><a href="#3-7-为什么不采取类似RNN的Sequence-model？" class="headerlink" title="3.7 为什么不采取类似RNN的Sequence model？"></a>3.7 为什么不采取类似RNN的Sequence model？</h3><p>论文提到主要是过于依赖临近行为，使得推荐结果容易大范围趋同于最近看过的 video。<br>此外，还有一个比较重要的原因便是 serving 时候的 infer 性能问题。</p><h3 id="3-8-为什么长尾的video直接用0向量代替？"><a href="#3-8-为什么长尾的video直接用0向量代替？" class="headerlink" title="3.8 为什么长尾的video直接用0向量代替？"></a>3.8 为什么长尾的video直接用0向量代替？</h3><p>把大量长尾的video截断掉，主要是为了<strong>节省online serving中宝贵的内存资源</strong>。</p><p>但现在问问不会这么粗暴的使用，一般是<strong>将长尾物料进行聚类，以改善他们样本过于稀疏而带来的收敛困难问题</strong>。极端情况就是聚为1类，共享一个 embedding，类似于冷启动一样，当逐渐脱离长尾后再出场拥有独立的 embedding。</p><p><strong>参考文章</strong><br><a href="https://dl.acm.org/doi/pdf/10.1145/2959100.2959190">Deep Neural Networks for YouTube Recommendations</a><br><a href="https://www.cnblogs.com/xumaomao/p/15207305.html">Weighted LR （WCE Weighted cross entropy）</a><br><a href="https://blog.csdn.net/u012328159/article/details/123986042">推荐系统（二十）谷歌YouTubeDNN</a><br><a href="https://zhuanlan.zhihu.com/p/52169807">重读Youtube深度学习推荐系统论文</a><br><a href="https://zhuanlan.zhihu.com/p/52504407">YouTube深度学习推荐系统的十大工程问题</a><br><a href="https://blog.csdn.net/weixin_46838716/article/details/126459692?spm=1001.2014.3001.5502">排序04：视频播放建模</a></p><hr>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-背景&quot;&gt;&lt;a href=&quot;#1-背景&quot; class=&quot;headerlink&quot; title=&quot;1 背景&quot;&gt;&lt;/a&gt;1 背景&lt;/h2&gt;&lt;p&gt;这是一篇推荐算法领域经典的论文，它由 YouTube 在2016年发表在 RecSys 上的文章&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/2959100.2959190&quot;&gt;Deep Neural Networks for YouTube Recommendations&lt;/a&gt;。&lt;br&gt;这篇文章是诸多推荐算法工程师的必学经典，可能很多人多次重读都会有新的思考，本文也重点总结文章的核心内容与一些实战经验的思考。&lt;/p&gt;
&lt;h2 id=&quot;2-原理&quot;&gt;&lt;a href=&quot;#2-原理&quot; class=&quot;headerlink&quot; title=&quot;2 原理&quot;&gt;&lt;/a&gt;2 原理&lt;/h2&gt;&lt;p&gt;首先便是其展示的系统链路示意图，这块与大多主流方案没有什么区别。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/youtubednn0.png&quot; alt=&quot;youtubednn0&quot;&gt;&lt;/p&gt;
&lt;p&gt;论文分别介绍了在 &lt;code&gt;recall&lt;/code&gt; 和 &lt;code&gt;ranking&lt;/code&gt; 两个模块的方案，但可以说，recall 部分的重要性远大于 ranking。就此文章发表后的几年而言，&lt;em&gt;recall 往往还在工业界主流召回的候选方案中，但 ranking 的方案基本已经成为历史，很少再使用了&lt;/em&gt;，不过其思想还是值得学习的。&lt;/p&gt;</summary>
    
    
    
    <category term="精排模型" scheme="https://www.xiemingzhao.com/categories/%E7%B2%BE%E6%8E%92%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="算法总结" scheme="https://www.xiemingzhao.com/categories/%E7%B2%BE%E6%8E%92%E6%A8%A1%E5%9E%8B/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="精排" scheme="https://www.xiemingzhao.com/tags/%E7%B2%BE%E6%8E%92/"/>
    
    <category term="YouTubeDNN" scheme="https://www.xiemingzhao.com/tags/YouTubeDNN/"/>
    
    <category term="WCE" scheme="https://www.xiemingzhao.com/tags/WCE/"/>
    
  </entry>
  
  <entry>
    <title>ExpectationI2I 召回</title>
    <link href="https://www.xiemingzhao.com/posts/expectationi2irecall.html"/>
    <id>https://www.xiemingzhao.com/posts/expectationi2irecall.html</id>
    <published>2021-11-27T16:00:00.000Z</published>
    <updated>2025-04-04T17:45:38.301Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>在推荐的发展历史中，Amazon 在 ItemCF 上进行了不少的探索。2003年，其在 IEEE INTERNET COMPUTING 发表的<a href="https://www.cs.umd.edu/~samir/498/Amazon-Recommendations.pdf">《http://Amazon.com Recommendations: Item-to-Item Collaborative Filtering》</a>一文中提出了 ItemCF 推荐算法，引起了不小的波澜。其<code>主要优势是</code>：</p><ul><li>简单可扩展；</li><li>可解释性强；</li><li>实时性高；</li></ul><p>在早期，ItemCF/UserCF 往往被用于推荐主算法，在当下一般作为召回算法。<strong>UserCF 适用于用户数的变化频率小于物品数的变化频率的场景，ItemCF 则相反。当今的互联网环境下确实是更适合 ItemCF 发挥。</strong></p><p>而本文就是为了介绍其新提出的一种改进的 ItemCF 算法 <code>ExpectationI2I</code>，当然有的地方名字可能不一样。这是由 Amazon 在 2017 年的 IEEE INTERNET COMPUTING 上发表的文章<a href="https://assets.amazon.science/76/9e/7eac89c14a838746e91dde0a5e9f/two-decades-of-recommender-systems-at-amazon.pdf">《Two Decades of Recommender Systems at Amazon.com》</a>中介绍的。</p><span id="more"></span><h2 id="2-动机"><a href="#2-动机" class="headerlink" title="2 动机"></a>2 动机</h2><p>主要是在如何定义物品的相关性上，有一定的改进空间。对于两个物品X和Y，在购买了X的用户中，假设有$N_{xy}$购买了Y。那在这里面，实际上有两部分组成：</p><ul><li>一个是因为X和Y<code>相关</code>，而产生的<code>关联购物</code>；</li><li>另一个则是X和Y<code>不相关</code>，仅仅是<code>随机性导致的共同购物</code>。</li></ul><p>所以，我们<strong>核心目标就是要度量其中关联购物的那一部分</strong>。在2003及之前的文章中，其大多使用下面的方法：</p><blockquote><p>首先假设购买X的user有同概率P(Y)购买Y，且 <strong>P(Y)=购买Y的uv/所有购买的uv</strong> 。那么X和Y的共同购买人数 $N<em>{xy}$ 的期望$E</em>{xy}$ = 购买X的uv * P(Y)。</p></blockquote><p>这里在理解上，个人认为<strong>有两点需要注意</strong>：</p><ol><li>$N_{xy}$ 实际上可以在实际中观测到，也<strong>就是X和Y的共同购买uv数，但是包含随机性共同购物</strong>；</li><li>上面实际上不仅假设了购买同概率，还同时默认一个假设：<strong>购买了X的用户群购买Y的概率与全局分布一致</strong>。</li></ol><p>而当出现很多购买两很大的用户时，就容易增加随机性共同购物，从而拉高了预估的结果。故，本文<strong>最核心是：如何度量$N_{xy}$中因随机性而产生的共同购物次数</strong>。那么剔除这一部分就可以得到因X和Y相关性而产生的<code>关联购物</code>次数，这便可以更有效的度量物品之间的相关性。</p><h2 id="3-算法原理"><a href="#3-算法原理" class="headerlink" title="3 算法原理"></a>3 算法原理</h2><p>为了计算购买X的用户会因随机性购买Y的期望$E_{xy}$，我们记：</p><ul><li>对于任意购买了X的用户c，|c|=其购买总次数-购买了X的次数；</li><li>P(Y)=全部用户购买Y的次数/全部用户的全部购买次数；</li><li>那么对于该用户c，在其除X外的|c|次购买中，有购买 Y 的概率$P_{xy} = 1-(1-P(Y))^{|c|}$</li></ul><p>所以$E<em>{xy}$就可以通过对所有购买了X的用户的$P</em>{xy}$求和来得到:</p><script type="math/tex; mode=display">\begin{array}{l}E_{XY} &=& \sum_{c \in X} [1 - (1-P(Y))^{|c|}] = \sum_{c \in X} [1 - \sum_{k=0}^{|c|} C_{|c|}^{k} (-P_Y)^k] \\&=& \sum_{c \in X}[1 - [1 + \sum_{k=1}^{|c|} C_{|c|}^{k} (-P_Y^k)]] = \sum_{c \in X} \sum_{k=1}^{|c|} (-1)^{k+1} C_{|c|}^{k}P_Y^k \\&=& \sum_{c \in X} \sum_{k=1}^{\infty} (-1)^{k+1} C_{|c|}^{k}P_Y^k \quad (since \ C_{|c|}^{k} = 0 \ for \ k>|c| ) \\&=& \sum_{k=1}^{\infty} \sum_{c \in X} (-1)^{k+1} C_{|c|}^{k}P_Y^k \quad (Fubini's \ theorem) \\&=& \sum_{k=1}^{\infty} \alpha_{k}(X) P_Y^k \quad where \ \alpha_k(X) = \sum_{c \in X}(-1)^{k+1} C_{|c|}^{k}\end{array}</script><p>作者提到一些<strong>计算技巧</strong>：</p><ul><li>即实际中$P_Y$一般比较小，所以可以设置一个k的上限 <code>max_k</code> 作为求和多项式的最多项，也即做了<strong>级数求和的近似</strong>。</li><li>另一方面，为了降低复杂度，$P<em>Y$和$\alpha</em>{k}{X}$的各组合项可以提前计算好，降低阶乘重复计算的复杂度。</li></ul><p>到这里，我们已经得到了购买X的用户随机购买Y的一个估计$E<em>{xy}$，据此可以判断真实中观测到的$N</em>{xy}$与随机估计的高低。所以，我们如果用同时购买的uv去除随机性的共同购买次数，得到的便是一个X和Y的关联性购买次数估计值。此便可以作为度量与X相关的商品Y的相关度，实际上作者还认为实际应用中有下面三种估计公式可选：</p><ol><li>$N<em>{xy}-E</em>{xy}$，<strong>会偏向于更流行的Y</strong>;</li><li>$(N<em>{xy}-E</em>{xy})/E_{xy}$，<strong>会使得低销量的物品很容易获得高分数</strong>;</li><li>$(N<em>{xy}-E</em>{xy})/sqrt(E_{xy})$，<strong>在高曝光商品和低销量商品之间找到平衡，需要动态调整</strong>。</li></ol><h2 id="4-实际应用"><a href="#4-实际应用" class="headerlink" title="4 实际应用"></a>4 实际应用</h2><p>在实际应用中，往往没有那么简单。首先我们介绍一下，该算法如何基于实际用户行为来计算，然后笔者将抛出一些问题和自己的优化建议。</p><script type="math/tex; mode=display">\sum_{c \in X} \sum_{k=1}^{|c|} (-1)^{k+1} C_{|c|}^{k}P_Y^k</script><p>现实一般基于上述公式来计算比较方便，原因是先处理好每个 <code>user x item</code> 的统计量，然后按照 user 聚合即可。</p><p><code>step1</code><br>构建所有行为（点击收藏等）的表，保留行为数量在合理区间 [n1,n2] 内的 user，保留 session 数量 &gt;m1 的 item_id<br>t1: user_id,item_id,time,session_id</p><p><code>step2</code><br>基于t1自join生成记录 session 内行为 pair 对表<br>t2: user_id,session_id,left_item_id,left_time,right_item_id,right_time,time_delta</p><p><code>step3</code><br>基于t2统计全局 pair 对数,session 去重 pair 对数统计表,计算 $N_{xy}$，<br>t3: left_item_id,right_item_id,cnt,user_session_cnt</p><p><code>step4</code><br>基于 expect_all_user_action 统计 user_id,item_id 共现的 session_id 次数<br>t4: user_id,item_id,sessioncnt</p><p><code>step5:</code><br>这一步最重要，<strong>我们需要计算 user x item 的|c|，下面记为 parm_c</strong>。记录 user_id,item_id 的统计参数<br>t5: user_id,item_id,clk_pv,clk_pv_all,parm_c(clk_pv_all-clk_pv)</p><p><strong>为了降低计算复杂度，这里的 parm_c 可以做上限截断</strong>。有了 parm<em>c 后就可以计算 $\sum</em>{k=1}^{|c|} (-1)^{k+1} C_{|c|}^{k}P_Y^k$。</p><p><code>step6:</code><br>基于t5对 user 进行聚合，统计 item<em>id 维度的参数<br>t6: item_id,clk_cnt,clk_all,clk_prob,parm_c_list<br>**其中 clk_prob 便是$P_Y$，即一个物品在全局被随机点击的概率。parm_c_list 则是将物品在所有的 user 上的 parm_c汇总到一起，为了后面来计算 $E</em>{xy}$。**</p><p><code>step7:</code><br>基于t6构建 item pair，来拼接 $N<em>{xy}$，并计算最重要的 $E</em>{xy}$<br>t7: left<em>item_id,right_item_id,Nxy,left_parm_c_list,right_clk_prob,Exy<br>其中，$E</em>{xy}$ 可以通过 left_parm_c_list,right_clk_prob 来计算。例如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.special <span class="keyword">import</span> comb</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ExpectScore</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">self, parm_c_list, clk_prob</span>):</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">get_exy</span>(<span class="params">parm_c, clk_prob</span>):</span><br><span class="line">            ans = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(parm_c):</span><br><span class="line">                ans += <span class="built_in">pow</span>(-<span class="number">1</span>, (k + <span class="number">1</span>)%<span class="number">2</span>) * <span class="built_in">pow</span>(prob_num,i) * comb(parm_c, i)</span><br><span class="line">                <span class="keyword">if</span> s_tmp[<span class="built_in">str</span>(i)+<span class="string">&#x27;.0&#x27;</span>] == <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">sum</span>(<span class="built_in">list</span>(res_t.values()))</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            res = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> parm_c <span class="keyword">in</span> parm_c_list.split(<span class="string">&#x27;,&#x27;</span>):</span><br><span class="line">                res += get_exy(parm_c, clk_prob)</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">sum</span>(res)</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0.0000</span></span><br></pre></td></tr></table></figure></p><p><code>step8:</code><br>已经拿到了最重要的两个统计项$N<em>{xy},E</em>{xy}$，计算最终结果分，并对每个 left_item_id 的所有 right_item_id 降序排列截断构建倒排即可。<br>t8: left_item_id,right_item_id, sc, rank<br>其中，</p><ul><li>sc 就是前述融合分数公式来计算得到的;</li><li>rank 便是按照sc降序排列得到的排名。</li></ul><p>实际上，如果计算中由于 parm<em>c 的上限截取的比较大，那么在计算$E</em>{xy}$中会频繁的遇到较大值的排列组合$C_{|c|}^{k}$的计算，有可能速度会比较慢。而在全局|c|的上限固定的情况下，我们可以利用<strong>空间换时间来优化</strong>这个：</p><blockquote><ul><li>因为|c|的上限固定，会使得$C<em>{|c|}^{k}$的计算比较高频重复，所以可以提前遍历全部有效k(即k&lt;=｜c|)的$C</em>{|c|}^{k}$结果；</li><li>然后，将表t5中的 parm<em>c 替换成存储各拆分项$C</em>{|c|}^{k}$的结果，在表t6中的 parm_c_list 也类似存储各个 user 的 parm_c 对应的各个拆分项；</li><li>最后，在表t7的$E<em>{xy}$计算环节，就不用计算排列组合项$$C</em>{|c|}^{k}$$，只需要将存好的<code>comb(parm_c, i)</code>项带入进行计算即可。</li></ul></blockquote><h2 id="5-问题与思考"><a href="#5-问题与思考" class="headerlink" title="5 问题与思考"></a>5 问题与思考</h2><p>实际上，如前文所述，这种算法不喜欢 user 行为覆盖过多的物品，<strong>高活用户对其不友好</strong>。而像 item2vec 或者 wbcos 这种对此就不敏感，因为基于 user 行为序列内构建样本 pair 对的。</p><p>在前面，介绍了 <code>ExpectationI2I</code> 算法的原理，实际构建方式，以及优化计算复杂度的方案。然而，实际应用中往往还存不少 badcase，我们需要对一些环节做一些精细的优化，这里列出部分问题和方案的思考，供备忘和参考。</p><p><strong>1. 有效行为数据的筛选逻辑</strong><br>部分行为过少或过多的极端用户会影响算法稳定性，所以在应用中我们也提到了n1,n2以及m1等边界超参数。这些参数的具体选值需要根据具体场景数据分布来确定，整体目标就是剔除不稳健的用户行为数据。</p><p><strong>2. 类目的信息，例如不同类目的惩罚或者加权；</strong><br>在构建的pair对上计算各统计项的时候，需要考虑类目相关度的一个影响，比如同类目是否进行加权，异类目是否进行降权，或者直接根据类目分布来进行权重调整，目的就是为了降低行为中偶然性类目的搭便车情况，降低算法的badcase率。</p><p><strong>3. 融合分公式的调整；</strong><br>在算法原理模块，已经介绍了论文作者提出的三种公式及其特性。在实际应用中，往往还需要进行一定的调整，否则很容易出现一些badcase。例如一种改进公式$\frac{N<em>{xy}-E</em>{xy}}{1 + \sqrt{E_{xy}}}$，总之还是以实际线上实验数据为准。</p><p><strong>4. 天级别数据的增量融合；</strong><br>在应用次算法的时候，往往面临一个行为数据时间窗口的选取，一次性选择太多，计算量会过大，选择太少数据会不准确。那么，一般可以采用每日增量更新。有两种方式：</p><ul><li>增量更新$N<em>{xy},E</em>{xy}$两个统计项，分别赋予历史值和增量值合适的权重；</li><li>增量直接更新表t8中最终的分数sc，然后构建新的排序。</li></ul><p><strong>参考文献</strong><br><a href="https://zhuanlan.zhihu.com/p/27656480">【翻译+批注】亚马逊推荐二十年</a><br><a href="https://www.jianshu.com/p/248209fd1038">【论文阅读】Two Decades of Recommender Systems at Amazon.com</a><br><a href="https://assets.amazon.science/76/9e/7eac89c14a838746e91dde0a5e9f/two-decades-of-recommender-systems-at-amazon.pdf">Two Decades of Recommender Systems at Amazon.com</a></p><hr>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-引言&quot;&gt;&lt;a href=&quot;#1-引言&quot; class=&quot;headerlink&quot; title=&quot;1 引言&quot;&gt;&lt;/a&gt;1 引言&lt;/h2&gt;&lt;p&gt;在推荐的发展历史中，Amazon 在 ItemCF 上进行了不少的探索。2003年，其在 IEEE INTERNET COMPUTING 发表的&lt;a href=&quot;https://www.cs.umd.edu/~samir/498/Amazon-Recommendations.pdf&quot;&gt;《http://Amazon.com Recommendations: Item-to-Item Collaborative Filtering》&lt;/a&gt;一文中提出了 ItemCF 推荐算法，引起了不小的波澜。其&lt;code&gt;主要优势是&lt;/code&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;简单可扩展；&lt;/li&gt;
&lt;li&gt;可解释性强；&lt;/li&gt;
&lt;li&gt;实时性高；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在早期，ItemCF/UserCF 往往被用于推荐主算法，在当下一般作为召回算法。&lt;strong&gt;UserCF 适用于用户数的变化频率小于物品数的变化频率的场景，ItemCF 则相反。当今的互联网环境下确实是更适合 ItemCF 发挥。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;而本文就是为了介绍其新提出的一种改进的 ItemCF 算法 &lt;code&gt;ExpectationI2I&lt;/code&gt;，当然有的地方名字可能不一样。这是由 Amazon 在 2017 年的 IEEE INTERNET COMPUTING 上发表的文章&lt;a href=&quot;https://assets.amazon.science/76/9e/7eac89c14a838746e91dde0a5e9f/two-decades-of-recommender-systems-at-amazon.pdf&quot;&gt;《Two Decades of Recommender Systems at Amazon.com》&lt;/a&gt;中介绍的。&lt;/p&gt;</summary>
    
    
    
    <category term="召回模型" scheme="https://www.xiemingzhao.com/categories/%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="算法总结" scheme="https://www.xiemingzhao.com/categories/%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="召回" scheme="https://www.xiemingzhao.com/tags/%E5%8F%AC%E5%9B%9E/"/>
    
    <category term="ExpectationI2I" scheme="https://www.xiemingzhao.com/tags/ExpectationI2I/"/>
    
  </entry>
  
  <entry>
    <title>知识蒸馏简述（Knowledge Distillation）</title>
    <link href="https://www.xiemingzhao.com/posts/distillationmodel.html"/>
    <id>https://www.xiemingzhao.com/posts/distillationmodel.html</id>
    <published>2021-09-15T16:00:00.000Z</published>
    <updated>2025-04-04T17:47:19.387Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1 背景"></a>1 背景</h2><p>目前有很多复杂的模型可以来完成不同的任务，但是部署重量级模型的集成在许多情况下并不总是可行的。有时，你的单个模型可能太大，以至于通常不可能将其部署到资源受限的环境中。这就是为什么我们一直在研究一些模型优化方法 ——量化和剪枝</p><h2 id="2-Softmax的故事"><a href="#2-Softmax的故事" class="headerlink" title="2 Softmax的故事"></a>2 Softmax的故事</h2><p>当处理一个分类问题时，使用 softmax 作为神经网络的最后一个激活单元是非常典型的用法。这是为什么呢？<strong>因为softmax函数接受一组 logit 为输入并输出离散类别上的概率分布</strong>。比如，手写数字识别中，神经网络可能有较高的置信度认为图像为1。不过，也有轻微的可能性认为图像为7。如果我们只处理像[1,0]这样的独热编码标签(其中1和0分别是图像为1和7的概率)，那么这些信息就无法获得。</p><span id="more"></span><p>人类已经很好地利用了这种相对关系。更多的例子包括，长得很像猫的狗，棕红色的，猫一样的老虎等等。正如 Hinton 等人所认为的</p><blockquote><p>一辆宝马被误认为是一辆垃圾车的可能性很小，但比被误认为是一个胡萝卜的可能性仍然要高很多倍。</p></blockquote><p>这些知识可以帮助我们在各种情况下进行极好的概括。这个思考过程帮助我们更深入地了解我们的模型对输入数据的想法。它应该与我们考虑输入数据的方式一致。</p><p><strong>而模型的 softmax 信息比独热编码标签更有用，因为本身的结果就是一种分布，人类认识世界又何尝不是如此</strong>。</p><h2 id="3-模型蒸馏的流程"><a href="#3-模型蒸馏的流程" class="headerlink" title="3 模型蒸馏的流程"></a>3 模型蒸馏的流程</h2><ul><li>在原始数据集上训练一个复杂但效果好的大模型，将此作为 <code>teacher model</code>；</li><li>基于教师模型在数据集上的预估结果 <code>soft label</code>，重新在数据集上训练一个轻量的模型，并尽量保留效果，此便是<code>student model</code>。</li></ul><blockquote><p>最终目的是得到一个小而美的模型以便于在生产中进行部署使用。</p></blockquote><p>本文以一个<strong>图像分类</strong>的例子，我们可以扩展前面的思想：</p><ul><li>训练一个在图像数据集上表现良好的教师模型。</li><li>在这里，交叉熵损失将根据数据集中的真实标签计算。</li><li>在相同的数据集上训练一个较小的学生模型，但是使用来自教师模型(softmax输出)的预测作为 ground-truth 标签。</li></ul><p>这些 softmax 输出称为软标签 <code>soft label</code>，原始的标签即为 <code>hard label</code>。</p><h2 id="4-使用soft-label的原理"><a href="#4-使用soft-label的原理" class="headerlink" title="4 使用soft label的原理"></a>4 使用soft label的原理</h2><p>在容量方面，我们的学生模型比教师模型要小。</p><p>因此，如果你的数据集足够复杂，那么较小的student模型可能不太适合捕捉训练目标所需的隐藏表示。我们在软标签上训练学生模型来弥补这一点，它提供了比独热编码标签更有意义的信息。在某种意义上，我们通过暴露一些训练数据集来训练学生模型来模仿教师模型的输出。</p><h2 id="5-损失函数的构建"><a href="#5-损失函数的构建" class="headerlink" title="5 损失函数的构建"></a>5 损失函数的构建</h2><blockquote><p>实际中存在<code>弱概率</code>的问题是：它们没有捕捉到学生模型有效学习所需的信息。</p></blockquote><p>例如，如果概率分布像[0.99, 0.01]，几乎不可能传递图像具有数字7的特征的知识。</p><p>Hinton 等人解决这个问题的方法是：<strong>在将原始 logits 传递给 softmax 之前，将教师模型的原始 logits 按一定的温度进行缩放</strong>。</p><p>这样，就会在可用的类标签中得到更广泛的分布。然后用同样的温度用于训练学生模型。</p><p>我们可以把学生模型的修正损失函数写成这个方程的形式：</p><script type="math/tex; mode=display">L_{KDCE}=-\sum_i p_i \log s_i</script><p>其中，$p_i$是教师模型得到软概率分布，si的表达式为：</p><script type="math/tex; mode=display">s_i=-\frac{exp(z_i/T)}{\sum_j exp(z_j/T)}</script><p>具体到代码的实现如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_kd_loss</span>(<span class="params">student_logits, teacher_logits, </span></span><br><span class="line"><span class="params">                true_labels, temperature,</span></span><br><span class="line"><span class="params">                alpha, beta</span>):</span><br><span class="line">    </span><br><span class="line">    teacher_probs = tf.nn.softmax(teacher_logits / temperature)</span><br><span class="line">    kd_loss = tf.keras.losses.categorical_crossentropy(</span><br><span class="line">        teacher_probs, student_logits / temperature, </span><br><span class="line">        from_logits=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> kd_loss</span><br></pre></td></tr></table></figure></p><h2 id="6-合并使用软硬标签"><a href="#6-合并使用软硬标签" class="headerlink" title="6 合并使用软硬标签"></a>6 合并使用软硬标签</h2><p>Hinton 等人还探索了在真实标签(通常是 one-hot 编码)和学生模型的预测之间使用传统交叉熵损失的想法。当训练数据集很小，并且软标签没有足够的信号供学生模型采集时，这一点尤其有用。</p><p>当它与扩展的 softmax 相结合时，这种方法的工作效果明显更好，而整体损失函数成为两者之间的加权平均。</p><script type="math/tex; mode=display">L=\frac{\alpha * L_{KDCE}+\beta * L_{CE}}{(\alpha + \beta)}</script><p>其中</p><script type="math/tex; mode=display">L_{CE}=-\sum_i y_i \log z_i</script><p>而$y_i$和$z_i$分别就是原始的标签即<code>hard label</code>和学生模型的原始预测结果。</p><p>具体代码实现可以如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_kd_loss</span>(<span class="params">student_logits, teacher_logits, </span></span><br><span class="line"><span class="params">                true_labels, temperature,</span></span><br><span class="line"><span class="params">                alpha, beta</span>):</span><br><span class="line">    teacher_probs = tf.nn.softmax(teacher_logits / temperature)</span><br><span class="line">    kd_loss = tf.keras.losses.categorical_crossentropy(</span><br><span class="line">        teacher_probs, student_logits / temperature, </span><br><span class="line">        from_logits=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    ce_loss = tf.keras.losses.sparse_categorical_crossentropy(</span><br><span class="line">        true_labels, student_logits, from_logits=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    total_loss = (alpha * kd_loss) + (beta * ce_loss)</span><br><span class="line">    <span class="keyword">return</span> total_loss / (alpha + beta)</span><br></pre></td></tr></table></figure></p><p>结合起来看便可以知道一个是基于软标签训练的，而另一个就是基于原始硬标签训练的。并且在实际使用中，<strong>一般的$\alpha$取值要大于$\beta$比较好</strong>，以便更多的吸收教师模型的信息。</p><h2 id="7-直接拟合软标签"><a href="#7-直接拟合软标签" class="headerlink" title="7 直接拟合软标签"></a>7 直接拟合软标签</h2><p>既然我们想学习教师模型的信息，最粗暴的做法便是以教师模型的结果<code>soft label</code>作为目标，直接进行回归。Caruana 等人便是如此，操作原始 logits，而不是 softmax 值。这个工作流程如下：</p><ul><li>这部分保持相同:训练一个教师模型。这里交叉熵损失将根据数据集中的真实标签计算。</li><li>现在，为了训练学生模型，训练目标变成分别最小化来自教师和学生模型的原始对数之间的平均平方误差。</li></ul><script type="math/tex; mode=display">L_{KDMSE}=\sum_i||z_i^{\theta_student} - z_{i(teacher)}^{fixed}||^2</script><p>具体代码实现可如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mse = tf.keras.losses.MeanSquaredError()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mse_kd_loss</span>(<span class="params">teacher_logits, student_logits</span>):</span><br><span class="line">    <span class="keyword">return</span> mse(teacher_logits, student_logits)</span><br></pre></td></tr></table></figure></p><p>使用这个损失函数的一个<strong>潜在缺点是它是无界的</strong>。原始 logits 可以捕获噪声，而一个小模型可能无法很好的拟合。这就是为什么为了使这个损失函数很好地适合蒸馏状态，学生模型需要更大一点。</p><p>Tang 等人探索了在两个损失之间插值的想法：<strong>扩展 softmax 和 MSE 损失</strong>。数学上，它看起来是这样的：</p><script type="math/tex; mode=display">L=(1-\alpha) \cdot L_{KDMSE} + \alpha \cdot L_{KDCE}</script><p>根据经验，他们发现当 $\alpha = 0$ 时，(在NLP任务上)可以获得最佳的性能。</p><h2 id="8-实践中的一些经验"><a href="#8-实践中的一些经验" class="headerlink" title="8 实践中的一些经验"></a>8 实践中的一些经验</h2><h3 id="8-1-使用数据增强"><a href="#8-1-使用数据增强" class="headerlink" title="8.1 使用数据增强"></a>8.1 使用数据增强</h3><p>他们在NLP数据集上展示了这个想法，但这也适用于其他领域。为了更好地指导学生模型训练，使用数据增强会有帮助，特别是当你处理的数据较少的时候。因为我们通常保持学生模型比教师模型小得多，所以我们希望学生模型能够获得更多不同的数据，从而更好地捕捉领域知识。</p><h3 id="8-2-使用未标记的数据"><a href="#8-2-使用未标记的数据" class="headerlink" title="8.2 使用未标记的数据"></a>8.2 使用未标记的数据</h3><blockquote><p>在像 Noisy Student Training 和 SimCLRV2 这样的文章中，作者在训练学生模型时使用了额外的未标记数据。</p></blockquote><p>因此，你将使用你的 teacher 模型来生成未标记数据集上的 ground-truth 分布。这在很大程度上有助于提高模型的可泛化性。<strong>这种方法只有在你所处理的数据集中有未标记数据可用时才可行</strong>。有时，情况可能并非如此(例如，医疗保健)。也有研究数据平衡和数据过滤等技术，以缓解在训练学生模型时合并未标记数据可能出现的问题。</p><h3 id="8-3-在训练教师模型时不要使用标签平滑"><a href="#8-3-在训练教师模型时不要使用标签平滑" class="headerlink" title="8.3 在训练教师模型时不要使用标签平滑"></a>8.3 在训练教师模型时不要使用标签平滑</h3><p><code>标签平滑</code>是一种技术，<strong>用来放松由模型产生的高可信度预测</strong>。它有助于减少过拟合，但不建议在训练教师模型时使用标签平滑，因为无论如何，它的 logits 是按一定的温度缩放的。因此，一般不推荐在知识蒸馏的情况下使用标签平滑。</p><h3 id="8-4-使用更高的温度值"><a href="#8-4-使用更高的温度值" class="headerlink" title="8.4 使用更高的温度值"></a>8.4 使用更高的温度值</h3><p>Hinton 等人建议使用更高的温度值来 soften 教师模型预测的分布，这样软标签可以为学生模型提供更多的信息。这在处理小型数据集时特别有用。对于更大的数据集，信息可以通过训练样本的数量来获得。</p><h2 id="9-代码"><a href="#9-代码" class="headerlink" title="9 代码"></a>9 代码</h2><p>具体的实现代码，可以参考<a href="https://github.com/DushyantaDhyani/kdtf">DushyantaDhyani</a>代码，是比较简洁易懂的。<br>值得注意的是：</p><ol><li><p>其在训练教师模型的时候使用的是</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">logits = tf.add(tf.matmul(fc1, <span class="variable language_">self</span>.weights[<span class="string">&#x27;out&#x27;</span>]), <span class="variable language_">self</span>.biases[<span class="string">&#x27;out&#x27;</span>]) / <span class="variable language_">self</span>.softmax_temperature</span><br></pre></td></tr></table></figure></li><li><p>在训练学生模型的时候，使用了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable language_">self</span>.total_loss += tf.square(<span class="variable language_">self</span>.softmax_temperature) * <span class="variable language_">self</span>.loss_op_soft</span><br></pre></td></tr></table></figure><p>并不是单独定义$\alpha$和$\beta$的。</p></li></ol><p><strong>参考文章</strong><br><a href="https://mp.weixin.qq.com/s/IAk61KBKgOBsx9X2zINlfg">神经网络中的蒸馏技术，从Softmax开始说起</a><br><a href="https://wandb.ai/authors/knowledge-distillation/reports/Distilling-Knowledge-in-Neural-Networks--VmlldzoyMjkxODk">Distilling Knowledge in Neural Networks</a></p><hr>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-背景&quot;&gt;&lt;a href=&quot;#1-背景&quot; class=&quot;headerlink&quot; title=&quot;1 背景&quot;&gt;&lt;/a&gt;1 背景&lt;/h2&gt;&lt;p&gt;目前有很多复杂的模型可以来完成不同的任务，但是部署重量级模型的集成在许多情况下并不总是可行的。有时，你的单个模型可能太大，以至于通常不可能将其部署到资源受限的环境中。这就是为什么我们一直在研究一些模型优化方法 ——量化和剪枝&lt;/p&gt;
&lt;h2 id=&quot;2-Softmax的故事&quot;&gt;&lt;a href=&quot;#2-Softmax的故事&quot; class=&quot;headerlink&quot; title=&quot;2 Softmax的故事&quot;&gt;&lt;/a&gt;2 Softmax的故事&lt;/h2&gt;&lt;p&gt;当处理一个分类问题时，使用 softmax 作为神经网络的最后一个激活单元是非常典型的用法。这是为什么呢？&lt;strong&gt;因为softmax函数接受一组 logit 为输入并输出离散类别上的概率分布&lt;/strong&gt;。比如，手写数字识别中，神经网络可能有较高的置信度认为图像为1。不过，也有轻微的可能性认为图像为7。如果我们只处理像[1,0]这样的独热编码标签(其中1和0分别是图像为1和7的概率)，那么这些信息就无法获得。&lt;/p&gt;</summary>
    
    
    
    <category term="算法总结" scheme="https://www.xiemingzhao.com/categories/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="蒸馏模型" scheme="https://www.xiemingzhao.com/tags/%E8%92%B8%E9%A6%8F%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>深入浅出计算机组成原理——应用篇（52-55）</title>
    <link href="https://www.xiemingzhao.com/posts/computerOrgArc52to55.html"/>
    <id>https://www.xiemingzhao.com/posts/computerOrgArc52to55.html</id>
    <published>2021-08-20T16:00:00.000Z</published>
    <updated>2025-04-08T16:46:28.037Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>全文内容主要来自对课程<a href="https://time.geekbang.org/column/article/91427">《深入浅出计算机组成原理》</a>的学习笔记。</p></blockquote><h2 id="52-设计大型DMP系统（上）"><a href="#52-设计大型DMP系统（上）" class="headerlink" title="52 | 设计大型DMP系统（上）"></a>52 | 设计大型DMP系统（上）</h2><h3 id="DMP：数据管理平台"><a href="#DMP：数据管理平台" class="headerlink" title="DMP：数据管理平台"></a>DMP：数据管理平台</h3><p><code>DMP</code> 系统的全称叫作<code>数据管理平台</code>（Data Management Platform），在搜索、推荐、广告领域使用很广。</p><p>DMP 可以简单地看成是一个键 - 值对（Key-Value）数据库，用来存储画像信息。期望的性能：</p><ul><li>低响应时间（Low Response Time）；</li><li>高可用性（High Availability）；</li><li>高并发（High Concurrency）；</li><li>海量数据（Big Data）；</li><li>可负担的成本（Affordable Cost）</li></ul><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac52p01.jpg" alt=""></p><p>如上图，为了维持 DMP 的运转，上游需要不断的采集数据更新其中信息：</p><ul><li>采集埋点日志，通过数据管道（Data Pipeline）落入数据仓库（Data Warehouse），挖掘和抽取画像更新到 DMP 中；</li><li>通过实时数据处理模块（Realtime Data Processing），进行实时的清洗和聚合，更新一些实时画像。</li></ul><h3 id="MongoDB-的例子"><a href="#MongoDB-的例子" class="headerlink" title="MongoDB 的例子"></a>MongoDB 的例子</h3><blockquote><p>MongoDB 的设计宣传：不需要预设数据 Schema，访问速度很快，还能够无限水平扩展。</p></blockquote><p>有人说只用它就够了，实际上很难有如此完美的情况，看下不同环节的性能取舍：<br><code>KV存取</code>：响应快、高并发、写多读少（全随机）；<br><code>数据管道</code>：高吞吐量、响应时间松、顺序读写。<br><code>数据仓库</code>：读取量巨大，很重的离线抽取和分析需求。</p><p>MongoDB 缺陷：</p><ul><li>没有针对 SSD 优化，高并发读取差；</li><li>顺序写入和吞吐率差；</li><li>没有 Schema，元信息占用空间大。</li></ul><p>相对可行的方案：<br><code>KV 数据库</code>：SSD + AeroSpike；（高并发、成本可控）<br><code>数据管道</code>：HDD + Kafka；（充分利用 Zero-Copy 和 顺序读写）<br><code>数据仓库</code>：HDD + Hive 等 Schema 数据库。（序列化存储）</p><hr><h2 id="53-设计大型DMP系统（下）"><a href="#53-设计大型DMP系统（下）" class="headerlink" title="53 | 设计大型DMP系统（下）"></a>53 | 设计大型DMP系统（下）</h2><h3 id="关系型数据库"><a href="#关系型数据库" class="headerlink" title="关系型数据库"></a>关系型数据库</h3><p>传统关系型数据库，为了避免读取的时候过多的扫描，往往给数据的行号加一个索引，这个映射关系可以让行号直接从硬盘的某个位置去读。索引不仅可以索引行号，还可以索引某个字段。但，写入数据时还要更新的索引。最终还要落到 HDD 硬盘的话，就很难做到高并发了。</p><p>DMP 的 KV 数据库主要是随机查询，数据管道的需求主要是写入和顺序读取就好了。因此<strong>就会面临大量的随机写入和随机读取的挑战。</strong></p><h3 id="Cassandra：顺序写和随机读"><a href="#Cassandra：顺序写和随机读" class="headerlink" title="Cassandra：顺序写和随机读"></a>Cassandra：顺序写和随机读</h3><h4 id="Cassandra-的数据模型"><a href="#Cassandra-的数据模型" class="headerlink" title="Cassandra 的数据模型"></a>Cassandra 的数据模型</h4><p>它是一个分布式 KV 数据库，键一般称为 <code>Row Key</code>，一个 16-36 字节的字符串。每个 Key 对应的 Value 是一个 Hash 表，可用键值对存入需要的数据。</p><p>有严格的 Schema，提前定义好列（Column），常一起用的聚合为<code>列族</code>（Column Family）。<strong>既保持了不需要严格的 Schema 这样的灵活性，也保留了可以把常常一起使用的数据存放在一起的空间局部性。</strong></p><h4 id="Cassandra-的写操作"><a href="#Cassandra-的写操作" class="headerlink" title="Cassandra 的写操作"></a>Cassandra 的写操作</h4><p>简单概述为<strong>不随机写，只顺序写</strong>。过程是两个动作：</p><ul><li>往磁盘上写入一条提交日志（Commit Log）；</li><li>直接在内存的数据结构上去更新数据。</li></ul><p>优势：</p><ul><li>都是顺序写（Sequential Write），可最大化吞吐量；</li><li>内存的数据量或条目超过限额，会 dump 到硬盘上，也顺序写；</li><li>Dump 同时，会根据 row key 来生成索引文件，用于快速查询；</li><li>当 Dump 的文件过多，Cassandra 会在后台进行文件的对比合并。</li></ul><h4 id="Cassandra-的读操作"><a href="#Cassandra-的读操作" class="headerlink" title="Cassandra 的读操作"></a>Cassandra 的读操作</h4><p><strong>先从内存查，再从硬盘读，合并成最终结果。</strong></p><ol><li>在内存会有 Cache，Cache 里面找不到，我们才会去请求硬盘里面的数据。</li><li>硬盘可能 Dump 了不同时间点的快照，所以按照时间从新的往旧的里面找。</li><li>为了避免查找过多 Dump 文件，会为每一个 Dump 的文件里面所有 Row Key 生成一个 BloomFilter 放进内存；</li><li>所以，不在内存，但是在 BloomFilter 中的时候，才会请求硬盘了。</li></ol><h3 id="利用-SSD-的优势"><a href="#利用-SSD-的优势" class="headerlink" title="利用 SSD 的优势"></a>利用 SSD 的优势</h3><p>Cassandra 的特点：</p><ul><li>没有任何的随机写请求，无论是 Commit Log 还是 Dump；</li><li>会优先从内存读，这相当于 LRU 的缓存机制；</li><li>BloomFilter，<strong>把本来因为 Dump 文件带来的需要多次读硬盘的问题，简化成多次内存读和一次硬盘读。</strong></li></ul><p>顺序读写下，HDD 硬盘的吞吐率还是很不错的：</p><blockquote><p>每秒写入 100MB 数据，如果一条 1KB，那么 10 万的 WPS（Writes per seconds）对于 DMP 也不错。</p></blockquote><p>但 DMP 的数据访问分布，<strong>往往缺少局部性的</strong>，随机读避免不了。HDD 硬盘在这块较差，全都放内存，成本在 HDD 硬盘 100 倍以上。<strong>相比较，用 SSD 硬盘，我们可以用 1/10 的成本获得和内存同样的 QPS。</strong></p><hr><h2 id="54-理解Disruptor（上）"><a href="#54-理解Disruptor（上）" class="headerlink" title="54 | 理解Disruptor（上）"></a>54 | 理解Disruptor（上）</h2><p>作者在最后2节主要以开源项目 Disruptor 为例，介绍如何利用 CPU 和高速缓存的硬件特性。Disruptor 是由一家专门做高频交易的公司 LMAX 开源出来的。</p><h3 id="Padding-Cache-Line"><a href="#Padding-Cache-Line" class="headerlink" title="Padding Cache Line"></a>Padding Cache Line</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">......</span><br><span class="line"></span><br><span class="line"><span class="keyword">abstract</span> <span class="keyword">class</span> <span class="title class_">RingBufferPad</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">protected</span> <span class="type">long</span> p1, p2, p3, p4, p5, p6, p7;</span><br><span class="line">&#125;</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"><span class="keyword">abstract</span> <span class="keyword">class</span> <span class="title class_">RingBufferFields</span>&lt;E&gt; <span class="keyword">extends</span> <span class="title class_">RingBufferPad</span></span><br><span class="line">&#123;</span><br><span class="line">    ......    </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">long</span> indexMask;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Object[] entries;</span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">final</span> <span class="type">int</span> bufferSize;</span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">final</span> Sequencer sequencer;</span><br><span class="line">    ......    </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">class</span> <span class="title class_">RingBuffer</span>&lt;E&gt; <span class="keyword">extends</span> <span class="title class_">RingBufferFields</span>&lt;E&gt; <span class="keyword">implements</span> <span class="title class_">Cursored</span>, EventSequencer&lt;E&gt;, EventSink&lt;E&gt;</span><br><span class="line">&#123;</span><br><span class="line">    ......    </span><br><span class="line">    <span class="keyword">protected</span> <span class="type">long</span> p1, p2, p3, p4, p5, p6, p7;</span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>看上面一段代码，Disruptor 分别在 RingBufferPad 和 RingBuffer 类里面都定义了 p1-p7 这样 7 个 long 变量。看上去很突兀，实际上这 14 个变量没有任何实际的用途，<strong>只是缓存行填充（Padding Cache Line），以发挥 CPU 高速缓存（CPU Cache）。</strong></p><p>我们知道，<strong>高速缓存的写回和加载，都是以整个 Cache Line 作为单位的</strong>。比如，64 位 Intel CPU，缓存行通常是 64 个字节（Bytes），即 8 个 long 类型的数据。</p><p>我想读者你大概已经猜到了。在 RingBufferFields 里 final 定义了一系列真正要使用的变量，我们期望他们一直在 CPU Cache 里。<strong>而高速缓存在写回和加载的时候，还会关联到这个数据前后定义的其他变量，以满足 64 个字节的 Cache Line 大小</strong>。而系统会有其他程序在使用，为了保证数据的同步更新，常量的缓存也就失效了，会频繁的读写，降低这里的速度。</p><blockquote><p>但是，当我们按照上述前后各新增 7 个 long 数据后，如下图所示。<strong>无论加载其中哪个 final 变量，对应的 Cache Line 都只包含这批 final 变量和定义的 pad 变量。所以，只要被频繁地读取访问，就不会再被换出 Cache 了。</strong></p></blockquote><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac54p01.jpg" alt=""></p><h3 id="使用-RingBuffer"><a href="#使用-RingBuffer" class="headerlink" title="使用 RingBuffer"></a>使用 RingBuffer</h3><p>Disruptor 整个框架，其实就是一个高速的<strong>生产者 - 消费者模型</strong>（Producer-Consumer）下的队列。</p><p>队列的实现，最合适的是链表，例如 java 中的 LinkedBlockingQueue。但在 Disruptor 里面用的是 RingBuffer 的数据结构，其底层是一个固定长度数组。</p><p><strong>数据存在空间局部性</strong>，即连续多个元素会一并加载到 CPU Cache 里面来，访问遍历的速度会更快。反观链表也不具备这个优势特性。</p><hr><h2 id="55-理解Disruptor（下）"><a href="#55-理解Disruptor（下）" class="headerlink" title="55 | 理解Disruptor（下）"></a>55 | 理解Disruptor（下）</h2><h3 id="缓慢的锁"><a href="#缓慢的锁" class="headerlink" title="缓慢的锁"></a>缓慢的锁</h3><p>上节提到的通过 RingBuffer 实现一个队列实际上是无锁的。</p><p>先回到 java 的 LinkedBlockingQueue，它是依赖锁的。其需要锁的原因是：</p><ol><li>可能会有多个生产者在队列尾加任务、多个消费者在消费队列头；</li><li>哪怕生产者、消费者均只有1个时，后者也会比前者快来防止任务积压，从而大多时候二者指向队列同一个节点产生锁竞争；</li></ol><p>所以，为了解决此问题，jvm 中实现了加锁机制，没有拿到锁的线程会挂起等待。</p><p>按照下面代码测试有无锁的性能差了几十倍。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.xuwenhao.perf.jmm;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.atomic.AtomicLong;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.locks.Lock;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.locks.ReentrantLock;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LockBenchmark</span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">runIncrement</span><span class="params">()</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">long</span> <span class="variable">counter</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="type">long</span> <span class="variable">max</span> <span class="operator">=</span> <span class="number">500000000L</span>;</span><br><span class="line">        <span class="type">long</span> <span class="variable">start</span> <span class="operator">=</span> System.currentTimeMillis();</span><br><span class="line">        <span class="keyword">while</span> (counter &lt; max) &#123;</span><br><span class="line">            counter++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">long</span> <span class="variable">end</span> <span class="operator">=</span> System.currentTimeMillis();</span><br><span class="line">        System.out.println(<span class="string">&quot;Time spent is &quot;</span> + (end-start) + <span class="string">&quot;ms without lock&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">runIncrementWithLock</span><span class="params">()</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">Lock</span> <span class="variable">lock</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ReentrantLock</span>();</span><br><span class="line">        <span class="type">long</span> <span class="variable">counter</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="type">long</span> <span class="variable">max</span> <span class="operator">=</span> <span class="number">500000000L</span>;</span><br><span class="line">        <span class="type">long</span> <span class="variable">start</span> <span class="operator">=</span> System.currentTimeMillis();</span><br><span class="line">        <span class="keyword">while</span> (counter &lt; max) &#123;</span><br><span class="line">            <span class="keyword">if</span> (lock.tryLock())&#123;</span><br><span class="line">                counter++;</span><br><span class="line">                lock.unlock();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">long</span> <span class="variable">end</span> <span class="operator">=</span> System.currentTimeMillis();</span><br><span class="line">        System.out.println(<span class="string">&quot;Time spent is &quot;</span> + (end-start) + <span class="string">&quot;ms with lock&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        runIncrement();</span><br><span class="line">        runIncrementWithLock();</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="无锁的-RingBuffer"><a href="#无锁的-RingBuffer" class="headerlink" title="无锁的 RingBuffer"></a>无锁的 RingBuffer</h3><p>Disruptor 是通过用 CPU 硬件支持的指令（CAS，Compare And Swap，比换和交换）来实现无锁。</p><p>如下图所示，</p><ol><li>创建了一个 Sequence 对象，用来指向当前的 RingBuffer 的头和尾，通过一个序号；</li><li><strong>对比序号的方式</strong>：当生产者添加时，它会把当前的序号，加上新数据的数量，然后和消费者位置对比，防止覆盖掉还没消费的数据。</li></ol><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac55p01.jpg" alt=""></p><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;全文内容主要来自对课程&lt;a href=&quot;https://time.geekbang.org/column/article/91427&quot;&gt;《深入浅出计算机组成原理》&lt;/a&gt;的学习笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;52-设计大</summary>
      
    
    
    
    <category term="学习笔记" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="计算机组成原理" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/"/>
    
    
    <category term="计算机原理" scheme="https://www.xiemingzhao.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%8E%9F%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>深入浅出计算机组成原理——原理篇：存储与I/O系统（46-51）</title>
    <link href="https://www.xiemingzhao.com/posts/computerOrgArc46to51.html"/>
    <id>https://www.xiemingzhao.com/posts/computerOrgArc46to51.html</id>
    <published>2021-08-17T16:00:00.000Z</published>
    <updated>2025-04-08T16:44:30.839Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>全文内容主要来自对课程<a href="https://time.geekbang.org/column/article/91427">《深入浅出计算机组成原理》</a>的学习笔记。</p></blockquote><h2 id="46-SSD硬盘（上）"><a href="#46-SSD硬盘（上）" class="headerlink" title="46 | SSD硬盘（上）"></a>46 | SSD硬盘（上）</h2><h3 id="SSD-的读写原理"><a href="#SSD-的读写原理" class="headerlink" title="SSD 的读写原理"></a>SSD 的读写原理</h3><p><strong>没有寻道，随机读写快。</strong><br>缺点：耐用性（重复擦写）差。</p><h3 id="SLC、MLC、TLC-和-QLC"><a href="#SLC、MLC、TLC-和-QLC" class="headerlink" title="SLC、MLC、TLC 和 QLC"></a>SLC、MLC、TLC 和 QLC</h3><p>一个电容，有无电压代表0、1，即1bit信息。这种方式为 <code>SLC</code>（Single-Level Cell） 颗粒。与 CPU Cache 类似，存储有限。</p><p><strong>如果一个单元存储超过1bit呢？</strong></p><p>于是发明了 <code>MLC</code>（Multi-Level Cell）、<code>TLC</code>（Triple-Level Cell）以及 <code>QLC</code>（Quad-Level Cell），分别可以存储2、3、4bit。</p><p>因为有一个<code>电压计</code>，<strong>可以通过不同的电压值表示不同的bit</strong>。比如，15个不同的电压位，加0电压总共16个值则可以表示4bit。但对电压精度要求很高，会导致充放电慢。</p><h3 id="P-E-擦写问题"><a href="#P-E-擦写问题" class="headerlink" title="P/E 擦写问题"></a>P/E 擦写问题</h3><p>SSD 由多个<code>裸片</code>（Die）叠在一起组成，每个如下所示。</p><ul><li>一个裸片会分成多个<code>平面</code>（Plane），容量约GB级；</li><li>一个平面会分很多<code>块</code>（Block），容量约MB级；</li><li>一个块分多个<code>页</code>（Page），容量约4KB。</li></ul><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac46p01.jpg" alt=""></p><p>SSD 写入叫 <code>Program</code>，不是复写来完成，需要先<code>擦除</code>（Erase）。单位是一个页（Page），擦除是按照块（Block）来进行。<strong>硬盘寿命是每个块的可擦除的次数，物理材料原因。</strong></p><blockquote><p>SLC 一般在10万次，MLC约1万次，TLC和QLC约几千次。</p></blockquote><h3 id="SSD-读写的生命周期"><a href="#SSD-读写的生命周期" class="headerlink" title="SSD 读写的生命周期"></a>SSD 读写的生命周期</h3><p>下图是SSD硬盘读写的流程：</p><ul><li>绿色：有效数据；</li><li>红色：已删除数据；</li><li>白色：无数据。</li></ul><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac46p02.jpg" alt=""></p><p>因为<strong>写入最小单位是页，擦除最小单位是块</strong>，数据一多容易出现红色空洞占的地方也会越来越多，因为绿色页的存在，整块不能擦除，白色页就越来越少。</p><p>所以需要<code>磁盘碎片整理</code>，如下图所示，即把有效的零散的绿色数据集中搬移到新空间，然后擦除原来的块，就可以空出容量了。</p><blockquote><p>厂商为了“磁盘碎片整理”，往往有一些预留空间（Over Provisioning），一般在7%-15%。</p></blockquote><hr><h2 id="47-SSD硬盘（下）"><a href="#47-SSD硬盘（下）" class="headerlink" title="47 | SSD硬盘（下）"></a>47 | SSD硬盘（下）</h2><p>作为系统盘的时候，大多部分只读不写。擦除会反复发生在其他用来存放数据的地方，容易变成坏块，即容量变小了。</p><h3 id="磨损均衡、TRIM-和写入放大效应"><a href="#磨损均衡、TRIM-和写入放大效应" class="headerlink" title="磨损均衡、TRIM 和写入放大效应"></a>磨损均衡、TRIM 和写入放大效应</h3><h4 id="FTL-和磨损均衡"><a href="#FTL-和磨损均衡" class="headerlink" title="FTL 和磨损均衡"></a>FTL 和磨损均衡</h4><p>为了减少坏块，我们尽量平摊擦除到每个块上，即<code>磨损均衡</code>（Wear-Leveling）。实现方式就是<code>FTL</code> 这个<code>闪存转换层</code>。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac47p01.jpg" alt=""></p><p>与内存管理类似，在 FTL 里面，存放了<code>逻辑块地址</code>（Logical Block Address，简称 LBA）到<code>物理块地址</code>（Physical Block Address，简称 PBA）的映射。系统都是访问前者，实际操作是后者。</p><blockquote><p>FTL 能够记录物理块被擦写的次数。如果某块被擦写的次数多了，可以将它挪到一个擦写次数少的物理块上。</p></blockquote><h4 id="TRIM-指令的支持"><a href="#TRIM-指令的支持" class="headerlink" title="TRIM 指令的支持"></a>TRIM 指令的支持</h4><p>操作系统的逻辑层和 SSD 的逻辑层里的块状态，是不匹配的。</p><p>因操作系统删除数据的时候，是把对应的 inode 里面的元信息清理掉，<strong>物理层面仅标记成可写入，没有实际擦除</strong>。所以日常的文件删除，都只是一个操作系统层面的逻辑删除，有被恢复的可能。</p><p>为了磨损均衡，很多时候在都在搬运很多已经删除了的数据。解法是 TRIM 命令，在文件被删除的时候，让操作系统去通知 SSD 硬盘，对应的逻辑块已经标记成已删除了。</p><h4 id="写入放大"><a href="#写入放大" class="headerlink" title="写入放大"></a>写入放大</h4><p>SSD 硬盘容易越用越慢。</p><blockquote><p>空间被占用多了之后，空白不足，需要经常进行整理，于是变慢。</p></blockquote><p><strong>写入放大 = 实际的闪存写入的数据量 / 系统通过 FTL 写入的数据量，该值越大（说明为了完成操作多了很多整理），性能越差。</strong></p><h3 id="AeroSpike"><a href="#AeroSpike" class="headerlink" title="AeroSpike"></a>AeroSpike</h3><p>SSD 虽然有劣势，但可以充分利用其优势，比如 <code>AeroSpike</code>。</p><blockquote><p>AeroSpike 这个专门针对 SSD 硬盘特性设计的 Key-Value 数据库（键值对数据库）</p></blockquote><p>利用SSD物理特性:</p><ul><li>不通过操作系统，直接操作 SSD 中的块和页；</li><li>写入的时候尽可能写较大的数据块，一般128KB；</li><li>读取的时候可以读512 字节（Bytes）这样的小数据。</li></ul><p>AeroSpike 需要响应时间短，所以写入放大严重，优化：</p><ul><li>持续地进行磁盘碎片整理。（块碎片&gt;50%）</li><li>建议只用到容量最大额的50%。（降低写放大）</li></ul><hr><h2 id="48-DMA-Kafka-快速原因"><a href="#48-DMA-Kafka-快速原因" class="headerlink" title="48 | DMA: Kafka 快速原因"></a>48 | DMA: Kafka 快速原因</h2><p>CPU 的主频 2GHz 意味每秒20亿次操作，诸如大文件复制等操作，CPU 大部分时间都空闲等待。<code>DMA</code> 技术，<code>直接内存访问</code>（Direct Memory Access）来减少 CPU 等待的时间。</p><h3 id="理解-DMA，一个协处理器"><a href="#理解-DMA，一个协处理器" class="headerlink" title="理解 DMA，一个协处理器"></a>理解 DMA，一个协处理器</h3><p>主板上一块芯片，在内存和 I/O 设备的数据传输时，不通过 CPU 控制，直接通过 <code>DMA 控制器</code>（DMA Controller，简称 DMAC），也是协处理器。</p><p>DMAC 使用价值：</p><ul><li>传输的数据极大、速度快，减少 CPU 依赖；</li><li>传输的数据极小、速度慢，等数据到齐再发送，减少 CPU 忙等待。</li></ul><p>DMAC 也是一个特殊的 I/O 设备，链接总线进行传输。总线上设备分<code>主设备</code>、<code>从设备</code>。<br>只有主设备可以主动发起数据传输，如 CPU。I/O 设备只能发送控制信号，告知 CPU 有数据要传，再由 CPU 拉数。<br><strong>DMAC 既是一个主设备（对于IO设备），又是一个从设备（对于CPU）。</strong></p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac48p01.jpg" alt=""></p><p>DMAC 进行数据传输的过程（硬盘加载数据为例）:</p><ol><li>CPU 向 DMAC 发起请求，即修改其配置寄存器；</li><li>提供给 DMAC 的信息：<ul><li>源地址的初始值（硬盘IO地址）以及传输时候的地址增减方式（是否大往小地址传）；</li><li>目标地址初始值（目的地地址）和传输时候的地址增减方式；</li><li>要传输的数据长度。</li></ul></li><li>设置后，DMAC 变为空闲（Idle）；</li><li>硬盘向 DMAC 发起数据传输请求；</li><li>DMAC 响应；</li><li>DMAC 向硬盘接口发起总线读请求并读取；</li><li>DMAC 向内存发起总线写请求并写入；</li><li>反复上述6、7，直到指定长度数据传输完成，回到第 3 步空闲。</li></ol><blockquote><p>因显示器、网卡、硬盘对于数据传输的需求都不一样，所以各个设备里面都有自己的 DMAC 芯片了。</p></blockquote><h3 id="Kafka-的实现原理"><a href="#Kafka-的实现原理" class="headerlink" title="Kafka 的实现原理"></a>Kafka 的实现原理</h3><p><strong>Kafka 项目是通过利用 DMA 的方式实现了非常大的性能提升</strong>，是目前实时数据传输管道的标准解决方案。</p><p>Kafka 两种常见海量数据传输：</p><ul><li>从网络中接收数据并落盘；</li><li>从本地磁盘读取并通过网络发送出去。</li></ul><p>从本地磁盘读取并通过网络发送出去，<strong>这个过程数据一共发生四次传输</strong>，2次 DMA 传输，2次 CPU 控制传输。</p><p>整个过程：</p><ol><li>从硬盘读到系统内核缓冲区，DMA 搬运；</li><li>内核缓冲区复制到内存，CPU 搬运；</li><li>从内存写到操作系统的 Socket 缓冲区，CPU 搬运；</li><li>从 Socket 缓冲区写到网卡缓冲区，DMA 搬运。</li></ol><p><strong>在 Kafka 中，则只有上述的 1、4 步骤</strong>。数据绕过内存，直接通过 Channel，写入到对应的网络设备里。并且，对于 Socket 的操作，也不是写入到 Socket 的 Buffer 里面，而是直接根据描述符（Descriptor）写入到网卡的缓冲区里面。</p><blockquote><p>没有在内存层面去“复制（Copy）”数据，所以也称上述方法是<code>零拷贝</code>（Zero-Copy）。</p></blockquote><p><em>传输同样数据的时间，可以缩减为原来的 1/3</em></p><hr><h2 id="49-数据完整性（上）"><a href="#49-数据完整性（上）" class="headerlink" title="49 | 数据完整性（上）"></a>49 | 数据完整性（上）</h2><h3 id="单比特翻转"><a href="#单比特翻转" class="headerlink" title="单比特翻转"></a>单比特翻转</h3><p>作者举例遇到的硬件错误，由于定制的硬件没有使用 ECC 内存，内存中出现了<code>单比特翻转</code>（Single-Bit Flip）。</p><p>一个 ASCII 码二进制表示是 0010 0100，可能是 0011 0100 在第4位发生单比特反转变来的。随机，不可复现，而 <code>ECC</code> 内存的全称是 <code>Error-Correcting Code memory</code>，中文名字叫作<code>纠错内存</code>。</p><h3 id="奇偶校验和校验位"><a href="#奇偶校验和校验位" class="headerlink" title="奇偶校验和校验位"></a>奇偶校验和校验位</h3><p>奇偶校验，即内存里面的 N 位比特当成是一组，1 的个数是奇数还是偶数，并记录在<code>校验码位</code>。</p><blockquote><p>如，101010111，前8位数数据位，最后一位就是校验码位</p></blockquote><p>该算法速度快，并且能判断内存数据是否出错。缺陷：</p><ol><li>只能解决遇到奇数个的错误；</li><li>只能发现错误，不能纠正错误。</li></ol><p>ECC 内存所使用的解决方案，既可以发现，也可以纠错。2个版本：</p><ul><li>纠错码（Error Correcting Code），发现并纠正；</li><li>纠删码（Erasure Code），不能纠正时，直接删除。</li></ul><hr><h2 id="50-数据完整性（下）"><a href="#50-数据完整性（下）" class="headerlink" title="50 | 数据完整性（下）"></a>50 | 数据完整性（下）</h2><p>纠错码，需要既能判断是否有错，还要找出错误位置。</p><h3 id="海明码"><a href="#海明码" class="headerlink" title="海明码"></a>海明码</h3><p><code>海明码</code>（Hamming Code）是最知名的纠错码，上世纪四十年代发明，沿用至今。</p><p>最基础的是 7-4 海明码，即 7bit 数据位，4bit 校验位。因为，4bit 可以表示 16 个数，假设全部正确状态占用一个数，那么剩余 15 个数可以用来纠正 15 中单比特反转错误。</p><blockquote><p>这里需要注意，如果纠错位少一位，变成3bit，就只能表达7个校验位，而校验位本身也会出现错误，所以7-4实际上有11种单比特反转错误。</p><p>数据位有 K 位，校验位有 N 位，需要<strong>满足 K + N + 1 &lt;= 2^N</strong></p></blockquote><p><strong>一个简单的理解：数据位+纠错位总共有 K+N 位，那么就有 K+N 种单比特错误位置，外加1个全部正确状态，故 N 位的校验位需要能够表达至少 K + N + 1 个数值，于是有了上述关系。</strong></p><h3 id="海明码的纠错原理"><a href="#海明码的纠错原理" class="headerlink" title="海明码的纠错原理"></a>海明码的纠错原理</h3><p>校验位数的原理清楚了，那么怎么构建校验位不同数值对应的错误位映射关系呢？</p><p>这里直接以7-4海明码为例：</p><ol><li>在 11 位中<strong>将 $2^k$ 位作为校验位</strong>，即 1、2、4、8 位，记为 p1-p4；</li><li>剩下的 7 位作为数据位，记为 d1-d7，来存储有效信息；</li><li>我们需要将数据位分组到每个校验位上，<strong>分组方式是根据二进制各个位置的1值</strong>：</li></ol><p>如下图，如：</p><ul><li>针对位置1的校验位p1，分到该组的数据位就是二进制下第0位为1的所有，包括1（自身）、3、5、7、9、11；</li><li>针对位置4的校验位p3，分到该组的数据位就是二进制下第2位为1的所有，包括4（自身）、5、6、7；</li></ul><ol><li>分好组了，校验位的值可以根据组内位置奇偶校验法得到。</li></ol><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac50p01.jpg" alt=""></p><p>任何一个数据码出错了，就至少会有对应的两个或者三个校验码对不上，这样我们就能反过来找到是哪一个数据码出错了。</p><p><strong>实际上找出错位方式很简单，即校验位按照 p4-p1 排列二进制的数值。</strong></p><blockquote><p>首先校验位需要按照 p4-p1 的方式来排列成二进制；比如 11 位中，第 3 位bit（即d1）反转了，那么对应上述表格，p1、p2校验位会为 1，即 0011，刚好是第 3 位。再比如第 7 位，那么就是 0111，刚好是7。</p></blockquote><p><em>这里很像一道 coding 问题：1000 瓶药水只有 1 瓶有毒，有 10 个试验用的小白鼠，如何定位有毒的那瓶？</em></p><p>解法：</p><ol><li>10 个小白鼠作为 10 个 bit 位，可以表示 0 到 $2^{10}-1$，能够涵盖 1000；</li><li>1000 瓶药水按照顺序编码，编码对应 bit 位为 1 的药水要给对应位置的小白鼠喝；</li><li>死亡的小白鼠，将对应 bit 位置为 1，那么 10 个 bit 位对应的值就是第几瓶药水有毒。</li></ol><h3 id="海明距离"><a href="#海明距离" class="headerlink" title="海明距离"></a>海明距离</h3><p>对于两个二进制表示的数据，他们之间有差异的位数，我们称之为海明距离。</p><hr><h2 id="51-分布式计算"><a href="#51-分布式计算" class="headerlink" title="51 | 分布式计算"></a>51 | 分布式计算</h2><p>数据中心里一台计算机一般不够，面临3个问题：</p><ol><li>垂直扩展和水平扩展的选择问题；</li><li>如何保持高可用性（High Availability）；</li><li>一致性问题（Consistency）。</li></ol><h3 id="从硬件升级到水平扩展"><a href="#从硬件升级到水平扩展" class="headerlink" title="从硬件升级到水平扩展"></a>从硬件升级到水平扩展</h3><p>假设采买了一台云服务器：1 个 CPU 核心、3.75G 内存以及一块 10G 的 SSD 系统盘。</p><p>当请求增长，性能不足时需要增加资源：</p><ul><li><code>垂直扩展</code>（Scale Up）：换成 2 个 CPU 核心、7.5G 内存；</li><li><code>水平扩展</code>（Scale Out）：买 2 台 1 个 CPU 核心、3.75G 内存。</li></ul><p>垂直扩展总有上限，因为物理机的原因。水平扩展，则需要软件层面改造，进入分布式，需要<code>负载均衡</code>。</p><blockquote><p>分布式，即通过<code>消息传递</code>（Message Passing）而不是<code>共享内存</code>（Shared Memory）的方式，让多台不同的计算机协作起来共同完成任务</p></blockquote><h3 id="理解高可用性和单点故障"><a href="#理解高可用性和单点故障" class="headerlink" title="理解高可用性和单点故障"></a>理解高可用性和单点故障</h3><p><strong>系统的可用性（Avaiability）：系统可以正常服务的时间占比。</strong></p><blockquote><p>如可用性是99.99%，则表示服务计划外的宕机时间&lt;=4.32分钟/月。</p></blockquote><p>水平扩展具有天然的优势，因为负载均衡能够通过<code>健康检测</code>（Health Check）发现坏掉的服务器，自动把其上的流量切换到其他正常服务器上，即<code>故障转移</code>（Failover）。</p><p>单点故障问题（Single Point of Failure，SPOF）：任何一台服务器出错了，整个系统就没法用了。</p><p><strong>解决单点故障问题就是移除单点</strong>，通过水平扩展，使得单台服务器挂了不影响其他正常使用。</p><p>但是，单点可能存在多处，比如虽然服务器水平扩展了，但物理机在一个机房，交换机容易成为单点。这时候就需要<code>异地多活</code>的系统设计和部署。即，服务器分地区采购后部署服务，很多公司都会采用。</p><p>故障转移（Failover）机制的生效，需要服务进行<code>健康监测</code>（Health Check），即每隔很短时间检查一次各服务器是否正常运行，如果有，则将流量转移到其他正常机器。</p><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;全文内容主要来自对课程&lt;a href=&quot;https://time.geekbang.org/column/article/91427&quot;&gt;《深入浅出计算机组成原理》&lt;/a&gt;的学习笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;46-SSD</summary>
      
    
    
    
    <category term="学习笔记" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="计算机组成原理" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/"/>
    
    
    <category term="计算机原理" scheme="https://www.xiemingzhao.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%8E%9F%E7%90%86/"/>
    
    <category term="存储与I/O系统" scheme="https://www.xiemingzhao.com/tags/%E5%AD%98%E5%82%A8%E4%B8%8EI-O%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>深入浅出计算机组成原理——原理篇：存储与I/O系统（40-45）</title>
    <link href="https://www.xiemingzhao.com/posts/computerOrgArc40to45.html"/>
    <id>https://www.xiemingzhao.com/posts/computerOrgArc40to45.html</id>
    <published>2021-08-11T16:00:00.000Z</published>
    <updated>2025-04-08T16:44:30.848Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>全文内容主要来自对课程<a href="https://time.geekbang.org/column/article/91427">《深入浅出计算机组成原理》</a>的学习笔记。</p></blockquote><h2 id="40-理解内存（上）"><a href="#40-理解内存（上）" class="headerlink" title="40 | 理解内存（上）"></a>40 | 理解内存（上）</h2><p><strong>虚拟内存地址究竟是怎么转换成物理内存地址?</strong></p><h3 id="简单页表"><a href="#简单页表" class="headerlink" title="简单页表"></a>简单页表</h3><p><code>页表</code>（Page Table）：虚拟内存里面的页，到物理内存里面的页的一一映射。</p><p>一个内存地址分成<code>页号</code>（Directory）和<code>偏移量</code>（Offset）。</p><blockquote><p>一页内存在物理层面是连续的，一般大小是 4K 字节（4KB），需要 20 位的高位，12 位的低位。</p></blockquote><p><strong>内存地址转换步骤</strong>：</p><ol><li>虚拟内存地址，切分成页号、偏移量；</li><li>从页表，查询对应的物理页号；</li><li>物理页号+偏移量，就是物理内存地址。</li></ol><p><strong>页表空间</strong>：</p><blockquote><p>32位地址，高位有20，需要记录$2^{20}$个物理页号的映射，数组形势，一个页号4字节，总计大约4MB。看上去不多，但如每个进程有一份，就很大了。</p></blockquote><h3 id="多级页表"><a href="#多级页表" class="headerlink" title="多级页表"></a>多级页表</h3><p><strong>只需要去存用到的页之间的映射关系。</strong></p><p>虚拟内存占用的地址空间，通常是两段连续的空间，<code>多级页表</code>（Multi-Level Page Table）适用这类。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac40p01.jpg" alt=""></p><p>如上图，一个4级多级页表：</p><ol><li>页号部分（20位）拆成4段（每段5位），对应各级页表索引；</li><li>每级索引对应的是下一级页表的位置；</li><li>最后的1级页表，继续用“页号 + 偏移量”来获取物理内存地址。</li></ol><blockquote><p>因为实际的虚拟内存空间通常是连续的，所以需要的 2、3 级页表很少。</p></blockquote><p>看下空间大小：</p><ul><li>每段索引5bit，1 级页表有$2^5=32$个条目映射，每个条目 4 字节（32bits=4Bytes），该 1 级页表总计 128 字节；</li><li>每个 1 级索引表映射 32 个 4KB 内存快，即 128Kb；</li><li>那么每个填满的2级索引表，对应 32 个 1 级索引表，总计 4MB 内存映射。</li></ul><p>需要存的映射少了，空间节省了，但查询次数多了，一个典型的时间换空间的方案。</p><hr><h2 id="41-理解内存（下）"><a href="#41-理解内存（下）" class="headerlink" title="41 | 理解内存（下）"></a>41 | 理解内存（下）</h2><ul><li><code>地址转换</code>是很高频，怎么解性能问题？</li><li>数据、指令都在内存，怎么解内存安全问题？</li></ul><h3 id="加速地址转换：TLB"><a href="#加速地址转换：TLB" class="headerlink" title="加速地址转换：TLB"></a>加速地址转换：TLB</h3><p>内存访问其实比 Cache 要慢，简单的内存转换，1次变4次。<strong>解法：加缓存。</strong></p><blockquote><p>这块缓存芯片我们称之为 TLB，全称是<code>地址变换高速缓冲</code>（Translation-Lookaside Buffer）。</p></blockquote><p>和 CPU 里的高速缓存类似，用脏标记来实现“写回”等缓存管理策略。为了性能，整个内存转换过程也要由硬件来执行，封装为内存管理单元（MMU，Memory Management Unit）芯片</p><h3 id="安全性与内存保护"><a href="#安全性与内存保护" class="headerlink" title="安全性与内存保护"></a>安全性与内存保护</h3><h4 id="可执行空间保护"><a href="#可执行空间保护" class="headerlink" title="可执行空间保护"></a>可执行空间保护</h4><p>一个进程使用的内存，指令部分设置成“可执行”的，数据等其他部分不给予“可执行”的权限。</p><h4 id="地址空间布局随机化"><a href="#地址空间布局随机化" class="headerlink" title="地址空间布局随机化"></a>地址空间布局随机化</h4><p><strong>核心问题</strong>：其他的人、进程、程序，会去修改掉特定进程的指令、数据，然后，让当前进程去执行这些指令和数据，造成破坏。</p><p>原进程内存空间是固定，容易获取指令位置。地址空间布局随机化之后，无法猜到指令的内存地址，随意修改只会让程序 crash 掉，不会执行危险代码。</p><p>一个经典的随机性应用：<br>用户名称+密码通过hash保护，但泄露太多加密的，容易通过彩虹表等方式推测结果。于是，在hash的时候全部加上<code>盐值</code>（Salt），即使猜出来也无法使用。</p><hr><h2 id="42-总线"><a href="#42-总线" class="headerlink" title="42 | 总线"></a>42 | 总线</h2><p>CPU 和内存、以及外部输入输出设备的通信，计算机是怎么完成的？</p><h3 id="降低复杂性"><a href="#降低复杂性" class="headerlink" title="降低复杂性"></a>降低复杂性</h3><p>假设计算机有 N 个不同的设备需要通信，并且是单独链接，那么复杂度就是 $N^2$。</p><p>优化的方案：不用单独通信，都是用一条公用线路，即<code>总线</code>。</p><p>总线，就是一组线路，英文是 BUS，设计模式是<code>事件总线</code>。<br>事件总线：</p><ul><li>发布者：各模块触发对应的事件，并把事件对象发送到总线上；</li><li>监听者：各模块也注册到总线上，去监听事件，并根据对象类型或内容来决定是否要处理或者响应。</li></ul><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac42p01.jpg" alt=""></p><h3 id="理解总线"><a href="#理解总线" class="headerlink" title="理解总线"></a>理解总线</h3><p><code>双独立总线</code>（Dual Independent Bus，缩写为 DIB）：</p><ul><li>快速的<code>本地总线</code>（Local Bus）；</li><li>较慢的<code>前端总线</code>（Front-side Bus）。</li></ul><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac42p02.jpg" alt=""></p><p>CPU链接前端总线，即系统总线，与 I/O 桥接器相连，分别连接内存总线和 I/O 总线。</p><p>总线通常有三类线路：</p><ul><li><code>数据线</code>（Data Bus），传输数据；</li><li><code>地址线</code>（Address Bus），数据传输的位置，是内存的某个位置，还是某 I/O 设备。</li><li><code>控制线</code>（Control Bus），用来控制对于总线的访问。</li></ul><hr><h2 id="43-输入输出设备"><a href="#43-输入输出设备" class="headerlink" title="43 | 输入输出设备"></a>43 | 输入输出设备</h2><h3 id="接口和设备"><a href="#接口和设备" class="headerlink" title="接口和设备"></a>接口和设备</h3><p>输入输出设备一般2个部分：</p><ul><li>接口（Interface）；</li><li>实际的 I/O 设备（Actual I/O Device）。</li></ul><p>三类寄存器（在设备的接口电路上）：</p><ul><li>状态寄存器（Status Register）；</li><li>命令寄存器（Command Register）；</li><li>数据寄存器（Data Register）。</li></ul><h3 id="控制-I-O-设备"><a href="#控制-I-O-设备" class="headerlink" title="控制 I/O 设备"></a>控制 I/O 设备</h3><p>设备接口上除了3类寄存器，还有控制电路来控制实际硬件。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac43p01.jpg" alt=""></p><ol><li>数据寄存器（Data Register）。<blockquote><p>CPU 向 I/O 设备写入需要传输的数据。实际上还有数据缓冲区。</p></blockquote></li><li>命令寄存器（Command Register）。<blockquote><p>CPU 发送打印命令给打印机。控制电路两个动作：状态寄存器把状态设置成 not-ready；操作打印机进行打印。</p></blockquote></li><li>状态寄存器（Status Register）。<blockquote><p>告诉 CPU 设备已在工作，其他数据和命令不能执行。直到完成，重新 ready 状态。</p></blockquote></li></ol><h3 id="信号和地址"><a href="#信号和地址" class="headerlink" title="信号和地址"></a>信号和地址</h3><p>CPU 往总线上发送的命令具体是什么，才能和 I/O 接口上的设备通信呢？</p><p>答案是<code>机器指令</code>。但 MIPS 并没有专门的 I/O 指令，实际上是使用<code>内存地址</code>。</p><blockquote><p>主内存会映射 I/O 设备的内存地址。CPU 通信时，往这些地址发送数据即可。这种叫内存映射 IO（Memory-Mapped I/O，简称 MMIO）。</p></blockquote><p>Intel CPU 既支持 MMIO，还可以通过特定的指令，来支持端口映射 I/O（Port-Mapped I/O，简称 PMIO）。核心的区别，PMIO 里面访问的设备地址，不再是在内存地址空间里面，而是一个专门的端口（Port）。</p><hr><h2 id="44-理解IO-WAIT"><a href="#44-理解IO-WAIT" class="headerlink" title="44 | 理解IO_WAIT"></a>44 | 理解IO_WAIT</h2><p>并不是所有问题都能靠利用内存或者 CPU Cache 做一层缓存来解决。硬盘使用还是很多的，尤其是大数据场景，那么硬盘的 I/O 性能就很重要。</p><h3 id="IO-性能、顺序访问和随机访问"><a href="#IO-性能、顺序访问和随机访问" class="headerlink" title="IO 性能、顺序访问和随机访问"></a>IO 性能、顺序访问和随机访问</h3><p>硬盘厂商的性能报告：</p><ul><li>响应时间；</li><li>数据传输率。</li></ul><p>硬盘：</p><ul><li>HDD：机械硬盘，SATA 3.0 接口。</li><li>SSD：固态硬盘，SATA 3.0 &amp; PCI Express 接口。</li></ul><p>SATA 3.0，带宽 6Gb/s，“b”是bit，则约768MB/s（$6 \times 1024 / 8 = 768$）。但日常约 200 MS/s。<br>SSD 大约 500 MB/s，如果换成 PCI Express 约 2 GB/s。</p><p>响应时间，HDD 约十毫秒左右，SSD 约几十微妙，差异更大。但，无论哪个，貌似性能还可以，与实际经验不符合。</p><p><strong>在顺序读写和随机读写的情况下，硬盘的性能是完全不同的</strong></p><p><code>IOPS</code>：每秒输入输出操作的次数，去随机读取磁盘上某一个 4KB 大小的数据，一秒之内可以读取到多少数据。</p><p>SSD，随机读写约 40MB/s，即1万次 4KB 数据，即 IOPS。写入的话约90MB/s，IOPS 约2万。<br>而HDD，IOPS 大约为100。</p><h3 id="定位-IO-WAIT"><a href="#定位-IO-WAIT" class="headerlink" title="定位 IO_WAIT"></a>定位 IO_WAIT</h3><p>CPU 的主频通常在 2GHz 以上，也就是每秒可以做 20 亿次操作。即使一条读写指令，需要很多个时钟周期，硬盘完全跟不上。</p><p><strong>通过 top 和 iostat 这些命令，一起来看看 CPU 到底有没有在等待 io 操作。</strong></p><p>top 去看服务的负载，也就是 load average。也可以看 CPU 是否在等待 IO 操作完成。</p><blockquote><p>以 %CPU 开头的行，有一个叫作 wa 的指标，这个指标就代表着 iowait</p></blockquote><p>如果 iostat 很大，可以通过 <code>iostat</code> 查看实际硬盘读写。</p><blockquote><p>其中， tps 指标，其实就对应着我们上面所说的硬盘的 IOPS 性能。而 kB_read/s 和 kB_wrtn/s 指标，就对应着我们的数据传输率的指标</p></blockquote><p>用<code>iotop</code>命令找哪一个进程是这些 I/O 读写的来源。</p><blockquote><p>通过<code>stress -i 2</code>，模拟两个进程往硬盘写数据。top 的输出里面，CPU 就有大量的 sy 和 wa。通过 iostat，里面的 tps 很快就到了 4 万左右。iotop， I/O 占用都来自于 stress 产生的两个进程了。</p></blockquote><hr><h2 id="45-机械硬盘"><a href="#45-机械硬盘" class="headerlink" title="45 | 机械硬盘"></a>45 | 机械硬盘</h2><h3 id="拆解机械硬盘"><a href="#拆解机械硬盘" class="headerlink" title="拆解机械硬盘"></a>拆解机械硬盘</h3><p>前面提过，机械硬盘 IOPS 大约100，这个的原理是什么？</p><p><strong>机械硬盘三个组成部分：盘面、磁头和悬臂。</strong></p><p><code>盘面</code>（Disk Platter）：实际存储数据的，跟光盘类似，铝、玻璃或者陶瓷制成，上有磁性涂层来存储数据。中间有转轴控制转速，RPM（Rotations Per Minute）一般为7200，每秒则120转。</p><p><code>磁头</code>（Drive Head）：从盘面读取数据传输给电路，一个硬盘会有多个堆叠的磁盘，每个磁盘正反面都会各有一个磁头。</p><p><code>悬臂</code>（Actutor Arm）：链接磁头，用来定位磁头到磁道的。</p><p>磁盘会切分不同半径的同心环作为<code>磁道</code>，每个磁道会分成多个<code>扇区</code>，上下不同磁盘但平行的扇区称为一个<code>柱面</code>。所以读取的数据的时候，悬臂需要控制磁头，移动到对应的磁道上，并且盘面需要转转道对应位置，如此磁头才能读取到指定的扇区/柱面。</p><p>硬盘上的随机访问：</p><ul><li><code>平均延时</code>（Average Latency）：盘面旋转，把几何扇区对准悬臂位置的时间。<blockquote><p>7200转/min 硬盘，即 240半圈/s，寻找指定扇区平均需要半圈距离，即 1s / 240 = 4.17ms。</p></blockquote></li><li><code>平均寻道时间</code>（Average Seek Time）：盘面旋转，悬臂定位到扇区的的时间。<blockquote><p>一般在 4-10ms</p></blockquote></li></ul><p>综上，随机访问时间约8-14ms，对应的 IOPS 为 125-70.</p><h3 id="Partial-Stroking"><a href="#Partial-Stroking" class="headerlink" title="Partial Stroking"></a>Partial Stroking</h3><p>提升 HDD 硬盘效率的一个办法就是提高转速，这是很多厂商历史做的事情。</p><p>还有一种，<strong>空间换时间</strong>的方法，来提高硬盘的IOPS，就是 <code>Partial Stroking</code> 或者 <code>Short Stroking</code>。</p><p>一般硬盘的寻道时间都比平均延时要长，如果消除这部分，可以有效提高 IOPS。那就是磁头仅仅用最外道的磁道，或者只用1/4磁道。</p><p>不过这是当年互联网蓬勃发展时，工程师们想要改善性能问题的有偿解法。</p><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;全文内容主要来自对课程&lt;a href=&quot;https://time.geekbang.org/column/article/91427&quot;&gt;《深入浅出计算机组成原理》&lt;/a&gt;的学习笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;40-理解内</summary>
      
    
    
    
    <category term="学习笔记" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="计算机组成原理" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/"/>
    
    
    <category term="计算机原理" scheme="https://www.xiemingzhao.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%8E%9F%E7%90%86/"/>
    
    <category term="存储与I/O系统" scheme="https://www.xiemingzhao.com/tags/%E5%AD%98%E5%82%A8%E4%B8%8EI-O%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>深入浅出计算机组成原理——原理篇：存储与I/O系统（35-39）</title>
    <link href="https://www.xiemingzhao.com/posts/computerOrgArc35to39.html"/>
    <id>https://www.xiemingzhao.com/posts/computerOrgArc35to39.html</id>
    <published>2021-08-07T16:00:00.000Z</published>
    <updated>2025-04-08T16:40:22.949Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>全文内容主要来自对课程<a href="https://time.geekbang.org/column/article/91427">《深入浅出计算机组成原理》</a>的学习笔记。</p></blockquote><h2 id="35-存储器层次结构全景"><a href="#35-存储器层次结构全景" class="headerlink" title="35 | 存储器层次结构全景"></a>35 | 存储器层次结构全景</h2><h3 id="理解存储器的层次结构"><a href="#理解存储器的层次结构" class="headerlink" title="理解存储器的层次结构"></a>理解存储器的层次结构</h3><p><code>寄存器</code>：更像 CPU 本身的部分，空间极其有限，但速度非常快，<br><code>CPU Cache</code>：CPU 高速缓存，我们常常简称为“缓存”，使用 SRAM 芯片。</p><h3 id="SRAM"><a href="#SRAM" class="headerlink" title="SRAM"></a>SRAM</h3><p><code>SRAM</code>（Static Random-Access Memory），即静态随机存取存储器。只要处在通电状态，里面的数据就可以保持存在，一断电就丢失。</p><blockquote><p>SRAM 中，1 bit 的数据，需要 6～8 个晶体管。存储密度不高，但电路简单，速度快。</p></blockquote><p>CPU 有 L1、L2、L3 这样<code>三层高速缓存</code>：<br><code>L1 Cache</code>：每个 CPU 有自己的，在CPU内部，分成指令缓存和数据缓存；<br><code>L2 Cache</code>：每个 CPU 有自己的，不在CPU内，速度略慢；<br><code>L3 Cache</code>：多个 CPU 共享，尺寸大，速度更慢。</p><h3 id="DRAM"><a href="#DRAM" class="headerlink" title="DRAM"></a>DRAM</h3><p>内存和 Cache 不同，用 <code>DRAM</code>（Dynamic Random Access Memory，动态随机存取存储器）芯片。密度高，大容量。</p><p>DRAM 需要靠不断地“刷新”，才能保持数据被存储。1 bit只需要一个晶体管和一个电容就能存储。存储密度大，速度比 SRAM 慢。且电容会漏电，需要定时刷新充电。</p><h3 id="存储器的层级结构"><a href="#存储器的层级结构" class="headerlink" title="存储器的层级结构"></a>存储器的层级结构</h3><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac35p01.png" alt=""></p><p>可以看到，从 Cache 到 HDD，<strong>容量越来越大，价格越来越便宜，速度越来越慢。</strong><br>故，一个完整计算机，会通过不等层级的内存组合，来实现性价比：<br><strong>少量贵的存储保障热数据的速度，大量便宜的存储来提供冷数据存储</strong></p><p>并且，<strong>各个存储器只和相邻的一层存储器打交道</strong>。</p><hr><h2 id="36-局部性原理"><a href="#36-局部性原理" class="headerlink" title="36 | 局部性原理"></a>36 | 局部性原理</h2><p>服务端开发时，数据一般存在数据库，访问数据库性能瓶颈是要点，一般就会用缓存来缓解，</p><h3 id="理解局部性原理"><a href="#理解局部性原理" class="headerlink" title="理解局部性原理"></a>理解局部性原理</h3><blockquote><p>挑战：既要 CPU Cache 的速度，又要内存、硬盘巨大的容量和低廉的价格。</p></blockquote><p>解法便是<code>局部性原理</code>（Principle of Locality）：<code>时间局部性</code>（temporal locality）和<code>空间局部性</code>（spatial locality）</p><ul><li><code>时间局部性</code>：如果一个数据被访问了，那么它在短时间内还会被再次访问。</li><li><code>空间局部性</code>：如果一个数据被访问了，那么和它相邻的数据也很快会被访问。</li></ul><p>使用缓存的时候，例如<code>LRU</code>（Least Recently Used）缓存算法，需要关注<code>缓存命中率</code>，越高说明缓存效果越好。</p><hr><h2 id="37-高速缓存（上）"><a href="#37-高速缓存（上）" class="headerlink" title="37 | 高速缓存（上）"></a>37 | 高速缓存（上）</h2><h3 id="需要高速缓存"><a href="#需要高速缓存" class="headerlink" title="需要高速缓存"></a>需要高速缓存</h3><p>基于摩尔定律，CPU 和 内存的访问速度差异越来越大，为了缓解数据跟不上计算的问题，在CPU中就引入了高速缓存。</p><blockquote><p>在 95% 的情况下，CPU 都只需要访问 L1-L3 Cache，从里面读取指令和数据，而无需访问内存</p></blockquote><p>CPU 从内存中读取数据到 CPU Cache ，是以小块为单位的而不是单个元素，在 CPU Cache 里面，叫作 <code>Cache Line</code>（缓存块），常是 64 字节（Bytes）。</p><h3 id="Cache-的数据结构和读取过程是"><a href="#Cache-的数据结构和读取过程是" class="headerlink" title="Cache 的数据结构和读取过程是"></a>Cache 的数据结构和读取过程是</h3><p>CPU 读取数时：</p><ol><li>先访问 Cache；</li><li>有，则取出；</li><li>没有，再访问内存；</li><li>并将读取的数写入到 Cache。</li></ol><p>问题：CPU 如何知道需要访问的内存数据对应的 Cache 位置呢？<br>答案：<code>直接映射 Cache</code>（Direct Mapped Cache）。<br>思路：CPU 拿到的是数据所在的内存块（Block）的地址，其通过 mod 运算，<strong>固定映射到一个的 CPU Cache 地址（Cache Line）</strong>，作为<code>索引</code>。</p><blockquote><p>mod 运算的技巧：缓存块的数量设置成 2 的 N 次方，直接取地址的低 N 位就是 mod 结果。</p></blockquote><p>这时候，肯定会有多个内存块地址映射到同一个 Cache 地址，<strong>需要辨识当前存储的是哪一块。</strong><br>缓存块中，我们会存储一个<code>组标记</code>（Tag），记录当前缓存块内存储的数据对应的内存块。</p><p>此外，缓存块中还有：</p><ul><li><code>实际存放的数据</code>，一个 Block；</li><li><code>有效位</code>（valid bit），其0/1代表是否可用；</li><li><code>偏移量</code>（Offset），记录需要取的数据在 Block 中哪个位置。</li></ul><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac37p01.png" alt=""></p><p>“索引 + 有效位 + 组标记 + 数据”数据结构，使得 Cache 访问时有4步：</p><ol><li>取内存地址低位，计算 Cache 对应的索引；</li><li>根据有效位，判断 Cache 数据是否可用；</li><li>取内存地址高位，和组标记，确认数据是否符合为目标数据，从 Cache Line 中读取到对应的数据块（Data Block）；</li><li>根据内存地址的 Offset 位，从 Data Block 中，读取希望读取到的字。</li></ol><p>如在 2、3 步骤中发现数据不可用，那 CPU 就会访问内存，并把对应的 Block Data 更新到 Cache Line 中，同时更新有效位和组标记的数据。</p><hr><h2 id="38-高速缓存（下）"><a href="#38-高速缓存（下）" class="headerlink" title="38 | 高速缓存（下）"></a>38 | 高速缓存（下）</h2><h3 id="volatile-关键词"><a href="#volatile-关键词" class="headerlink" title="volatile 关键词"></a>volatile 关键词</h3><p>作者从 java 中“volatile”关键词出发，讨论它的作用和原理。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">VolatileTest</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">volatile</span> <span class="type">int</span> <span class="variable">COUNTER</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">ChangeListener</span>().start();</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">ChangeMaker</span>().start();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">ChangeListener</span> <span class="keyword">extends</span> <span class="title class_">Thread</span> &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">()</span> &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">threadValue</span> <span class="operator">=</span> COUNTER;</span><br><span class="line">            <span class="keyword">while</span> ( threadValue &lt; <span class="number">5</span>)&#123;</span><br><span class="line">                <span class="keyword">if</span>( threadValue!= COUNTER)&#123;</span><br><span class="line">                    System.out.println(<span class="string">&quot;Got Change for COUNTER : &quot;</span> + COUNTER + <span class="string">&quot;&quot;</span>);</span><br><span class="line">                    threadValue= COUNTER;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// try &#123; </span></span><br><span class="line">                <span class="comment">//     Thread.sleep(5); </span></span><br><span class="line">                <span class="comment">// &#125; catch (InterruptedException e) &#123; e.printStackTrace();&#125;</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">ChangeMaker</span> <span class="keyword">extends</span> <span class="title class_">Thread</span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">()</span> &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">threadValue</span> <span class="operator">=</span> COUNTER;</span><br><span class="line">            <span class="keyword">while</span> (COUNTER &lt;<span class="number">5</span>)&#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;Incrementing COUNTER to : &quot;</span> + (threadValue+<span class="number">1</span>) + <span class="string">&quot;&quot;</span>);</span><br><span class="line">                COUNTER = ++threadValue;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    Thread.sleep(<span class="number">500</span>);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123; e.printStackTrace(); &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>先说结果：</p><ul><li>直接运行，ChangeListener 能够监听 COUNTER 变化；</li><li>去掉 volatile 则不行；</li><li>去掉 volatile，但是又让 ChangeListener 每次 sleep 5ms 则又可以。</li></ul><blockquote><p>volatile 会确保我们对于这个变量的读取和写入，都一定会同步到主内存里，而不是从 Cache 里面读取。</p></blockquote><h3 id="写直达（Write-Through）"><a href="#写直达（Write-Through）" class="headerlink" title="写直达（Write-Through）"></a>写直达（Write-Through）</h3><blockquote><p>每一次数据都要写入到主内存里面。</p></blockquote><ul><li>先判断是否在 Cache；</li><li>在，先写 Cache，再写入主内存；</li><li>不在，直接写主内存。</li></ul><h3 id="写回（Write-Back）"><a href="#写回（Write-Back）" class="headerlink" title="写回（Write-Back）"></a>写回（Write-Back）</h3><blockquote><p>只写到 CPU Cache 里。只有当 CPU Cache 里面的数据要被“替换”的时候，我们才把数据写入到主内存里面去。</p></blockquote><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac38p01.jpg" alt=""></p><p>结合流程图，逻辑比较清晰。重点解释一下其中的“脏”的概念。<br><strong>标记“脏”：就是指这个时候，CPU Cache 里面的这个 Block 的数据，和主内存是不一致的。</strong></p><p>以上2中写法都需要考虑一个问题，就是在多线程/多CPU时缓存一致性的问题。</p><hr><h2 id="39-MESI协议解决缓存一致性"><a href="#39-MESI协议解决缓存一致性" class="headerlink" title="39 | MESI协议解决缓存一致性"></a>39 | MESI协议解决缓存一致性</h2><h3 id="缓存一致性问题"><a href="#缓存一致性问题" class="headerlink" title="缓存一致性问题"></a>缓存一致性问题</h3><p>假设有2核CPU，执行改价格任务，如果核1改了价格，写入到 Cache 中，在 Cache Block 交换出去前不会写入到内存，那么核2在这期间取到的数据就不一致。</p><p>为了解决此，需要做到：</p><ul><li>写传播：Cache 的更新必须同步到其他 CPU 核的 Cache里。</li><li>事务的串行化：按顺序执行修改，防止不同 CPU 核之间乱序。</li></ul><h3 id="总线嗅探机制和-MESI-协议"><a href="#总线嗅探机制和-MESI-协议" class="headerlink" title="总线嗅探机制和 MESI 协议"></a>总线嗅探机制和 MESI 协议</h3><p><strong>总线嗅探（Bus Snooping）</strong>：</p><blockquote><p>把所有的读写请求都通过总线（Bus）<code>广播</code>给所有的 CPU 核心，然后让各个核心去<code>嗅探</code>这些请求，再根据本地的情况进行响应。</p></blockquote><p><strong>写失效（Write Invalidate）协议</strong>：只有一个 CPU 核心负责写入数据，其他的核心，只是同步读取到这个写入。<br><strong>写广播（Write Broadcast）协议</strong>：一个写入请求广播到所有的 CPU 核心，同时更新各个核心里的 Cache，大家一起更新。</p><p>MESI 协议对 Cache Line 有 4 种标记：</p><ul><li><code>M</code>：代表已修改（Modified）；</li><li><code>E</code>：代表独占（Exclesive）；</li><li><code>S</code>：代表共享（Shared）；</li><li><code>I</code>：代表已失效（Invalidated）。</li></ul><p>M 和 I 都代表 Cache 和主内存数据不一致，即“脏”数据。E 和 S 都是一致的，但他们有区别：</p><ul><li>E 代表仅当前 CPU 的 Cache 里加载了这块数据，则可以自由写入；</li><li>S 代表有其他 CPU 也把同一块 Cache Block 从内存加载到其 Cache中，这时候写入就需要向所有 CPU 核广播请求（RFO，Request For Ownership）。</li></ul><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;全文内容主要来自对课程&lt;a href=&quot;https://time.geekbang.org/column/article/91427&quot;&gt;《深入浅出计算机组成原理》&lt;/a&gt;的学习笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;35-存储器</summary>
      
    
    
    
    <category term="学习笔记" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="计算机组成原理" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/"/>
    
    
    <category term="计算机原理" scheme="https://www.xiemingzhao.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%8E%9F%E7%90%86/"/>
    
    <category term="存储与I/O系统" scheme="https://www.xiemingzhao.com/tags/%E5%AD%98%E5%82%A8%E4%B8%8EI-O%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>深入浅出计算机组成原理——原理篇：处理器（30-34）</title>
    <link href="https://www.xiemingzhao.com/posts/computerOrgArc30to34.html"/>
    <id>https://www.xiemingzhao.com/posts/computerOrgArc30to34.html</id>
    <published>2021-08-02T16:00:00.000Z</published>
    <updated>2025-04-08T16:38:18.634Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>全文内容主要来自对课程<a href="https://time.geekbang.org/column/article/91427">《深入浅出计算机组成原理》</a>的学习笔记。</p></blockquote><h2 id="30-GPU（上）"><a href="#30-GPU（上）" class="headerlink" title="30 | GPU（上）"></a>30 | GPU（上）</h2><h3 id="GPU-的历史进程"><a href="#GPU-的历史进程" class="headerlink" title="GPU 的历史进程"></a>GPU 的历史进程</h3><p>GPU 是为了改善计算机渲染三维图像，先驱是 <code>SGI</code>（Silicon Graphics Inc.），即硅谷图形公司，创始人 Jim Clark 是斯坦福的教授，图形学专家。</p><p>最早 3D 是大神卡马克开发的著名 Wolfenstein 3D（德军总部 3D），从不同视角看到的是 8 幅不同的贴图，不是真正的3D。直到90年代中期，才出现实时渲染多边形。</p><h3 id="图形渲染的流程"><a href="#图形渲染的流程" class="headerlink" title="图形渲染的流程"></a>图形渲染的流程</h3><p>3D画面实际上是多边形组合出来的。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac30p01.png" alt=""></p><p>而完成这样的渲染需要5个步骤：</p><ol><li><code>顶点处理</code>（Vertex Processing）</li><li><code>图元处理</code>（Primitive Processing）</li><li><code>栅格化</code>（Rasterization）</li><li><code>片段处理</code>（Fragment Processing）</li><li><code>像素操作</code>（Pixel Operations）</li></ol><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac30p02.jpg" alt=""></p><h4 id="顶点处理"><a href="#顶点处理" class="headerlink" title="顶点处理"></a>顶点处理</h4><p>基于当前视角，把三维空间中的顶点转换到屏幕的二维空间上，即投影的过程。是一种通过线性代数计算得来，<strong>顶点坐标的计算无依赖，可并行。</strong></p><h4 id="图元处理"><a href="#图元处理" class="headerlink" title="图元处理"></a>图元处理</h4><p>将顶点处理后的各个顶点链接起来，变成多边形。但是，需要多做一个操作：<strong>剔除和裁剪（Cull and Clip）</strong>，即把不在屏幕里的内容去掉，减少后续工作量。（比如上图图元部分的 v0）</p><h4 id="栅格化"><a href="#栅格化" class="headerlink" title="栅格化"></a>栅格化</h4><p>屏幕分辨率有限，<code>栅格化</code>就是把多边形转成屏幕里的像素（Pixel）点（图元处理后的多边形覆盖的所有像素点）。这里<strong>每一个图元都可以并行独立地栅格化。</strong></p><h4 id="片段处理"><a href="#片段处理" class="headerlink" title="片段处理"></a>片段处理</h4><p>栅格化后的像素点只有黑白色，需要计算每一个像素点的颜色、透明度等进行上色，就是<code>片段处理</code>。</p><h4 id="像素操作"><a href="#像素操作" class="headerlink" title="像素操作"></a>像素操作</h4><p>最后就是将不同的多边形像素点混合到一起，输出到显示设备。</p><blockquote><p>以上5个渲染步骤就是<code>图形流水线</code>（Graphic Pipeline）。</p></blockquote><h3 id="解放图形渲染的-GPU"><a href="#解放图形渲染的-GPU" class="headerlink" title="解放图形渲染的 GPU"></a>解放图形渲染的 GPU</h3><p>计算一下渲染需要的资源：</p><blockquote><p>假设90年代，屏幕640x480，每秒60帧。从栅格化开始每个像素3个流水线步骤，每步即使1个指令。那么：$640 \times 480 \times 60 \times 3 = 54M$。然而，当时的CPU一般主频60MHz，基本上要被上述渲染占满，实际上每个渲染步骤不止一个指令。</p></blockquote><p>解决办法就是 Voodoo FX 这样的图形加速卡的出现。因为渲染流程固定，且计算逻辑简单（不需要流水线停顿、乱序执行等），并行度高，单独造硬件比用 CPU 划算。</p><p>于是，在当时整个顶点处理的过程还是都由 CPU 进行的，<strong>不过后续所有到图元和像素级别的处理都是通过显卡去处理的。</strong></p><hr><h2 id="31-GPU（下）"><a href="#31-GPU（下）" class="headerlink" title="31 | GPU（下）"></a>31 | GPU（下）</h2><h3 id="Shader-和可编程图形处理器"><a href="#Shader-和可编程图形处理器" class="headerlink" title="Shader 和可编程图形处理器"></a>Shader 和可编程图形处理器</h3><p>GPU 也在逐步优化迭代。首先，1999 年 NVidia 推出的 GeForce 256 显卡，就<strong>把顶点处理的计算能力，也从 CPU 里挪到了显卡里</strong>。但渲染过程都是固定管线，程序员不能干预，只能调配置。</p><p>从 2001 年的 Direct3D 8.0 开始，微软第一次引入了<code>可编程管线</code>（Programable Function Pipeline）的概念。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac31p01.jpg" alt=""></p><p>一开始的可编程管线呢，<strong>仅限于顶点处理（Vertex Processing）和片段处理（Fragment Processing）部分。</strong></p><p>可以编程的接口，我们称之为 <code>Shader</code>，即着色器，由可编程的模块功能决定。统一着色器架构（Unified Shader Architecture）就应运而生了。</p><p>顶点处理和片段处理上的逻辑不太一样，但指令集同一套。Vertex 和 Fragment Shader 分开，虽然设计简单，但资源有浪费，因为硬件串行的。于是，<strong>统一着色器架构（Unified Shader Architecture）就应运而生了。</strong></p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac31p02.jpg" alt=""></p><p>将 Shader 变成通用模块，多加一点。如此，可以把 GPU 拿来做各种通用计算的用法，即 GPGPU（General-Purpose Computing on Graphics Processing Units，通用图形处理器）。</p><h3 id="现代-GPU-的三个核心创意"><a href="#现代-GPU-的三个核心创意" class="headerlink" title="现代 GPU 的三个核心创意"></a>现代 GPU 的三个核心创意</h3><h4 id="芯片瘦身"><a href="#芯片瘦身" class="headerlink" title="芯片瘦身"></a>芯片瘦身</h4><p>现代 CPU 里的晶体管变得越来越多，越来越复杂，不是主要为了“计算”这个功能，而是：<strong>拿来处理乱序执行、进行分支预测，以及高速缓存部分。</strong></p><p>GPU 中没有上述那些，只有流式处理。只留下取指令、指令译码、ALU 以及执行这些计算需要的寄存器和缓存即可。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac31p03.jpg" alt=""></p><h4 id="多核并行和-SIMT"><a href="#多核并行和-SIMT" class="headerlink" title="多核并行和 SIMT"></a>多核并行和 SIMT</h4><p>基于瘦身后，可以在 GPU 中塞多一些并行电路，实现多核并行。</p><p>CPU 里有 SIMD 技术，在做向量计算的时候，我们要执行的指令是一样的，只是同一个指令的数据有所不同而已。</p><p>GPU 就借鉴了 SIMD，做了 SIMT（Single Instruction，Multiple Threads）的技术。比 SIMD 更加灵活，CPU 一次性取出了固定长度的多个数据，放到寄存器里面，用一个指令去执行。而 <strong>SIMT，可以把多条数据，交给不同的线程去处理。</strong></p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac31p04.jpg" alt=""></p><p>如上，在取指令和指令译码的阶段，可以给到后面多个不同的 ALU 并行进行运算。这样，就可以放下更多的 ALU，同时进行更多的并行运算了。</p><h4 id="GPU-里的“超线程”"><a href="#GPU-里的“超线程”" class="headerlink" title="GPU 里的“超线程”"></a>GPU 里的“超线程”</h4><p>GPU 的指令可能也遇到流水线停顿，而解决方案类似CPU，可以调度别的计算任务给当前 ALU。一个 Core 里面的执行上下文的数量，需要比 ALU 多。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac31p05.jpg" alt=""></p><h3 id="GPU-性能差异"><a href="#GPU-性能差异" class="headerlink" title="GPU 性能差异"></a>GPU 性能差异</h3><p>以 NVidia 2080 显卡为例，其算力如何？</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac31p06.jpg" alt=""></p><blockquote><p>2080 显卡有 46 个 SM（Streaming Multiprocessor，流式处理器），每个 SM（也就是 GPU Core）里有 64 个 Cuda Core（ALU 的数量），也就是 Shader。<br>还有 184 个 TMU，用来做纹理映射的计算单元，另一种 Shader。<br>主频是 1515MHz，如果自动超频（Boost）的话，可以到 1700MHz。每个时钟周期可以执行两条指令。<br>于是，浮点算力：（46 × 64 + 184）× 1700 MHz × 2 = 10.06 TFLOPS</p></blockquote><p>Intel i9 9900K 的性能不到 1TFLOPS，它们的价格却差不多，算力差10倍。</p><hr><h2 id="32-FPGA和ASIC"><a href="#32-FPGA和ASIC" class="headerlink" title="32 | FPGA和ASIC"></a>32 | FPGA和ASIC</h2><p>20 世纪末，计算机世界热衷于硬件的创新，21世纪初则转到了软件上。FPGA 和 ASIC 是2类经典的芯片。</p><h3 id="FPGA"><a href="#FPGA" class="headerlink" title="FPGA"></a>FPGA</h3><p>一个四核 i7 的 Intel CPU，晶体管数量差不多有 20 亿个，设计和制作很困难。周期一般几月到几年，期间还需要各种测试，如果每次重做成本太高。</p><p>于是，编程化思想，能否制作一个硬件，通过不同的程序代码，来操作这个硬件之前的电路连线，以形成需要的芯片？有，<code>FPGA</code>，即<code>现场可编程门阵列</code>（Field-Programmable Gate Array）。</p><p>FPGA 有很多门电路，可以反复烧录，组合实现不同功能芯片。但如何编程呢？<br><strong>1. 用存储换功能实现组合逻辑；</strong></p><blockquote><p>其基本电路不用布线连接，而是通过软件设计真值表，存到LUT（Look-Up Table，查找表）这一存储空间中，其实就是不是真计算，而是存储不同逻辑计算结果映射关系。</p></blockquote><p><strong>2. 对于需要实现的时序逻辑电路，直接放上 D 触发器，作为寄存器。</strong></p><blockquote><p>多个 LUT 的电路和寄存器组合成一个<code>逻辑簇</code>（Logic Cluster）的东西。在 FPGA 里，它也被叫做 CLB（Configurable Logic Block，可配置逻辑块）。基于 CLB 可以搭建我们需要的芯片。</p></blockquote><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac32p01.jpg" alt=""></p><p><strong>3. FPGA 是通过可编程逻辑布线，来连接各个不同的 CLB 组成芯片功能。</strong></p><blockquote><p>在 CLB 之间留有很多电路开关，通过控制此便可以实现不同 CLB 之间的连接方式。</p></blockquote><h3 id="ASIC"><a href="#ASIC" class="headerlink" title="ASIC"></a>ASIC</h3><p>除了 CPU、GPU、FPGA 这些通用型芯片外，日常中经常需要一些专用芯片处理某些固定任务。比如录音笔的音频芯片，遥控汽车的芯片等。</p><p>这种专用芯片一般称为 <code>ASIC</code>（Application-Specific Integrated Circuit），也就是专用集成电路。往往电路精简、设计简单、功耗低、成本低。</p><p>但，FPGA 成熟，没有硬件研发成本；ASIC 需要仿真、验证，还需要经过流片（Tape out）。单独研发成本也不一定低。</p><hr><h2 id="33-解读TPU"><a href="#33-解读TPU" class="headerlink" title="33 | 解读TPU"></a>33 | 解读TPU</h2><p>TPU 是过去几年比较知名的 ASIC。</p><h3 id="TPU-V1-的来源"><a href="#TPU-V1-的来源" class="headerlink" title="TPU V1 的来源"></a>TPU V1 的来源</h3><p>深度学习火起来后，计算量最大的是模型推理部分，第一代 TPU 便是为了优化此。</p><p>目标：</p><ol><li>响应快；</li><li>功耗低。</li></ol><h3 id="深入理解-TPU-V1"><a href="#深入理解-TPU-V1" class="headerlink" title="深入理解 TPU V1"></a>深入理解 TPU V1</h3><p><strong>TPU 需要快速上线和向前兼容。</strong></p><blockquote><p>TPU 设计成可插拔的板卡，甚至没有取指令的电路，而是通过 CPU，向 TPU 发送需要执行的指令。</p></blockquote><p><strong>专用电路和大量缓存</strong></p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac33p01.jpg" alt=""></p><p>其中，</p><ul><li>矩阵乘法单元（Matrix Multiply Unit）；</li><li>累加器（Accumulators）模块；</li><li>激活函数（Activation）模块和归一化 / 池化（Normalization/Pool）模块<br>是顺序串联在一起的。</li></ul><p>控制电路（Control）只占了 2%，没有冒险、分支预测等等。<br>超过一半的 TPU 的面积：作为 <code>本地统一缓冲区</code>（Local Unified Buffer）（29%）和<code>矩阵乘法单元</code>（Matrix Mutliply Unit）(24%)。其中，缓冲区使用 SRAM，比起内存使用的 DRAM 速度要快上很多，利于深度模型推理的高频读写。</p><p><strong>细节优化，使用 8 Bits 数据</strong><br>正常深度模型使用 32Bits 来处理浮点数，而在 TPU 内使用 8Bits。这是一种对深度模型进行 int8 量化的方案，可以使得存储更小，计算更快。</p><blockquote><p>当然，如果对精度比较依赖的话，可能会成为弊端。</p></blockquote><hr><h2 id="34-理解虚拟机"><a href="#34-理解虚拟机" class="headerlink" title="34 | 理解虚拟机"></a>34 | 理解虚拟机</h2><blockquote><p>如何让空闲的机器分时段分大小租给不同需求的用户。</p></blockquote><h3 id="分时系统"><a href="#分时系统" class="headerlink" title="分时系统"></a>分时系统</h3><p>多个终端连接同一个主机，会自动给程序或任务分配计算时间。</p><h3 id="公有云"><a href="#公有云" class="headerlink" title="公有云"></a>公有云</h3><p>早期亚马逊租服务器只能整租，起步配置高，且换用户的时候需要清空数据和程序。</p><p><code>虚拟机技术</code>，可以在一台物理服务器上，同时运行多个虚拟服务器，并且可以动态去分配，每个虚拟服务器占用的资源。不需要的可以关闭服务器，并保留数据资源。</p><h3 id="虚拟机的技术变迁"><a href="#虚拟机的技术变迁" class="headerlink" title="虚拟机的技术变迁"></a>虚拟机的技术变迁</h3><p><code>虚拟机</code>（Virtual Machine）技术，在现有硬件的操作系统上，模拟一个计算机系统的技术。</p><p><strong>解释型虚拟机</strong><br>模拟系统最简单的就是兼容这个计算机系统的指令集。</p><p>这种解释执行方式的最大的优势就是，模拟的系统可以跨硬件。</p><p>但，有缺陷：</p><ul><li>无法精确“模拟”；</li><li>性能差。</li></ul><p><strong>虚拟机的性能提升</strong><br>为了克服上述缺陷，又要支持一个操作系统上跑多个完整的操作系统，方案就是<strong>加入一个中间层</strong>。即<code>虚拟机监视器</code>，英文叫 VMM（Virtual Machine Manager）或者 Hypervisor。</p><p>实际的指令是怎么落到硬件上去实际执行？</p><p><code>Type-2 虚拟机</code>：像一个运行在操作系统上的软件。对于最终到硬件的指令，客户机的操作系统-&gt;虚拟机监视器-&gt;宿主机的操作系统。</p><blockquote><p>只是把在模拟器里的指令翻译工作，挪到了虚拟机监视器里。更多是用在我们日常的个人电脑里，而不是用在数据中心里。</p></blockquote><p><code>Type-1 虚拟机</code>：客户机的指令交给虚拟机监视器之后呢，不再需要通过宿主机的操作系统调用硬件，而是可以直接由虚拟机监视器去调用硬件。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac34p01.jpg" alt=""></p><h3 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h3><p>Type-1 虚拟机缺点：实际的物理机上，我们可能同时运行了多个的虚拟机，每个都运行了一个属于自己的单独的操作系统。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac34p02.jpg" alt=""></p><p>不管依赖什么，其实都是跑在 Linux 内核上的。通过 Docker，我们不再需要在操作系统上再跑一个操作系统，而只需要通过容器编排工具（如 Kubernetes），能够进行各个应用之间的<strong>环境和资源隔离就好了。</strong></p><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;全文内容主要来自对课程&lt;a href=&quot;https://time.geekbang.org/column/article/91427&quot;&gt;《深入浅出计算机组成原理》&lt;/a&gt;的学习笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;30-GPU</summary>
      
    
    
    
    <category term="学习笔记" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="计算机组成原理" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/"/>
    
    
    <category term="计算机原理" scheme="https://www.xiemingzhao.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%8E%9F%E7%90%86/"/>
    
    <category term="处理器" scheme="https://www.xiemingzhao.com/tags/%E5%A4%84%E7%90%86%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>深入浅出计算机组成原理——原理篇：处理器（22-29）</title>
    <link href="https://www.xiemingzhao.com/posts/computerOrgArc22to29.html"/>
    <id>https://www.xiemingzhao.com/posts/computerOrgArc22to29.html</id>
    <published>2021-07-27T16:00:00.000Z</published>
    <updated>2025-04-08T16:34:44.076Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>全文内容主要来自对课程<a href="https://time.geekbang.org/column/article/91427">《深入浅出计算机组成原理》</a>的学习笔记。</p></blockquote><h2 id="22-冒险和预测（一）"><a href="#22-冒险和预测（一）" class="headerlink" title="22 | 冒险和预测（一）"></a>22 | 冒险和预测（一）</h2><p>流水线设计需要解决的<code>三大冒险</code>：</p><ul><li>结构冒险（Structural Hazard）；</li><li>数据冒险（Data Hazard）；</li><li>控制冒险（Control Hazard）</li></ul><h3 id="结构冒险"><a href="#结构冒险" class="headerlink" title="结构冒险"></a>结构冒险</h3><p><strong>本质上是一个硬件层面的资源竞争问题。</strong></p><blockquote><p>CPU 在同一个时钟周期，同时运行两条指令的不同阶段。但是可能会用到同样的硬件电路。</p></blockquote><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac22p01.png" alt=""></p><p>如上图所示就是内存读取的结构冒险。因为内存只有一个地址译码器的作为地址输入，在一个时钟周期里面只能读取一条数据。</p><p>一个<strong>直观的方案：内存分成两部分（存放指令的程序内存和存放数据的数据内存），各有各的地址译码器</strong>。这称为<a href="https://en.wikipedia.org/wiki/Harvard_architecture">哈佛架构</a>（Harvard Architecture）。有弊端：没法动态分配内存了。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac22p02.jpg" alt=""></p><p>然而，现在都是冯·诺依曼体系结构，其参考上述，在 CPU 内部加了高速缓存部分，主要是为了缓解访问内存速度过慢于 CPU。但在这里把高速缓存分成了<code>指令缓存</code>（Instruction Cache）和<code>数据缓存</code>（Data Cache）两部分。CPU 并不会直接读取主内存。它会从主内存把指令和数据加载到高速缓存中，这样后续的访问都是访问高速缓存。<strong>解法的本质都是增加资源。</strong></p><h3 id="数据冒险"><a href="#数据冒险" class="headerlink" title="数据冒险"></a>数据冒险</h3><p>三大类：</p><ul><li><code>先写后读</code>（Read After Write，RAW）——数据依赖；</li><li><code>先读后写</code>（Write After Read，WAR）——反依赖；</li><li><code>写后再写</code>（Write After Write，WAW）——输出依赖。</li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// RAW</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span> &#123;</span><br><span class="line">  <span class="type">int</span> a = <span class="number">1</span>;</span><br><span class="line">  <span class="type">int</span> b = <span class="number">2</span>;</span><br><span class="line">  a = a + <span class="number">2</span>;</span><br><span class="line">  b = a + <span class="number">3</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// WAR</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span> &#123;</span><br><span class="line">  <span class="type">int</span> a = <span class="number">1</span>;</span><br><span class="line">  <span class="type">int</span> b = <span class="number">2</span>;</span><br><span class="line">  a = b + a;</span><br><span class="line">  b = a + b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// WAW</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span> &#123;</span><br><span class="line">  <span class="type">int</span> a = <span class="number">1</span>;</span><br><span class="line">  a = <span class="number">2</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="通过流水线停顿解决数据冒险"><a href="#通过流水线停顿解决数据冒险" class="headerlink" title="通过流水线停顿解决数据冒险"></a>通过流水线停顿解决数据冒险</h3><p>冲突：</p><ul><li>流水线架构的核心是在前一个指令还没有结束的时候，后面的指令就要开始执行。</li><li>但，对于同一个寄存器或者内存地址的操作，都有明确强制的顺序要求。</li></ul><p>解法： <code>流水线停顿</code>（Pipeline Stall），或者叫<code>流水线冒泡</code>（Pipeline Bubbling）。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac22p03.jpg" alt=""></p><p>在执行后面的操作步骤前面，插入一个 NOP 操作，也就是执行一个其实什么都不干的操作。</p><hr><h2 id="23-冒险和预测（二）"><a href="#23-冒险和预测（二）" class="headerlink" title="23 | 冒险和预测（二）"></a>23 | 冒险和预测（二）</h2><p>前面两种冒险的解决方案可以归纳为“加资源”和“加时间”。这里介绍一个更有效的方案：<code>操作数前推</code>。</p><h3 id="NOP-操作和指令对齐"><a href="#NOP-操作和指令对齐" class="headerlink" title="NOP 操作和指令对齐"></a>NOP 操作和指令对齐</h3><p>五级流水线：“取指令（IF）- 指令译码（ID）- 指令执行（EX）- 内存访问（MEM）- 数据写回（WB） ”。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac23p01.jpg" alt=""></p><p>但并不是所有的指令都需要完全的5级流水线，如上表，STORE 和 ADD/SUB 就分别不需要 WB 和 MEM 操作</p><p>但是我们并不能跳过对应的阶段直接执行下一阶段，否则容易出现结构冒险，例如 LOAD 指令和 ADD 先后执行的时候，WB 是在统一时钟周期，所以需要针对确实的阶段进行插入 NOP。</p><h3 id="操作数前推"><a href="#操作数前推" class="headerlink" title="操作数前推"></a>操作数前推</h3><p>插入过多的 NOP 操作，带来的坏处就是浪费了CPU的资源。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">add $t0, $s2,$s1</span><br><span class="line">add $s2, $s1,$t0</span><br></pre></td></tr></table></figure><p>上述2行 code 的流水线如下，后者依赖前者计算结果。为了流水线对齐和结构依赖，多了4个 NOP 的操作。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac23p02.jpg" alt=""></p><p>实际上，第二条指令未必要等待第一条指令写回完成。可将第一条指令的执行结果直接传输给第二条指令的执行阶段。如下图所示，就叫作<code>操作数前推</code>（Operand Forwarding），或者<code>操作数旁路</code>（Operand Bypassing）。</p><blockquote><p>它的实现是 CPU 的硬件里面，再单独拉一根信号传输的线路出来，使得 ALU 的计算结果能够重新回到 ALU 的输入里。</p></blockquote><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac23p03.jpg" alt=""></p><hr><h2 id="24-冒险和预测（三）"><a href="#24-冒险和预测（三）" class="headerlink" title="24 | 冒险和预测（三）"></a>24 | 冒险和预测（三）</h2><h3 id="填上空闲的-NOP"><a href="#填上空闲的-NOP" class="headerlink" title="填上空闲的 NOP"></a>填上空闲的 NOP</h3><p><strong>流水线停顿的时候，对应的电路闲着，可以先完成后面指令的执行阶段。</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = b + c</span><br><span class="line">d = a * e</span><br><span class="line">x = y * z</span><br></pre></td></tr></table></figure><p>如上，后面的指令不依赖前面的，那就不用等，可以先执行。这就是<code>乱序执行</code>（Out-of-Order Execution，OoOE）</p><h3 id="CPU-里的“线程池”"><a href="#CPU-里的“线程池”" class="headerlink" title="CPU 里的“线程池”"></a>CPU 里的“线程池”</h3><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac24p01.jpg" alt=""></p><p>乱序执行的流水线不同于历史的5级流水线，如上图：</p><ol><li>取指令和指令译码没有变化；</li><li>译码后，不直接执行，先分发到<code>保留站</code>（Reservation Stations）；</li><li>这些指令等待依赖的数据，等到后才交到 ALU 执行；；</li><li>结果也不直接写回寄存器，先存在<code>重排缓冲区</code>（Re-Order Buffer，ROB）；</li><li>CPU 按照取指令的顺序，对结果重新排序，从前往后依赖提交完成；</li><li>结果数据也不直接写内存，先写入<code>存储缓冲区</code>(Store Buffer)后再写。</li></ol><blockquote><p><strong>即使执行乱序，但最终结果会排序，确保写入内存和寄存器是有序的</strong></p></blockquote><hr><h2 id="25-冒险和预测（四）"><a href="#25-冒险和预测（四）" class="headerlink" title="25 | 冒险和预测（四）"></a>25 | 冒险和预测（四）</h2><p>所有的流水线停顿都从指令执行开始，但取指令和指令译码不需要任何停顿。当然，<strong>这有一个前提：所有的指令代码都是顺序加载执行的。</strong></p><p>但是，遇到条件分支时就不成立：</p><blockquote><p>要等 jmp 指令执行完成，去更新了 PC 寄存器之后，才能判断是否执行下一条指令，还是跳转到另外内存地址，去取别的指令。</p></blockquote><p>上述提到的就是<code>控制冒险</code>。</p><h3 id="分支预测"><a href="#分支预测" class="headerlink" title="分支预测"></a>分支预测</h3><h4 id="缩短分支延迟"><a href="#缩短分支延迟" class="headerlink" title="缩短分支延迟"></a>缩短分支延迟</h4><p>可以将条件判断、地址跳转，<strong>都提前到指令译码阶段进行</strong>。CPU 里面设计对应的旁路，在指令译码阶段，就提供对应的判断比较的电路，节省等待时间。<br>但并不能彻底解决问题，跳转指令的比较结果，仍然要在指令执行的时候才能知道。</p><h4 id="分支预测-1"><a href="#分支预测-1" class="headerlink" title="分支预测"></a>分支预测</h4><p>让 CPU 预测下一跳执行指令，无非 2 选 1，最朴素的就是<strong>假装分支不发生</strong>，即<code>静态预测</code>技术。统计学角度，约50%正确率。</p><h4 id="动态分支预测"><a href="#动态分支预测" class="headerlink" title="动态分支预测"></a>动态分支预测</h4><p>上面一种属实太随机，实际上可以<strong>根据之前条件跳转的比较结果来预测</strong>。</p><p>类似于天气预报，如果始终选择跟上次状态一样，便是<code>一级分支预测</code>（One Level Branch Prediction），或者叫 <code>1 比特饱和计数</code>（1-bit saturating counter）。</p><p>进一步提升，我们引入一个<code>状态机</code>（State Machine）如下图，4 个状态，所以需要 2 个比特来记录。这样这整个策略，就可以叫作 <code>2 比特饱和计数</code>，或者叫<code>双模态预测器</code>（Bimodal Predictor）。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac25p01.jpg" alt=""></p><h3 id="循环嵌套的改变会影响性能"><a href="#循环嵌套的改变会影响性能" class="headerlink" title="循环嵌套的改变会影响性能"></a>循环嵌套的改变会影响性能</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">public class BranchPrediction &#123;</span><br><span class="line">    public static void main(String args[]) &#123;        </span><br><span class="line">        long start = System.currentTimeMillis();</span><br><span class="line">        for (int i = 0; i &lt; 100; i++) &#123;</span><br><span class="line">            for (int j = 0; j &lt;1000; j ++) &#123;</span><br><span class="line">                for (int k = 0; k &lt; 10000; k++) &#123;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        long end = System.currentTimeMillis();</span><br><span class="line">        System.out.println(&quot;Time spent is &quot; + (end - start));</span><br><span class="line">                </span><br><span class="line">        start = System.currentTimeMillis();</span><br><span class="line">        for (int i = 0; i &lt; 10000; i++) &#123;</span><br><span class="line">            for (int j = 0; j &lt;1000; j ++) &#123;</span><br><span class="line">                for (int k = 0; k &lt; 100; k++) &#123;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        end = System.currentTimeMillis();</span><br><span class="line">        System.out.println(&quot;Time spent is &quot; + (end - start) + &quot;ms&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如上述2种嵌套循环代码，性能差异很大，主要是因为：</p><ol><li>每次循环需要 cmp 和 jle 指令，后者就需要分支预测；</li><li>最内层只有最后一次会预测错（跳到外层），故外层循环次数越多，整体预测错的越多。</li></ol><hr><h2 id="26-Superscalar-和-VLIW"><a href="#26-Superscalar-和-VLIW" class="headerlink" title="26 | Superscalar 和 VLIW"></a>26 | Superscalar 和 VLIW</h2><p>程序的 CPU 执行时间 = 指令数 × CPI × Clock Cycle Time</p><p>CPI 的倒数，即 IPC（Instruction Per Clock），也就是一个时钟周期里面能够执行的指令数，代表了 CPU 的<code>吞吐率</code>。</p><p><strong>最佳情况下，IPC 也只能到 1</strong></p><p>即一个时钟周期也只能执行完取一条指令，但有办法突破。</p><h3 id="多发射与超标量"><a href="#多发射与超标量" class="headerlink" title="多发射与超标量"></a>多发射与超标量</h3><p>乱序执行的时候，你会看到，其实指令的执行阶段，是由很多个功能单元（FU）并行（Parallel）进行的。</p><p><em>取指令（IF）和指令译码（ID）部分并不是并行进行的。如何实现并行？</em></p><p>其实只要我们把取指令和指令译码，也一样通过增加硬件的方式。一次性从内存里面取出多条指令，然后分发给多个并行的指令译码器，进行译码，然后对应交给不同的功能单元去处理。</p><p>这种 CPU 设计，我们叫作<code>多发射</code>（Mulitple Issue）和<code>超标量</code>（Superscalar）。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac26p01.jpg" alt=""></p><p>如此，流水线就会有所变化，</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac26p02.jpg" alt=""></p><h3 id="Intel-失败的超长指令字"><a href="#Intel-失败的超长指令字" class="headerlink" title="Intel 失败的超长指令字"></a>Intel 失败的超长指令字</h3><p>乱序执行和超标量在硬件层面实现都很复杂，因为要解决依赖冲突问题，所以需要考虑放到软件里面做。</p><p>通过编译器来优化指令数，一个 CPU 设计叫作<code>超长指令字设计</code>（Very Long Instruction Word，VLIW）。即 IA-64 架构的安腾（Itanium）处理器，使用<code>显式并发指令运算</code>（Explicitly Parallel Instruction Computer）。</p><p>在超长指令字架构里，将检测指令的前后依赖关系由 CPU 硬件电路转到了编译器。</p><blockquote><p>让编译器把没有依赖关系的代码位置进行交换。然后，再把多条连续的指令打包成一个指令包，安腾是3.</p></blockquote><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac26p03.jpg" alt=""></p><p>其失败的重要原因——<strong>向前兼容</strong>：</p><ol><li>与x86指令集不同，x86的程序全部要重新编译；</li><li>想要提升并行度，需要增加指令包里的指令数量，就需要重新编译。</li></ol><blockquote><p>在 Intel 的 x86 的 CPU 里，从 Pentium 时代，第一次开始引入超标量技术，整个 CPU 的性能上了一个台阶。依赖于在硬件层面，能够检测到对应的指令的先后依赖关系，解决“冒险”问题。所以，它也使得 CPU 的电路变得更复杂了。</p></blockquote><hr><h2 id="27-SIMD：加速矩阵乘法"><a href="#27-SIMD：加速矩阵乘法" class="headerlink" title="27 | SIMD：加速矩阵乘法"></a>27 | SIMD：加速矩阵乘法</h2><h3 id="超线程"><a href="#超线程" class="headerlink" title="超线程"></a>超线程</h3><p>2002 年底，Intel 在的 3.06GHz 主频的 Pentium 4 CPU 上，第一次引入了<code>超线程</code>（Hyper-Threading）技术。</p><p><strong>朴素思想：找一些没有依赖完全独立的指令来并行运算</strong>。不同的程序貌似天然符合该要求。</p><p>看上去没有什么技术，但实际上我们并没有真正地做到指令的并行运行：</p><blockquote><p>在同一时间点上，一个物理的 CPU 核心只会运行一个线程的指令。</p></blockquote><p>超线程的 CPU，其实是把一个物理层面 CPU 核心，“伪装”成两个逻辑层面的 CPU 核心。硬件上增加很多电路，<strong>使得一个 CPU 维护两个不同线程的指令的状态信息</strong>。其中会有双份的 PC 寄存器、指令寄存器乃至条件码寄存器，不过指令译码器还是 ALU等其他组件没有双份。超线程技术一般也被叫作<code>同时多线程</code>（Simultaneous Multi-Threading，简称 SMT）技术。</p><h3 id="SIMD"><a href="#SIMD" class="headerlink" title="SIMD"></a>SIMD</h3><p>SIMD，中文叫作<code>单指令多数据流</code>（Single Instruction Multiple Data）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> timeit</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">1000</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = np.array(<span class="built_in">range</span>(<span class="number">1000</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>timeit.timeit(<span class="string">&quot;[i + 1 for i in a]&quot;</span>, setup=<span class="string">&quot;from __main__ import a&quot;</span>, number=<span class="number">1000000</span>)</span><br><span class="line"><span class="number">32.82800309999993</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>timeit.timeit(<span class="string">&quot;np.add(1, b)&quot;</span>, setup=<span class="string">&quot;from __main__ import np, b&quot;</span>, number=<span class="number">1000000</span>)</span><br><span class="line"><span class="number">0.9787889999997788</span></span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure><p>上述两种计算法，性能差30多倍，主要因为：<br><strong>NumPy 直接用到了 SIMD 指令，能够并行进行向量的操作。</strong></p><p><code>SIMD</code> 在获取数据和执行指令的时候，都做到了并行。且在从内存里面读取数据的时候，SIMD 是一次性读取多个数据。</p><p>正是 SIMD 技术的出现，使得我们在 Pentium 时代的个人 PC，开始有了多媒体运算的能力。</p><hr><h2 id="28-异常和中断"><a href="#28-异常和中断" class="headerlink" title="28 | 异常和中断"></a>28 | 异常和中断</h2><h3 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h3><p>这里不是指 Exception 这种“软件异常”，而是和硬件、系统相关的“硬件异常”。</p><p>比如，除以 0，溢出，CPU 运行程序时收到键盘输入信号等。计算机会为每一种可能会发生的异常，分配一个异常代码（Exception Number）。</p><blockquote><p>这些异常代码里，I/O 发出的信号的异常代码，是由操作系统来分配的，也就是由软件来设定的。而像加法溢出这样的异常代码，则是由 CPU 预先分配好的，也就是由硬件来分配的。</p></blockquote><p>内存中有一个<code>异常表</code>（Exception Table），也叫作<code>中断向量表</code>（Interrupt Vector Table），存放的是不同的异常代码对应的异常处理程序（Exception Handler）所在的地址。</p><h3 id="异常的分类"><a href="#异常的分类" class="headerlink" title="异常的分类"></a>异常的分类</h3><ul><li>中断（Interrupt）：程序员执行时被打断，一般来自 I/O 设备。</li><li>陷阱（Trap）：程序员“故意“主动触发的异常，类似断点。</li><li>故障（Fault）：陷阱是我们开发程序的时候刻意触发的异常，而故障通常不是。</li><li>中止（Abort）：CPU 遇到故障无法恢复时需要终止。</li></ul><div class="table-container"><table><thead><tr><th>类型</th><th>原因</th><th>示例</th><th>触发时机</th><th>处理后操作</th></tr></thead><tbody><tr><td>中断</td><td>I/O设备信号</td><td>用户键盘输入</td><td>异步</td><td>下一条指令</td></tr><tr><td>陷阱</td><td>程序刻意触发</td><td>程序进行系统调用</td><td>同步</td><td>下一条指令</td></tr><tr><td>故障</td><td>程序执行出错</td><td>程序加载的缺页错误</td><td>同步</td><td>当前指令</td></tr><tr><td>终止</td><td>故障无法恢复</td><td>ECC内存校验失败</td><td>同步</td><td>退出程序</td></tr></tbody></table></div><ul><li>异步：中断异常的信号来自系统外部，而不是在程序自己执行的过程中；</li><li>同步：在程序执行的过程中发生的。</li></ul><p><strong>处理流程：保存现场、异常代码查询、异常处理程序调用“</strong></p><h3 id="异常的处理"><a href="#异常的处理" class="headerlink" title="异常的处理"></a>异常的处理</h3><p>切换到异常处理程序，像两个不同的独立进程之间在 CPU 层面的切换，所以这个过程我们称之为<code>上下文切换</code>（Context Switch）。</p><p>难点：</p><ol><li>异常情况往往发生在程序正常执行的预期之外；</li><li>像陷阱类，涉及程序指令在用户态和内核态之间的切换；</li><li>像故障类，在异常处理程序执行完成之后。</li></ol><hr><h2 id="29-CISC和RISC指令集"><a href="#29-CISC和RISC指令集" class="headerlink" title="29 | CISC和RISC指令集"></a>29 | CISC和RISC指令集</h2><p>CPU 的指令集可分：</p><ul><li><code>复杂指令集</code>（Complex Instruction Set Computing，简称 <code>CISC</code>），机器码是固定长度;</li><li><code>精简指令集</code>（Reduced Instruction Set Computing，简称 <code>RISC</code>），机器码是可变长度。</li></ul><h3 id="CISC-VS-RISC"><a href="#CISC-VS-RISC" class="headerlink" title="CISC VS RISC"></a>CISC VS RISC</h3><p>CISC 的挑战：</p><ul><li>在硬件层，支持更多的复杂指令，电路更复杂，设计更困难，散热和功耗更高。</li><li>在软件层，因为指令更多，编译器的优化更困难。</li></ul><p>最早只有 CISC，70年代末，大卫·帕特森（David Patterson）发现<strong>在 CPU 运行的程序里，80% 的时间都是在使用 20% 的简单指令</strong>。于是提出了 RISC。</p><ul><li>CISC 的架构，通过<strong>优化指令数</strong>，来减少 CPU 的执行时间。</li><li>RISC 的架构，在<strong>优化 CPI</strong>，指令简单，需要的时钟周期就比较少。</li></ul><h3 id="微指令架构"><a href="#微指令架构" class="headerlink" title="微指令架构"></a>微指令架构</h3><p>指令集的<strong>向前兼容性</strong>，即历史程序是否废弃，是 Intel 想要放弃 x86 重点要考虑的问题。</p><blockquote><p>x86 下的 64 位的指令集 x86-64，并不是 Intel 发明的，而是 AMD 发明的。</p></blockquote><p>Intel 在微指令架构的 CPU 里面，译码器会把一条机器码，“翻译”成好几条“微指令”，使之变成了固定长度的 RISC 风格的了。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac29p01.jpg" alt=""></p><p>如上，指令译码器变复杂，性能又有浪费。但因为“二八现象”的存在，<strong>对于这种有着很强局部性的问题，常见的解决方案就是使用缓存</strong>。<br>于是，Intel 加了一层 <code>L0 Cache</code> 来保存 CISC 翻译成 RISC 的微指令。不仅优化了性能，因为译码器的晶体管开关动作变少了，还减少了功耗。</p><p>由于 Intel 本身在 CPU 层面做的大量优化，比如乱序执行、分支预测等。故 x86 的 CPU 始终在功耗上还是要远远超过 RISC 架构的 ARM，所以最终在智能手机崛起替代 PC 的时代，落在了 ARM 后面。</p><h3 id="ARM-和-RISC-V"><a href="#ARM-和-RISC-V" class="headerlink" title="ARM 和 RISC-V"></a>ARM 和 RISC-V</h3><p>ARM 能够在移动端战胜 Intel，并不是因为 RISC 架构。</p><p>CISC 和 RISC 的分界已经没有那么明显了。Intel 和 AMD 的 CPU 也都是采用译码成 RISC 风格的微指令来运行。而 ARM 的芯片，一条指令同样需要多个时钟周期，有乱序执行和多发射。</p><p><strong>核心差异是</strong>：</p><ul><li>功耗优先的设计<br>一个 4 核的 Intel i7 的 CPU，设计的功率就是 130W。而 ARM A8 的单个核心的 CPU，设计功率只有 2W。</li><li>低价。<br>ARM 只是进行 CPU 设计，然后产权授权出去。尽管出货量远大于 Intel，但是收入和利润却比不上 Intel。</li></ul><p>图灵奖的得主大卫·帕特森教授从伯克利退休之后，成了 RISC-V 国际开源实验室的负责人，开始推动 RISC-V 这个开源 CPU 的开发。</p><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;全文内容主要来自对课程&lt;a href=&quot;https://time.geekbang.org/column/article/91427&quot;&gt;《深入浅出计算机组成原理》&lt;/a&gt;的学习笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;22-冒险和</summary>
      
    
    
    
    <category term="学习笔记" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="计算机组成原理" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/"/>
    
    
    <category term="计算机原理" scheme="https://www.xiemingzhao.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%8E%9F%E7%90%86/"/>
    
    <category term="处理器" scheme="https://www.xiemingzhao.com/tags/%E5%A4%84%E7%90%86%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>深入浅出计算机组成原理——原理篇：处理器（17-21）</title>
    <link href="https://www.xiemingzhao.com/posts/computerOrgArc17to21.html"/>
    <id>https://www.xiemingzhao.com/posts/computerOrgArc17to21.html</id>
    <published>2021-07-18T16:00:00.000Z</published>
    <updated>2025-04-08T16:29:33.882Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>全文内容主要来自对课程<a href="https://time.geekbang.org/column/article/91427">《深入浅出计算机组成原理》</a>的学习笔记。</p></blockquote><h2 id="17-建立数据通路（上）"><a href="#17-建立数据通路（上）" class="headerlink" title="17 | 建立数据通路（上）"></a>17 | 建立数据通路（上）</h2><h3 id="指令周期（Instruction-Cycle）"><a href="#指令周期（Instruction-Cycle）" class="headerlink" title="指令周期（Instruction Cycle）"></a>指令周期（Instruction Cycle）</h3><p>一条指令的执行过程：</p><ol><li><strong>Fetch</strong>：取得指令</li><li><strong>Decode</strong>：指令译码</li><li><strong>Execute</strong>：执行指令</li></ol><p>（重复以上步骤）</p><p>上述的一个循环称为<code>指令周期（Instruction Cycle）</code>。</p><p>指令存在<code>存储器</code>，由<code>控制器</code>操作，通过 <code>PC 寄存器</code>和<code>指令寄存器</code>取出指令，<code>控制器</code>也控制<code>解码</code>过程。<br>指令执行则是由<code>算术逻辑单元（ALU）</code>操作的，简单的无条件地址跳转则在控制器内完成。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac17p01.jpg" alt=""></p><p><strong>三个 Cycle：</strong></p><ul><li><code>Instruction Cycle</code> ：指令周期</li><li><code>Machine Cycle</code> ：机器周期/ CPU 周期</li><li><code>Clock Cycle</code> ：也就是时钟周期，机器的主频</li></ul><blockquote><p>一个指令周期由多个机器周期组成，一个机器周期由多个时钟周期（至少2个）组成。</p></blockquote><h3 id="建立数据通路"><a href="#建立数据通路" class="headerlink" title="建立数据通路"></a>建立数据通路</h3><p><strong>数据通路就是我们的处理器单元</strong><br>一类是<code>操作元件</code>：也叫<code>组合逻辑元件</code>（Combinational Element），其实就是我们的 <code>ALU</code>；<br>一类是<code>存储元件</code>：也有叫<code>状态元件</code>（State Element），例如寄存器。</p><p>通过数据总线链接起来进行使用，即建立数据通路。</p><p>控制器，<strong>为了循环完成指令的读取和解码，将结果信号输送给ALU</strong>。例如 CPU 有 2k+ 个指令，即有同样多的控制信号组合。</p><blockquote><p>正是控制器，使得可以“编程”来实现功能，构建“存储程序型计算机”。</p></blockquote><h3 id="CPU-所需要的硬件电路"><a href="#CPU-所需要的硬件电路" class="headerlink" title="CPU 所需要的硬件电路"></a>CPU 所需要的硬件电路</h3><ul><li>ALU：根据输入计算结果输出；</li><li>寄存器：进行状态读写；</li><li>“自动”的电路：按照固定的周期，不停地实现 PC 寄存器自增；</li><li>“译码”的电路：对指令进行 decode。</li></ul><p><code>cpu 满载</code>：如果操作系统调度了一个高优先级的任务，那么cpu就优先执行这个任务即满载，<br><code>Idle 闲置</code>：如果操作系统调度了一个低优先级的idle任务，那么cpu就执行这个优先级最低的简单任务，显示为空闲状态，即假装“没事做”，有其他高优任务时可随时抢占。</p><blockquote><p>idle 进程，优先级最低，仅当其他进程都阻塞时被调度器选中。idle 进程循环执行 HLT 指令，关闭 CPU 大部分功能以降低功耗，收到中断信号时 CPU 恢复正常状态。</p></blockquote><hr><h2 id="18-建立数据通路（中）"><a href="#18-建立数据通路（中）" class="headerlink" title="18 | 建立数据通路（中）"></a>18 | 建立数据通路（中）</h2><p><strong>组合逻辑电路（Combinational Logic Circuit）</strong>：给定输入，就能得到固定的输出。</p><p>光有上述的不足够，更像机械计算机，电子计算机则需要<strong>时序逻辑电路（Sequential Logic Circuit）</strong>，可解决3个问题：</p><ul><li>自动运行</li><li>存储</li><li>时序协调</li></ul><h3 id="时钟信号的硬件"><a href="#时钟信号的硬件" class="headerlink" title="时钟信号的硬件"></a>时钟信号的硬件</h3><blockquote><p>CPU 的主频是由一个<code>晶体振荡器</code>来实现的，而它生成的电路信号，就是<code>时钟信号</code>。</p></blockquote><p><strong>核心：电的磁效应。</strong><br>如下图：<br>开关A：手动控制；<br>开关B：自然状态是合上。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac18p01.jpg" alt=""></p><p>那么，开关A合上后，由于线圈通电B就会被断开，而后线圈没有磁性B又会弹回，于是又下面的<code>时钟信号</code>，叫做<code>反馈电路</code>：<br><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac18p02.jpg" alt=""></p><p>可简化成如下方式，一个输出结果接回输入的<code>反相器</code>（Inverter），也即<code>非门</code>。<br><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac18p03.jpg" alt=""></p><h3 id="通过-D-触发器实现存储功能"><a href="#通过-D-触发器实现存储功能" class="headerlink" title="通过 D 触发器实现存储功能"></a>通过 D 触发器实现存储功能</h3><p>基于反馈电路构建有记忆的电路，以实现寄存器和存储器。先看如下电路（2个或非门）：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac18p04.jpg" alt=""></p><div class="table-container"><table><thead><tr><th>NOR</th><th>0</th><th>1</th></tr></thead><tbody><tr><td>0</td><td>1</td><td>0</td></tr><tr><td>1</td><td>0</td><td>0</td></tr></tbody></table></div><p>如上图和表：</p><ol><li>开始R、S断开，A输入0、0，输出则1；B输入0、1，输出则0；电路稳定，Q输出0；</li><li>如R闭合，A输入1、0，输出则0；B输如0，0，输出则1；A输入变成1、1，输出则0；电路稳定，Q输出1；</li><li>如R重断开，A输入1、0，输出则0；B不变；电路稳定；但R、S和步骤1一样，Q却输出1；</li><li>如S闭合，B必然输出0；则Q也输出0。</li></ol><p>上述为<code>触发器</code>（Flip-Flop）：当两个开关都断开的时候，最终的输出结果，取决于之前动作的输出结果，即<code>记忆功能</code>。</p><p>为了实现利用上述写入数据，加入了两个与门和一个时钟信号，看下如电路：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac18p05.jpg" alt=""></p><ul><li>CLK为低电平的时候，R、S无论开关，其紧邻的与门必然输出0；</li><li>CLK为高电平的时候，R、S状态会控制Q输出。</li></ul><p>再将 R 和 S 用一个反相器连起来，就成为最常用的 <code>D 型触发器</code>，一个输入的数据信号 D，也就是 Data：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac18p06.jpg" alt=""></p><p>图中 Q 表示主输出，!Q 表示 Q 的补码。可以发现：</p><ul><li>CLK 低电平的时候，D 输入无影响，Q 始终 0；</li><li>CLK 高电平的时候，D 的输入会决定 Q 的结果，且信号一致。</li></ul><p>故，一个 <code>D 型触发器</code>控制一个bit读写，N 个并列可做成 N 位触发器。</p><p><strong>因此，程序可以“存储”，而不是靠固定的线路连接或者手工拨动开关，如此便解决。</strong></p><h2 id="19-建立数据通路（下）"><a href="#19-建立数据通路（下）" class="headerlink" title="19 | 建立数据通路（下）"></a>19 | 建立数据通路（下）</h2><p>让计算机“自动”跑起来：<strong>时钟信号-&gt;实现计数器-&gt;PC计数器-&gt;译码器-&gt;CPU。</strong></p><h3 id="计数器"><a href="#计数器" class="headerlink" title="计数器"></a>计数器</h3><blockquote><p><code>PC（Program Counter）寄存器</code>，又叫<code>程序计数器</code>。</p></blockquote><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac19p01.jpg" alt=""></p><p>如上图所示：<strong>每过一个时钟周期，就能固定自增 1。</strong></p><ul><li>自增之后，可以取D型触发器里的值作为指令地址；</li><li>顺序地存放指令，就是为了能定时地不断执行新指令。</li></ul><p>一条指令，经历程序计数，到获取指令、执行指令，需要在一个时钟周期里，否则可能会出错。<br>设计确保上限即耗时最长的一条 CPU 指令能完成的，我们称之为<code>单指令周期处理器</code>（Single Cycle Processor）。</p><h3 id="读写数据所需要的译码器"><a href="#读写数据所需要的译码器" class="headerlink" title="读写数据所需要的译码器"></a>读写数据所需要的译码器</h3><p>很多 D 型触发器可以组成一块存储空间作为内存。寻址的电路就是译码器。</p><p>先简化，两个地址选一个，成为 2-1 选择器；如下图：</p><ul><li>输入0，则输出和A一致；</li><li>输入1，则输出和B一致。</li></ul><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac19p02.jpg" alt=""></p><p>如果输入信号有3个不同开关，则能选择$2^3$个，称为<code>3-8译码器</code>。现在CPU上64位的，即有$2^{64}$个开关的译码器。</p><h3 id="构造一个最简单的-CPU"><a href="#构造一个最简单的-CPU" class="headerlink" title="构造一个最简单的 CPU"></a>构造一个最简单的 CPU</h3><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac19p03.jpg" alt=""></p><p>CPU通路：</p><ol><li>自动计数器，作为 PC 寄存器；</li><li>连一个地址译码器+内存（大量 D 型触发器）；</li><li>计数器随着时钟主频自增，译码器获取指令的内存地址，写入指令寄存器；</li><li>指令译码器将指令解析成 opcode 和操作数；</li><li>链接 ALU 获取计算结果，写回到寄存器或者内存。</li></ol><p>if…else…电路：</p><blockquote><p>实际上不是控制电路，被拆解成一条 cmp 和一条 jmp 指令；<br>条件跳转指令也是在 ALU 层面执行的，“译码 - 执行 - 更新寄存器“，不需要控制器。</p><p>执行一条计算机指令，其实可以拆分到很多个时钟周期，而不是必须使用单指令周期处理器的设计。</p></blockquote><hr><h2 id="20-面向流水线的指令设计（上）"><a href="#20-面向流水线的指令设计（上）" class="headerlink" title="20 | 面向流水线的指令设计（上）"></a>20 | 面向流水线的指令设计（上）</h2><h3 id="单指令周期处理器"><a href="#单指令周期处理器" class="headerlink" title="单指令周期处理器"></a>单指令周期处理器</h3><blockquote><p>CPU 指令：“取得指令（Fetch）- 指令译码（Decode）- 执行指令（Execute）</p></blockquote><p>如果 CPI = 1，即 1 个时钟周期执行一个指令，叫<code>单指令周期处理器</code>（Single Cycle Processor）。<br>但，由于指令电路复杂程度不一，实际上时间就不一样，<strong>因此就只能取最长的指令运行时间作为时钟周期，造成资源浪费。</strong></p><h3 id="现代处理器的流水线设计"><a href="#现代处理器的流水线设计" class="headerlink" title="现代处理器的流水线设计"></a>现代处理器的流水线设计</h3><p>现代的 CPU 基本都是<code>指令流水线</code>（Instruction Pipeline）：其实就是将不同步骤拆开来执行，同时指令间也不用等待串行。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac20p01.jpg" alt=""></p><p>指令流水线中的每个步骤称为<code>流水线阶段</code>或者<code>流水线级</code>（Pipeline Stage）。例如“取指令 - 指令译码 - ALU 计算（指令执行）- 内存访问 - 数据写回”就是5级。</p><p>虽然流水线会使得指令的时钟周期增加，但复杂指令被拆分成多个小流水线级，就可以提高 CPU 主频了。<strong>只要保障一个最复杂的流水线级的操作，在一个时钟周期内完成就好了。</strong></p><blockquote><p>现代的 ARM 或者 Intel 的 CPU，流水线级数都已经到了 14 级。</p></blockquote><p>该技术下，单指令时间没变，但提升 CPU 的“吞吐率”，同时执行 5 条不同指令的不同阶段。</p><h3 id="性能瓶颈"><a href="#性能瓶颈" class="headerlink" title="性能瓶颈"></a>性能瓶颈</h3><p>流水线级数并不是越高越好。<strong>每增加一级的流水线，就要多一级写入到流水线寄存器的操作</strong>。所以单纯地增加流水线级数，不仅不能提升性能，反而会有更多的 overhead 的开销。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac20p02.jpg" alt=""></p><p><strong>一个 CPU 的时钟周期：可以认为是完成一条最复杂的指令拆分后流水线级的操作时间。</strong></p><hr><h2 id="21-面向流水线的指令设计（下）"><a href="#21-面向流水线的指令设计（下）" class="headerlink" title="21 | 面向流水线的指令设计（下）"></a>21 | 面向流水线的指令设计（下）</h2><h3 id="芯片的主频战争"><a href="#芯片的主频战争" class="headerlink" title="芯片的主频战争"></a>芯片的主频战争</h3><p>Intel 在 2001 年推出 Pentium 4，特点就是高主频，1GHZ，设计的最高是 10GHz。但，在这过程中，使用超长的流水线。</p><blockquote><p>在 Pentium 4 之前的 Pentium III ，流水线的深度是 11 级，当今一般也就 14；<br>而 Pentium 4 是 20 级。</p></blockquote><p>流水线记住不能缩短单指令效应时间，但可以增加吞吐率。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac21p01.jpg" alt=""></p><h3 id="冒险和分支预测"><a href="#冒险和分支预测" class="headerlink" title="冒险和分支预测"></a>冒险和分支预测</h3><p>Pentium 4 为什么失败？</p><ol><li>功耗问题。流水线深度增加，需要更高主频；</li><li>且电路增多，晶体管增加。于是功耗增加；</li><li>性能提升不一定。例如互相有依赖的code，不能分级后并行。</li></ol><p><strong>冒险（Hazard）问题：数据冒险、结构冒险、控制冒险。</strong></p><p>流水线越长，这个冒险的问题就越难以解决。因为级数越高，越难使用乱序执行、分支预测等方案。</p><p>一般用 <code>IPC</code>（Instruction Per Cycle）来衡量 CPU 执行指令的效率。 即 CPI（Cycle Per Instruction）的倒数。</p><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;全文内容主要来自对课程&lt;a href=&quot;https://time.geekbang.org/column/article/91427&quot;&gt;《深入浅出计算机组成原理》&lt;/a&gt;的学习笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;17-建立数</summary>
      
    
    
    
    <category term="学习笔记" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="计算机组成原理" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/"/>
    
    
    <category term="计算机原理" scheme="https://www.xiemingzhao.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%8E%9F%E7%90%86/"/>
    
    <category term="处理器" scheme="https://www.xiemingzhao.com/tags/%E5%A4%84%E7%90%86%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>深入浅出计算机组成原理——原理篇：指令和运算（11-16）</title>
    <link href="https://www.xiemingzhao.com/posts/computerOrgArc11to16.html"/>
    <id>https://www.xiemingzhao.com/posts/computerOrgArc11to16.html</id>
    <published>2021-07-12T16:00:00.000Z</published>
    <updated>2025-04-08T16:25:38.357Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>全文内容主要来自对课程<a href="https://time.geekbang.org/column/article/91427">《深入浅出计算机组成原理》</a>的学习笔记。</p></blockquote><h2 id="11-二进制编码"><a href="#11-二进制编码" class="headerlink" title="11 | 二进制编码"></a>11 | 二进制编码</h2><h3 id="逢二进一"><a href="#逢二进一" class="headerlink" title="逢二进一"></a>逢二进一</h3><p><code>源码</code>表示法（最左侧位的 0/1 代表整体正负）：<br>$0011 = + 0 \times 2^2 + 1 \times 2^1 + 1 \times 2^0 = 3$<br>$1011 = - (0 \times 2^2 + 1 \times 2^1 + 1 \times 2^0) = -3$</p><blockquote><p><code>ASCII 码</code>（American Standard Code for Information Interchange，美国信息交换标准代码）但此时0可以表示成 0000 和 1000，一对多，不合适。</p></blockquote><p><code>补码</code>表示法（最左侧位仅表其位负）：<br>$1011 = -1 \times 2^3 + 0 \times 2^2 + 1 \times 2^1 + 1 \times 2^0) = -5$</p><blockquote><p>如此4位的话，可以表示-8到7这16个整数。</p></blockquote><h3 id="字符串的表示"><a href="#字符串的表示" class="headerlink" title="字符串的表示"></a>字符串的表示</h3><p><code>ASCII</code> 码（American Standard Code for Information Interchange，美国信息交换标准代码）。<br><strong>它存储了128个字符和8位二进制数的映射关系，一一对应。</strong></p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac11p01.png" alt=""></p><p>表示逻辑case：</p><blockquote><p>a：ASCII 里第 97 个，二进制表示为 01100001，而上图映射表中为十六进制表示，每4位一组即可，即0110|0001=61(16进制)=6x16+1=97(10进制)；<br>9：ASCII 里第 57 个（字符9不是整数9），二进制表示为 00111001，而上图映射表中为十六进制表示，每4位一组即可，即0011|1001=39。</p></blockquote><p>一个弊端：<br>如果是int32最大数2147483647，二进制只需要32位，但上述的拆分字符串表示法就需要8x10=80位。</p><p><strong>不管是整数也好，浮点数也好，采用二进制序列化会比存储文本省下不少空间。</strong></p><p>当使用国家多了后，上述的128个字符表示就不够了，就需要其他的字符映射关系。日常说的 Unicode，其实就是一个字符集，包含了 150 种语言的 14 万个不同的字符。其可以用UTF-8 来编码成二进制，当然也可以用GT-32 编码等，只是不同的编码对应关系。</p><hr><h2 id="12-理解电路"><a href="#12-理解电路" class="headerlink" title="12 | 理解电路"></a>12 | 理解电路</h2><h3 id="电报原理"><a href="#电报原理" class="headerlink" title="电报原理"></a>电报原理</h3><p>古时候军队的金和鼓、烽火、灯塔等都是信号传递，通过不同的组合表达不同的意思，但都是二进制的类似。</p><p>电报传输的信号有两种：</p><ul><li>短促点信号（dot 信号），1</li><li>长的划信号（dash 信号），0<br>那么“SOS”就可以表示为111000111。</li></ul><p><strong>电报机本质是“蜂鸣器 + 电线 + 开关”</strong></p><h3 id="理解继电器"><a href="#理解继电器" class="headerlink" title="理解继电器"></a>理解继电器</h3><p>上述电报机的问题：如果电线太长，使得电阻大，蜂鸣器电压不足会不响。<br>解决办法：在路线中，插入一个机器，将接收的信号原模原样的发送出去，这就是继电器。</p><p>继电器一般就用“螺旋线圈 + 磁性开关”构建。通过线圈和开关的组合，也可以创造出“与（AND）”“或（OR）”“非（NOT）”等逻辑电路。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac12p01.jpg" alt=""></p><h2 id="13-加法器"><a href="#13-加法器" class="headerlink" title="13 | 加法器"></a>13 | 加法器</h2><p>门电路图，就是计算机硬件的积木，组合成cpu的核心模块。</p><ul><li>与门：1、1 输出 1，其他 0；</li><li>或门：有任一个 1 输出 1，其他 0；</li><li>非门：1 输出 0，0 输出 1；</li><li>或非门：或门+非门，0、0 输出 1，其他均 0；</li><li>异或门：相同（同 0 或同 1）则输出 1，否则输出 0；</li><li>与非门：与门+非门，1、1 输出 0，其他均 1。</li></ul><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac13p01.jpg" alt=""></p><h3 id="异或门和半加器"><a href="#异或门和半加器" class="headerlink" title="异或门和半加器"></a>异或门和半加器</h3><p>两个二进制无符号整数相加，<strong>对于个位</strong>需要判断的就是<strong>进位与个位</strong>。实际上对应2种门电路：</p><ul><li>个位：异或（XOR）</li><li>进位：与门</li></ul><p>将上述2种电路打包就可以得到一个<code>半加器（Half Adder）</code>。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac13p02.jpg" alt=""></p><h3 id="全加器"><a href="#全加器" class="headerlink" title="全加器"></a>全加器</h3><p>上述强调了个位的加法可以通过半加器（Half Adder）实现，但再往后面的位半加器就不够用了，原因很简单，<em>还需要考虑上一位的进位</em>。</p><p><strong>解法：全加器（Full Adder），用两个半加器和一个或门。</strong></p><ul><li>输入：进位信号、加数和被加数</li><li>输出：进位信号、和</li></ul><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac13p03.jpg" alt=""></p><p>如此，两个 8 bit 数的加法可以通过8个全加器串联：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac13p04.jpg" alt=""></p><blockquote><p>溢出：整数是没有位置标记溢出的，比如 int32。实际上，计算结果是否溢出是通过加法器的结果中，将溢出输出给到硬件中其他标志位里实现的，也是硬件层面的支持。</p></blockquote><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>我们通过门电路、半加器、全加器可以搭建出类似加法器这样的组建，一般把这些用来做算术逻辑计算的组件叫作 <code>ALU</code>，算术逻辑单元。</p><hr><h2 id="14-乘法器"><a href="#14-乘法器" class="headerlink" title="14 | 乘法器"></a>14 | 乘法器</h2><p>从 $13 \times 9$ 开始，二进制的方式和10进制一样：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac14p01.jpg" alt=""></p><h3 id="顺序乘法"><a href="#顺序乘法" class="headerlink" title="顺序乘法"></a>顺序乘法</h3><p>对于上述乘法过程，只需要简单的加法器、一个可以左移一位的电路和一个右移一位的电路，就能完成整个乘法。过程如下所示：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac14p02.jpg" alt=""></p><blockquote><p>弊端：不同位置间串行，复杂度高。</p></blockquote><h3 id="并行加速"><a href="#并行加速" class="headerlink" title="并行加速"></a>并行加速</h3><p>朴素的思想就是通过并行把 O(N) 的时间复杂度，降低到 O(logN)。</p><p>如下图所示，通过并联更多的 ALU，加上更多的寄存器，让不同位置的乘法并行计算，然后进行结果的合并。<br><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac14p03.jpg" alt=""></p><h3 id="电路并行"><a href="#电路并行" class="headerlink" title="电路并行"></a>电路并行</h3><p>上述的算法并行实际上还是比较慢，前后需要有结果的依赖等待。</p><p>例如每一个全加器，都要等待上一个全加器，把对应的进入输入结果算出来，才能算下一位的输出。这个每一步等待的时间叫<code>门延迟（Gate Delay）</code>，一般作“T”。</p><blockquote><p>一个全加器，其实就已经有了 3T 的延迟（进位需要经过 3 个门电路）。</p></blockquote><p><strong>加速思路：空间换时间</strong></p><blockquote><p>把原来需要较少的，但是有较多层前后计算依赖关系的门电路，展开成需要较多的，但是依赖关系更少的门电路。</p></blockquote><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac14p04.jpg" alt=""></p><p>如上图，例如一个 4 位整数最高位是否进位，展开门电路图只需要 3T 的延迟就可以拿到进位结果。64 位的整数，多复制上述电路即可。</p><p><strong>计算机通过更多的晶体管，就可以拿到更低的门延迟，以及用更少的时钟周期完成一个计算指令。</strong></p><hr><h2 id="15-浮点数和定点数（上）"><a href="#15-浮点数和定点数（上）" class="headerlink" title="15 | 浮点数和定点数（上）"></a>15 | 浮点数和定点数（上）</h2><blockquote><p>如何用二进制表示所有的实数?</p></blockquote><h3 id="浮点数的不精确性"><a href="#浮点数的不精确性" class="headerlink" title="浮点数的不精确性"></a>浮点数的不精确性</h3><p>看下面代码结果，可以发现计算机简单的计算精度也有丢失。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">0.3</span> + <span class="number">0.6</span> </span><br><span class="line"><span class="number">0.8999999999999999</span></span><br></pre></td></tr></table></figure><p>计算机一般用 16 或 32 位来表示数，32 个比特，只能表示 2 的 32 次方个不同的数，差不多是 40 亿个。而这里必然是使用了其他表示法导致精度丢失。</p><h3 id="定点数"><a href="#定点数" class="headerlink" title="定点数"></a>定点数</h3><p>一个朴素的想法：32位拆成8段4位，每4个bit可以表示0-9整数。那么把前6段作为整数部分，后2段作为小数部分，就可以表示0-999999.99这1亿个数。</p><p>上述叫<code>BCD编码</code>，一般在超市、银行使用。</p><p>缺点：</p><ol><li>浪费；<blockquote><p>32位从40亿的表示能力降为了1亿，且范围不够用。</p></blockquote></li><li>没办法表示更大更小数；<blockquote><p>物理和数学上的高精度需要就不行。</p></blockquote></li></ol><h3 id="浮点数"><a href="#浮点数" class="headerlink" title="浮点数"></a>浮点数</h3><p>解决方案：<code>浮点数</code>（Floating Point），也就是 float 类型。</p><p><strong>思想：科学计数法。</strong></p><p><code>IEEE</code>标准，有2个基本格式：</p><ul><li>32 bit，即 float / float32；</li><li>64 bit，即 double / float64。</li></ul><p>以 float32 为例：</p><ul><li>s：符号位，1 bit，表正负；</li><li>e：指数位，8 bit，用 1～254 映射 -126～127；</li><li>f：有效数位，23 bit。</li></ul><p>表示形式：</p><script type="math/tex; mode=display">(-1)^s \times 1.f \times 2^e</script><p>此外，还有一些<code>特殊值</code>表示：</p><div class="table-container"><table><thead><tr><th>e</th><th>f</th><th>s</th><th>浮点数</th></tr></thead><tbody><tr><td>0</td><td>0</td><td>0 or 1</td><td>0</td></tr><tr><td>0</td><td>!=0</td><td>0 or 1</td><td>0.f</td></tr><tr><td>255</td><td>0</td><td>0</td><td>无穷大</td></tr><tr><td>255</td><td>0</td><td>1</td><td>无穷小</td></tr><tr><td>255</td><td>!=0</td><td>0 or 1</td><td>NAN</td></tr></tbody></table></div><p>case：0.5</p><blockquote><p>s = 0,f = 0, e = -1, $(-1)^0 \times 1.0 \times 2^{-1}=0.5$</p></blockquote><p>因此，float32 能表示的绝对值范围是 $1.17 \times 10^{-38} to 3.40 \times 10^{38}$。</p><blockquote><p>上述范围大约由 1.9999999 ^(2^127) 和 1.0000000 ^ (2^-126) 转成科学记数法得到。</p></blockquote><hr><h2 id="16-浮点数和定点数（下）"><a href="#16-浮点数和定点数（下）" class="headerlink" title="16 | 浮点数和定点数（下）"></a>16 | 浮点数和定点数（下）</h2><h3 id="浮点数的二进制转化"><a href="#浮点数的二进制转化" class="headerlink" title="浮点数的二进制转化"></a>浮点数的二进制转化</h3><p>以 9.1 换算成float为例，整数部分9，换算成 1001。</p><p>对于小数部分，先看<strong>二进制小数转10进制</strong>。</p><blockquote><p>把二进制小数点后的每一位，都对应的 2 的 -N 次方。如 0.1001 可转化成</p><script type="math/tex; mode=display">1 \times 2^{-1} + 0 \times 2^{-2} + 0 \times 2^{-3} + 1 \times 2^{-4}=0.5625</script></blockquote><p>反过来，将<strong>十进制的小数转为二进制</strong>：</p><blockquote><p>则和整数的二进制表示采用“除以 2，然后看余数”的方式类似；<br>即乘以 2，记录是否超过 1，剩余差值则是乘积结果（<1）或者 乘积结果-1（>1）.</p></blockquote><p>如下方示例：<br>9.1 拆成 9 和 0.1，前者是 1001，后者通过上述算法（如下表过程） .000110011…(23位)。<br>最终有：$1001.000110011 \times 2^0$   或小数点左移3位后   $1.001000110011 \times 2^3$。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac16p01.jpg" alt=""></p><p>二进制的存储如下：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac16p02.jpg" alt=""></p><p><strong>注意：指数位 e 是用 1～254 映射 -126～127，所以 3 需要用 3 + 127 = 130 来映射的</strong></p><p>因此，将其再转为十进制，就是9.09999942779541015625，精度有一定损失。</p><h3 id="加法和精度损失"><a href="#加法和精度损失" class="headerlink" title="加法和精度损失"></a>加法和精度损失</h3><p><strong>先对齐、再计算。</strong></p><p>以 0.5 + 0.125 为例，过程如下表所示：</p><div class="table-container"><table><thead><tr><th>步骤</th><th>符号位s</th><th>指数位e</th><th>有效位1.f</th></tr></thead><tbody><tr><td>0.5</td><td>0</td><td>-1</td><td>1.00…</td></tr><tr><td>0.125</td><td>0</td><td>-3</td><td>1.00…</td></tr><tr><td>0.125对齐指数位</td><td>0</td><td>-1</td><td>0.01</td></tr><tr><td>0.5+0.125</td><td>0</td><td>-1</td><td>1.01</td></tr></tbody></table></div><p><strong>问题：<br>在有效位对齐的时候，指数位较小的需要进行右移，会使得最右侧的有效位丢失进而造成<code>精度丢失</code>，除非最右侧丢失的都是0。</strong></p><blockquote><p>case:<br>Java 程序，让一个值为 2000 万的 32 位浮点数和 1 相加，你会发现，+1 这个过程因为精度损失，被“完全抛弃”了。</p></blockquote><h3 id="Kahan-Summation-算法"><a href="#Kahan-Summation-算法" class="headerlink" title="Kahan Summation 算法"></a>Kahan Summation 算法</h3><p>用一个循环相加 2000 万个 1.0f，最终的结果会是 1600 万左右，而不是 2000 万。因为1600w之后精度就丢失了。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FloatPrecision</span> &#123;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">    <span class="type">float</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0.0f</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">20000000</span>; i++) &#123;</span><br><span class="line">      <span class="type">float</span> <span class="variable">x</span> <span class="operator">=</span> <span class="number">1.0f</span>;</span><br><span class="line">      sum += x;      </span><br><span class="line">    &#125;</span><br><span class="line">    System.out.println(<span class="string">&quot;sum is &quot;</span> + sum);   </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>Kahan Summation 算法</code>可以解决，其原理就是：</p><blockquote><p>在每次的计算过程中，都用一次减法，把当前加法计算中损失的精度记录下来，然后在后面的循环中，把这个精度损失放在要加的小数上，再做一次运算。</p></blockquote><h2 id=""><a href="#" class="headerlink" title=""></a><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">KahanSummation</span> &#123;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">    <span class="type">float</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0.0f</span>;</span><br><span class="line">    <span class="type">float</span> <span class="variable">c</span> <span class="operator">=</span> <span class="number">0.0f</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">20000000</span>; i++) &#123;</span><br><span class="line">      <span class="type">float</span> <span class="variable">x</span> <span class="operator">=</span> <span class="number">1.0f</span>;</span><br><span class="line">      <span class="type">float</span> <span class="variable">y</span> <span class="operator">=</span> x - c;</span><br><span class="line">      <span class="type">float</span> <span class="variable">t</span> <span class="operator">=</span> sum + y;</span><br><span class="line">      c = (t-sum)-y;</span><br><span class="line">      sum = t;      </span><br><span class="line">    &#125;</span><br><span class="line">    System.out.println(<span class="string">&quot;sum is &quot;</span> + sum);   </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></h2>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;全文内容主要来自对课程&lt;a href=&quot;https://time.geekbang.org/column/article/91427&quot;&gt;《深入浅出计算机组成原理》&lt;/a&gt;的学习笔记。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;11-二进制</summary>
      
    
    
    
    <category term="学习笔记" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="计算机组成原理" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/"/>
    
    
    <category term="计算机原理" scheme="https://www.xiemingzhao.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%8E%9F%E7%90%86/"/>
    
    <category term="指令和运算" scheme="https://www.xiemingzhao.com/tags/%E6%8C%87%E4%BB%A4%E5%92%8C%E8%BF%90%E7%AE%97/"/>
    
  </entry>
  
</feed>
