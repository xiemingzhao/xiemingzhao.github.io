<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>小火箭的博客</title>
  
  <subtitle>愿世界和平！！！</subtitle>
  <link href="https://www.xiemingzhao.com/atom.xml" rel="self"/>
  
  <link href="https://www.xiemingzhao.com/"/>
  <updated>2025-04-02T16:57:47.452Z</updated>
  <id>https://www.xiemingzhao.com/</id>
  
  <author>
    <name>小火箭</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>推荐模型中的 position bias 和 debias</title>
    <link href="https://www.xiemingzhao.com/posts/biasnet.html"/>
    <id>https://www.xiemingzhao.com/posts/biasnet.html</id>
    <published>2022-03-26T16:00:00.000Z</published>
    <updated>2025-04-02T16:57:47.452Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>在推荐系统中一个重要的任务就是 CTR 建模，其本质的思想便是<strong>预估 user 对 item 的点击率</strong>。但是实际中获取的样本往往是在一定条件（时间、机型、位置等）下的后验结果，所以使得建模的 Label 往往是夹杂了这些因素的结果。</p><p>这些影响后验结果的因素一般称为 <code>偏置（bias）项</code>，而去除这些偏置项的过程就称为 <code>消偏（debias）</code>。在这其中最重要的便是 <code>位置偏置（position bias）</code>，即 item 展示在不同位置会有不同的影响，且用户往往更偏向点击靠前的位置。本文将重点介绍业界在 <code>position bias</code> 消除上的一般做法和相关经验。</p><h2 id="2-Position-Bias"><a href="#2-Position-Bias" class="headerlink" title="2 Position Bias"></a>2 Position Bias</h2><p>看下面的图，是笔者实际工作场景中部分位置的 CTR 趋势图。可以明显地看到：</p><ul><li>呈现每 20 个 position 位一个周期；每刷请求的个数是 20.</li><li>周期内位置越靠前，CTR 越大；靠前效率高，用户更偏好点靠前的。</li></ul><span id="more"></span><p><img src="evernotecid://5E1922BA-4A0F-4E74-8876-BB522F7481EE/appyinxiangcom/26644553/ENResource/p789" alt="43804e6794a00b371f3b79e78957c6fd.png"></p><p>在华为的研究中也论证了用户对 position 靠前的偏好。固定 item 在不同 position 的 CTR 和不固定 item 的趋势差别较为显著。</p><p><img src="evernotecid://5E1922BA-4A0F-4E74-8876-BB522F7481EE/appyinxiangcom/26644553/ENResource/p790" alt="6cac886f326237437d94655617b76fa0.png"></p><h2 id="3-Position-Debias—特征法"><a href="#3-Position-Debias—特征法" class="headerlink" title="3 Position Debias—特征法"></a>3 Position Debias—特征法</h2><p><img src="evernotecid://5E1922BA-4A0F-4E74-8876-BB522F7481EE/appyinxiangcom/26644553/ENResource/p791" alt="fb42997d2c728c829eb605c6ec52a2b0.png"></p><p>比较朴素的想法，便是在特征体系中引入 position, 如上图所示。</p><ul><li>模型 <code>offline training</code> 的时候，把 position 作为特征输入模型，让模型学习 position 带来的后验影响。</li><li>而在 <code>online infer</code> 的时候，并没有 position 这样后验的信息，往往可以选择填充一个默认值，比如 0。</li></ul><p><strong>注意：具体填什么也需要测试，不同默认值的结果差别还不小。</strong></p><h2 id="4-Position-Debias—Shallow-Tower"><a href="#4-Position-Debias—Shallow-Tower" class="headerlink" title="4 Position Debias—Shallow Tower"></a>4 Position Debias—Shallow Tower</h2><p>此方法核心是：<strong>构建一个 Shallow Tower 来预估 Position Debias。</strong></p><p>方法来源是 Youtube 发表在 RecSys 2019上的文章：<a href="https://daiwk.github.io/assets/youtube-multitask.pdf">Recommending What Video to Watch Next: A Multitask Ranking System</a></p><p><img src="evernotecid://5E1922BA-4A0F-4E74-8876-BB522F7481EE/appyinxiangcom/26644553/ENResource/p792" alt="0849909dcdc6c10617b2132714ebc6c8.png"></p><p>如上图所示，文章阐述在 <code>position debias</code> 上的做法是:</p><ul><li>保持原来的主模型(main model)不变</li><li>新增一个专门拟合 position bias 的浅层网络(shallow tower)</li><li>将 main model 和 shallow tower 的 logit 相加再过 sigmoid 层后构建 loss。</li></ul><p>其中，<code>shallow tower</code> 的输入主要包含 <code>position feature</code>, <code>device info</code> 等会带来 bias 的特征，而加入 device info 的原因是<em>在不同的设备上会观察到不同的位置偏差</em>。</p><blockquote><p>注意：文章提到在 training 的时候，<strong>position 的特征会应用 10% 的 drop-out</strong>，目的是为了防止模型过度依赖 position 特征。在 online infer 的时候，由于没有后验的 position 特征，<strong>直接丢掉 shallow tower 即可</strong>。</p></blockquote><p>在文章中，披露了模型训练结果提取出的 position bias，如下图所示，可以看到随之位置的增长，bias 越大。因为越靠后，用户更有可能看不到。</p><p><img src="evernotecid://5E1922BA-4A0F-4E74-8876-BB522F7481EE/appyinxiangcom/26644553/ENResource/p793" alt="f40bc1bdcc1a117d484846972ca8a386.png"> <img src="evernotecid://5E1922BA-4A0F-4E74-8876-BB522F7481EE/appyinxiangcom/26644553/ENResource/p794" alt="d138747505122df1921b5d28c9b28ff5.png"></p><blockquote><p>实际上，bias 还可以拓展更多的特征，包括 user 和 item 侧的属性，具体如何还需依赖对业务的理解和实验。</p></blockquote><h2 id="5-Position-Debias—PAL"><a href="#5-Position-Debias—PAL" class="headerlink" title="5 Position Debias—PAL"></a>5 Position Debias—PAL</h2><p>此方法核心是：<strong>将 position bias 从后验点击概率中拆出来，看作是用户看到的概率。</strong></p><p>方法来源是华为发表在 RecSys 2019上的文章：<a href="https://dl.acm.org/doi/abs/10.1145/3298689.3347033">PAL: A Position-bias Aware Learning Framework for CTR Prediction in Live Recommender Systems</a></p><script type="math/tex; mode=display">p(y = 1|x, pos) = p(seen|x, pos) p(y = 1|x, pos, seen)</script><p>如上公式，作者将后验点击概率拆成了2个条件概率的乘积：</p><ul><li>Item 被用户看到的概率</li><li>用户看到 item 后，再点击的概率</li></ul><p>那么可以进一步假设：</p><ul><li>用户是否看到 item 只跟位置有关系</li><li>用户看到 item 后，是否点击 item 与位置无关</li></ul><script type="math/tex; mode=display">p(y = 1|x, pos) = p(seen|pos) p(y = 1|x, seen)</script><p>基于上述假设，就可以建模如下：</p><p><img src="evernotecid://5E1922BA-4A0F-4E74-8876-BB522F7481EE/appyinxiangcom/26644553/ENResource/p795" alt="fec9e337c7a2c620f2f90e3f1891f8f5.png"></p><p>如上图所示，其中：</p><ul><li><code>ProbSeen</code>： 是预估广告被用户看到的概率</li><li><code>pCTR</code>：是用户看到 item 后，点击的概率</li></ul><p>可以看到与 YouTube 做法的<strong>区别主要有2点：bias net 和 main model 都先过激活层；然后两边的值再相乘。</strong></p><p>最后 loss 是两者的结合：</p><script type="math/tex; mode=display">L(\theta_{ps},\theta_{pCTR}) = \frac{1}{N}\sum_{i = 1}^Nl(y_i,vCTR_i)) = \frac{1}{N}\sum_{i = 1}^N l(y_i,ProbSeen_i \times pCTR_i))</script><p>在 online infer 的时候，也是类似地丢掉 position 相关的 ProbSeen 的网络，只保留 pCTR 部分即可。</p><h2 id="6-拓展思考"><a href="#6-拓展思考" class="headerlink" title="6 拓展思考"></a>6 拓展思考</h2><h3 id="6-1-假设是否成立？"><a href="#6-1-假设是否成立？" class="headerlink" title="6.1 假设是否成立？"></a>6.1 假设是否成立？</h3><p>两种主流的做法都是将 position 等可能造成 bias 影响的信息单独构建 <code>bias net</code>，然后与 <code>main model</code> 进行融合。<br>但是，</p><blockquote><p><em>Position 带来的 bias 是否可以独立于 main model 进行建模？</em><br><em>用户是否看到是否可以简化为只与 position 相关？</em><br><em>Bias net 的作用是否可以简化为与主塔结果的相加再激活/先激活再乘积？</em></p></blockquote><p>上述问题也许没有标准答案。实际上，笔者在实际中还做了另一种方案，即真的只将结果看成 bias 项，那么就简单的与主网络相加即可，实际上结果也不差。为了控制值域依然在 (0,1) 从而不影响 loss 的构建，最终输出变成：</p><script type="math/tex; mode=display">p(y=1|x,pos) = \frac{1}{2}(p(seen|pos) + p(y = 1|x, seen))</script><h3 id="6-2-pCTR-的分布问题"><a href="#6-2-pCTR-的分布问题" class="headerlink" title="6.2 pCTR 的分布问题"></a>6.2 pCTR 的分布问题</h3><p>容易发现，无论哪种 bias net 的融合方式，最后 loss 所使用的 pCTR 已经发生了变化，而在 online 阶段去除 bias net 部分后，保留的 main tower 对应的输出 pCTR 的分布必然会发生变化。最明显的表现就是 <strong>pcoc（sum(clk)/sum(pCTR）将会偏离 1 附近</strong>。</p><p>而这带来的影响就是：</p><blockquote><p>如果后排和重排中使用到 pCTR 的时候，就会出现含义偏离，会带来一些连锁效应，并且不利于数据分析。当然，有的系统可能没有这个要求。</p></blockquote><p>对于这个问题，笔者试过一些缓解方案：</p><ul><li>增加辅助 loss：比如主网络的 pCTR 也增加一个 logloss 来修正齐 pcoc</li><li>增加 pcoc 正则：针对主网络的 pCTR 新增一个 pcoc 偏离 1 的惩罚项，类似于正则的思想</li><li>矫正结果分值：统计主网络输出偏离期望的比例，直接将输出结果根据该值进行矫正即可</li></ul><p>从效果上来说：</p><ul><li>辅助 loss 和正则的方式确实有助于改善 pcoc，但往往也会影响效果，毕竟梯度被分散了；</li><li>矫正的方式最明显，但是会面临校正系数变化的问题。</li></ul><h3 id="6-3-业务效果"><a href="#6-3-业务效果" class="headerlink" title="6.3 业务效果"></a>6.3 业务效果</h3><p>在推荐系统中，一切脱离实验业务效果的优化，往往都不够坚挺。笔者主要在电商推荐领域内，那么这里给出的经验也仅仅针对此类做参考，不一定具有普适性。</p><p>笔者主要实验了 PAL 和 $\frac{1}{2}(p(seen|pos) + p(y = 1|x, seen))$ 的方式，并且都做了预估分值矫正。离线上 auc 往往会有微幅提升。线上的 CTR 和 IPV 一般会有一定涨幅，在笔者的实验中 +1% 左右。<br>但是，一些体验指标变差了，比如负反馈率、猎奇品的占比等。综合分析下来，click 主要涨在猎奇、标题党等低质品的流量增加上，是不利于系统健康的，于是最终实验没有推全。当然，如果是 UGC 或者 Ads 类业务可能会是另一个逻辑，所以仅供参考。</p><p><strong>参考文章</strong><br><a href="https://zhuanlan.zhihu.com/p/342905546">推荐生态中的bias和debias</a><br><a href="https://tech.meituan.com/2021/06/10/deep-position-wise-interaction-network-for-ctr-prediction.html">SIGIR 2021 | 广告系统位置偏差的CTR模型优化方案</a><br><a href="https://zhuanlan.zhihu.com/p/420373594">推荐系统中的bias&amp;&amp;debias(二)：position bias的消偏</a><br><a href="https://daiwk.github.io/assets/youtube-multitask.pdf">Recommending What Video to Watch Next: A Multitask Ranking System</a><br><a href="https://dl.acm.org/doi/abs/10.1145/3298689.3347033">PAL: a position-bias aware learning framework for CTR prediction in live recommender systems</a><br><a href="https://arxiv.org/pdf/2010.03240.pdf">Bias and Debias in Recommender System: A Survey and Future Directions</a></p><hr>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-引言&quot;&gt;&lt;a href=&quot;#1-引言&quot; class=&quot;headerlink&quot; title=&quot;1 引言&quot;&gt;&lt;/a&gt;1 引言&lt;/h2&gt;&lt;p&gt;在推荐系统中一个重要的任务就是 CTR 建模，其本质的思想便是&lt;strong&gt;预估 user 对 item 的点击率&lt;/strong&gt;。但是实际中获取的样本往往是在一定条件（时间、机型、位置等）下的后验结果，所以使得建模的 Label 往往是夹杂了这些因素的结果。&lt;/p&gt;
&lt;p&gt;这些影响后验结果的因素一般称为 &lt;code&gt;偏置（bias）项&lt;/code&gt;，而去除这些偏置项的过程就称为 &lt;code&gt;消偏（debias）&lt;/code&gt;。在这其中最重要的便是 &lt;code&gt;位置偏置（position bias）&lt;/code&gt;，即 item 展示在不同位置会有不同的影响，且用户往往更偏向点击靠前的位置。本文将重点介绍业界在 &lt;code&gt;position bias&lt;/code&gt; 消除上的一般做法和相关经验。&lt;/p&gt;
&lt;h2 id=&quot;2-Position-Bias&quot;&gt;&lt;a href=&quot;#2-Position-Bias&quot; class=&quot;headerlink&quot; title=&quot;2 Position Bias&quot;&gt;&lt;/a&gt;2 Position Bias&lt;/h2&gt;&lt;p&gt;看下面的图，是笔者实际工作场景中部分位置的 CTR 趋势图。可以明显地看到：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;呈现每 20 个 position 位一个周期；每刷请求的个数是 20.&lt;/li&gt;
&lt;li&gt;周期内位置越靠前，CTR 越大；靠前效率高，用户更偏好点靠前的。&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="精排模型" scheme="https://www.xiemingzhao.com/categories/%E7%B2%BE%E6%8E%92%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="算法总结" scheme="https://www.xiemingzhao.com/categories/%E7%B2%BE%E6%8E%92%E6%A8%A1%E5%9E%8B/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="精排" scheme="https://www.xiemingzhao.com/tags/%E7%B2%BE%E6%8E%92/"/>
    
    <category term="debias" scheme="https://www.xiemingzhao.com/tags/debias/"/>
    
  </entry>
  
  <entry>
    <title>YouTubeDNN 和 WCE</title>
    <link href="https://www.xiemingzhao.com/posts/youtubednn.html"/>
    <id>https://www.xiemingzhao.com/posts/youtubednn.html</id>
    <published>2021-12-17T16:00:00.000Z</published>
    <updated>2025-04-02T16:56:52.087Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1 背景"></a>1 背景</h2><p>这是一篇推荐算法领域经典的论文，它由 YouTube 在2016年发表在 RecSys 上的文章<a href="https://dl.acm.org/doi/pdf/10.1145/2959100.2959190">Deep Neural Networks for YouTube Recommendations</a>。<br>这篇文章是诸多推荐算法工程师的必学经典，可能很多人多次重读都会有新的思考，本文也重点总结文章的核心内容与一些实战经验的思考。</p><h2 id="2-原理"><a href="#2-原理" class="headerlink" title="2 原理"></a>2 原理</h2><p>首先便是其展示的系统链路示意图，这块与大多主流方案没有什么区别。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/youtubednn0.png" alt="youtubednn0"></p><p>论文分别介绍了在 <code>recall</code> 和 <code>ranking</code> 两个模块的方案，但可以说，recall 部分的重要性远大于 ranking。就此文章发表后的几年而言，<em>recall 往往还在工业界主流召回的候选方案中，但 ranking 的方案基本已经成为历史，很少再使用了</em>，不过其思想还是值得学习的。</p><span id="more"></span><h3 id="2-1-recall"><a href="#2-1-recall" class="headerlink" title="2.1 recall"></a>2.1 recall</h3><blockquote><p>任务目标：预测用户下一个观看的视频（next watch），一个多分类问题。</p></blockquote><script type="math/tex; mode=display">P(w_t=i|U,C)=\frac{e^{v_i,u}}{\sum_{j \in V} e^{v_j,u}}</script><p>这里先上模型结构，如下图所示。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/youtubednn1.png" alt="youtubednn1"></p><p><strong>特征</strong></p><ul><li>用户历史看的视频序列，取 embedding 做 <code>average pooling</code>；</li><li>用户历史搜索的 token 序列，也做 <code>average pooling</code>；</li><li>用户的地理位置、性别、年龄等；</li><li>样本年龄（后续单独介绍）。</li></ul><p>之后便是把所有 embedding 进行 concat 拼接，过3层 DNN 以得到 user vector 即 user embedding。</p><p><strong>注意：这里只有 user 的特征输入。</strong></p><blockquote><p>这是召回模型的通用方法，类似于双塔模型。主要是先构建 user embedding 的网络，利于后续线上服务。而与 item 的交互，往往放在最后一个环节。</p></blockquote><p>可以看到，在 user vector 生成后，被分成了 <code>training</code> 和 <code>serving</code> 两个分支。</p><p>先看 <code>training</code> 部分，看上去2步：</p><ol><li>先经过 softmax 层预估 video 的多分类概率；</li><li>然后产出 video vector 供 serving 使用。</li></ol><p>我们假设 3 层的 DNN 后得到的 user vector 是 K 维的，而需要进行多分类的候选集有 M 个 video，那么 training 侧的结构便是：</p><script type="math/tex; mode=display">output = softmax(user_{vec} \cdot W)</script><p>如果 $user_{vec}$ 是 $1 \times K$ 的，那么 $W$ 便是 $K \times M$ 的，如此输出就是 M 维的 softmax 结果。<strong>那么 W 的 M 列 K 维向量即可作为候选集 M 个 video 的 vector。</strong></p><p>其实不用陌生：让我们再次联想召回的双塔模型，是不是就相当于将候选 M 个 video 先过 embedding 层，之后与user vector 做点积，这也是召回模型的经典做法。</p><p>再看 <code>serving</code> 环节，也是经典的召回方案。即：</p><ol><li>离线模型中训练好的 video vector 保存下来；</li><li>将 video vector 构建到 ANN 等向量索引库中；</li><li>线上 serving 的时候，user vector 通过模型实时 infer 得到；</li><li>用 user vector 和索引库进行近邻召回。</li></ol><p><strong>如此的优势</strong>：</p><ul><li>因为每次 serving 需要处理的 video 很多，其 vector 不适合实时生成；</li><li>每次 serving 时 user vector 只需要 infer 一条样本，性能可控，捕捉 user 实时偏好就更重要。</li></ul><p><strong>样本年龄</strong><br>针对论文提到的 <code>Example Age</code> 特征，可能很多人（包括我本人），一开始对此理解是；</p><blockquote><p>视频上架距离 log 的时间差，比如曝光的视频已经上架 48h，那么该特征便是 48。即文中的 <code>Days Since Upload</code>。</p></blockquote><p>然而，结合其他观点和重读论文，应该是：</p><blockquote><p><code>sample log</code> 距离当前的时间作为 <code>example age</code>，比如一条曝光/点击日志 ，发生在 2h 前，在被 training 的时候，也许其 video 已经上架 48h 了，但 example age 特征的取值是前者 2。</p></blockquote><p>作者提到，在加入该特征后，<strong>模型能够较好地学习到视频的 freshness 程度对 popularity 的影响</strong>。体现在下图的 <code>Days Since Upload</code> 后验分布的变化，新模型比 baseline 表现得更接近真实分布。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/youtubednn2.png" alt="youtubednn2"></p><h3 id="2-2-ranking"><a href="#2-2-ranking" class="headerlink" title="2.2 ranking"></a>2.2 ranking</h3><blockquote><p>优化目标：expected watch time，即视频期望观看时长。</p></blockquote><p>这里我们重点介绍一些核心，先上模型结构图：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/youtubednn3.png" alt="youtubednn3"></p><p><strong>特征：</strong></p><ul><li><code>video embedding</code>：<ul><li><code>impression video ID</code>: 当前待评估的video的embedding</li><li><code>watched video IDs</code>: 用户最近观看的N个 video 的 embedding 的 average pooling</li></ul></li><li><code>language embedding</code>:<ul><li><code>user language</code>: 用户语言的 embedding</li><li><code>video language</code>: 当前视频语言的 embedding</li></ul></li><li><code>time since last watch</code>: 用户上次观看同 channel 视频距今的时间</li><li><code>#previous impressions</code>: 该 video 已经被曝光给该用户的次数</li></ul><p>披露的特征设计非常经典且贴合实际业务，也许真实的特征体系比这要更丰富，但论文披露的更多是特征设计的思想。</p><ul><li><code>video embedding</code> 代表了捕捉用户历史行为序列关于当前视频的相关度；</li><li><code>language</code> 非常具有 youtube 全球视频网站的特色，捕捉用户与视频语言差异。</li><li>后面的2个统计值度量了一些时间因素，用户看同 <code>channel</code> 隔了多久以捕捉兴趣衰减，已经曝光的次数代表了用户忽视程度。</li></ul><p>此外，论文提到了一些<code>trick</code>：</p><ul><li>连续型特征做 <code>normalization</code>，利于模型收敛；</li><li>部分统计特征进行了 <code>box-cox 变化</code>，<strong>是一种增加特征非线性输入的办法，辅助模型训练</strong>；</li><li>长尾 video，其 embedding 用 0 来代替，降低长尾影响。</li></ul><p>模型将输入 embedding 进行 concat 后过了一个 3 层 DNN，之后类似 recall 环节，又分成了 training 和 serving 这2个分支，实际上这里是<strong>巧妙地将回归问题转分类了</strong>。</p><ul><li><code>training</code> 时，Weighted LR 方式，label 为是否观看，weight 是观看时长，作用在 loss 上；</li><li><code>serving</code> 时，使用 $e^{wx+b}$ 作为观看时长的预估值，其中指数部分是训练时 sigmoid 的 input 部分。</li></ul><h2 id="3-实践经验"><a href="#3-实践经验" class="headerlink" title="3 实践经验"></a>3 实践经验</h2><p>结合王哲老师的工程10问，这里总结和补充一下个人认为比较重要的实战经验，供自己复盘和其他读者批评。</p><h3 id="3-1-Recall-model-的性能问题"><a href="#3-1-Recall-model-的性能问题" class="headerlink" title="3.1 Recall model 的性能问题"></a>3.1 Recall model 的性能问题</h3><p><code>next watch</code> 的目标下，候选 video 有数百万量级，这在使用 softmax 进行多分类更低效。论文有提到这块，类似于 word2vec 的解决方案，<strong>负采样（negative sampling）或者 分层处理（hierarchical softmax）。</strong>效果是没有太大差异，一般负采样使用更广泛。</p><blockquote><p>其原理是：每次不用预估所有候选，而只采样一定数量（超参数）的样本作为负样本，甚至进一步可以转化成基于点积的二分类。</p></blockquote><h3 id="3-2-Ranking-Model-为什么选择用分类而不是回归？"><a href="#3-2-Ranking-Model-为什么选择用分类而不是回归？" class="headerlink" title="3.2 Ranking Model 为什么选择用分类而不是回归？"></a>3.2 Ranking Model 为什么选择用分类而不是回归？</h3><p>我认为在该问题上主要有2点。</p><ol><li>是业务目标的决策。如果是点击等目标天然满足，这里这不满足此。</li><li>实际工业应用中，以时长等连续型数据作为 Label 时，因其<strong>具有严重的长尾分布特性，这会使得回归模型在拟合过程中容易欠佳</strong>。一般体现在对 Label 值过低和过高的两端样本拟合偏差，MSE、PCOC等预估统计量偏差很大。因而一般会转成分类任务来处理。<br><em>具体原因则和回归模型特性以及样本梯度分布有关系，不过多赘述。相对地，分类模型则在这方面稳健性会高一些。</em></li></ol><h3 id="3-3-Ranking-model-使用-weighted-LR-的原理"><a href="#3-3-Ranking-model-使用-weighted-LR-的原理" class="headerlink" title="3.3 Ranking model 使用 weighted LR 的原理"></a>3.3 Ranking model 使用 weighted LR 的原理</h3><p>我们来理解一下为什么论文中做法能生效？这里阐述一下个人的理解。</p><script type="math/tex; mode=display">p = sigmoid(z)=\frac{e^z}{1 + e^{z}}</script><p>其中 z 是最后一层，即 $z = wx+b$。</p><p>那么 LR 模型的交叉熵 loss 为：</p><script type="math/tex; mode=display">loss = \sum -(\log{p} + \log(1-p))</script><p>那么，如果我们将 label 由“是否观看”变成 $\frac{t}{1+t}$ ，其中 t 是观看时长，那么 loss 就变成：</p><script type="math/tex; mode=display">loss = \sum -(\frac{t}{1+t} \cdot \log{p} + \frac{1}{1+t} \cdot \log(1-p))</script><p>注意！这时候，$p$ 拟合的就是 $\frac{t}{1+t}$，当其不断逼近的时候，就有：</p><script type="math/tex; mode=display">e^{z} \to t</script><p>故，<strong>在 serving 的时候就使用 $e^{z}=e^{wx+b}$作为观看时长 t 的预估值。</strong></p><p>进一步，因大多时候 1+t 等于或接近1，那么 loss 近似等价于：</p><script type="math/tex; mode=display">loss = \sum -(t \cdot \log{p} + 1 \cdot \log(1-p))</script><blockquote><p>注：这里类似王哲老师提到的“概率p往往是一个很小的值”来近似上一个道理。</p></blockquote><p>这便是一个<strong>目标是否观看的 weighted LR 的 loss，且 weight 为观看时间 t。</strong></p><p><strong>补充：</strong><br><code>Weighted LR</code> 实际上就是 <code>WCE(weighted cross entropy) Loss</code>，一般来说有两种方法：</p><ul><li>将正样本按照 weight 做重复 sampling；</li><li>通过改变梯度的 weight 来得到 Weighted LR （论文方法）。</li></ul><p>但是 <code>WCE</code> 有2个缺点：</p><ul><li>其实假设了样本分布服从几何分布，否则可能导致效果不好；</li><li>在低估时（$\hat y &lt; y$）梯度很大，高估时（$\hat y &gt; y$）梯度很小，很容易导致模型高估。</li></ul><h3 id="3-4-如何生成-user-和-video-的-embedding？"><a href="#3-4-如何生成-user-和-video-的-embedding？" class="headerlink" title="3.4 如何生成 user 和 video 的 embedding？"></a>3.4 如何生成 user 和 video 的 embedding？</h3><p>文中介绍用 word2vec 预训练得到。当然，我们知道，也可以使用 embedding layer 去联合训练，且往往这种实践更好，也是现如今的主流做法。</p><h3 id="3-5-example-age-的处理和原因？"><a href="#3-5-example-age-的处理和原因？" class="headerlink" title="3.5 example age 的处理和原因？"></a>3.5 example age 的处理和原因？</h3><p><strong>猜想：可能是从特征维度区分开历史样本的重要度，越新的样本（不是 video）可能对模型参考价值越大。有点类似于 biasnet 的修正作用。</strong></p><blockquote><p>比如有一个 video（不一定新上架） 1h 前有很多正样本，且越来越少，那么模型通过此特征可能能够感知到这种热度的时序变化趋势。</p></blockquote><p>但，对于 youtube 而言，其模型训练应该是：</p><ul><li>实时（至少日内）的；</li><li>批次增量的（即不会回溯更久远的样本）；</li></ul><p>假设样本的切分窗口为 T（比如2h），那么就一定 $0&lt;example age&lt;T$，那么问题来了：</p><blockquote><p>样本只能学习到该特征在（0，T）的分布，但论文图中却展示 Days Since Upload 特征的后验分布改善了。</p></blockquote><p>serving 的时候，虽然说通过置0来消除 example age 的bias，但实际上样本距离当下的时间又发生了变化，即 example age 信息发生了偏移。</p><p><strong>总结</strong>：也许这两个特征都在模型中，且交叉效应大于边际效应。描述 video fresh 程度交给 Days Since Upload 特征，再加上描述 sample fresh 程度的 example age。能够使得前者后验预估的更准确，也能够通过后者修正历史样本的 times bias。</p><h3 id="3-6-为什么对每个用户提取等数量的训练样本？"><a href="#3-6-为什么对每个用户提取等数量的训练样本？" class="headerlink" title="3.6 为什么对每个用户提取等数量的训练样本？"></a>3.6 为什么对每个用户提取等数量的训练样本？</h3><p>原文中提到“这是为了减少高度活跃用户对于loss的过度影响。”但实际上个人觉得这个方法可能不一定最适合。结合逻辑和个人经验，个人认为主要有2点：</p><ul><li>模型学习了一个有偏于真实分布的样本，预估会有偏差；</li><li>本末倒置，使得低活的影响力反而增强，线上 ABtest 的时候，其贡献往往不足，因线上业务收益往往也是高活主导，二八法则；</li></ul><p>虽如此，<strong>倒不是说高活不应该抑制</strong>，对于比较异常的高活，可以针对他们样本欠采样或者加正则，而不是只有通过提高重要性更低的非高活人群的作用。</p><h3 id="3-7-为什么不采取类似RNN的Sequence-model？"><a href="#3-7-为什么不采取类似RNN的Sequence-model？" class="headerlink" title="3.7 为什么不采取类似RNN的Sequence model？"></a>3.7 为什么不采取类似RNN的Sequence model？</h3><p>论文提到主要是过于依赖临近行为，使得推荐结果容易大范围趋同于最近看过的 video。<br>此外，还有一个比较重要的原因便是 serving 时候的 infer 性能问题。</p><h3 id="3-8-为什么长尾的video直接用0向量代替？"><a href="#3-8-为什么长尾的video直接用0向量代替？" class="headerlink" title="3.8 为什么长尾的video直接用0向量代替？"></a>3.8 为什么长尾的video直接用0向量代替？</h3><p>把大量长尾的video截断掉，主要是为了<strong>节省online serving中宝贵的内存资源</strong>。</p><p>但现在问问不会这么粗暴的使用，一般是<strong>将长尾物料进行聚类，以改善他们样本过于稀疏而带来的收敛困难问题</strong>。极端情况就是聚为1类，共享一个 embedding，类似于冷启动一样，当逐渐脱离长尾后再出场拥有独立的 embedding。</p><p><strong>参考文章</strong><br><a href="https://dl.acm.org/doi/pdf/10.1145/2959100.2959190">Deep Neural Networks for YouTube Recommendations</a><br><a href="https://www.cnblogs.com/xumaomao/p/15207305.html">Weighted LR （WCE Weighted cross entropy）</a><br><a href="https://blog.csdn.net/u012328159/article/details/123986042">推荐系统（二十）谷歌YouTubeDNN</a><br><a href="https://zhuanlan.zhihu.com/p/52169807">重读Youtube深度学习推荐系统论文</a><br><a href="https://zhuanlan.zhihu.com/p/52504407">YouTube深度学习推荐系统的十大工程问题</a><br><a href="https://blog.csdn.net/weixin_46838716/article/details/126459692?spm=1001.2014.3001.5502">排序04：视频播放建模</a></p><hr>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-背景&quot;&gt;&lt;a href=&quot;#1-背景&quot; class=&quot;headerlink&quot; title=&quot;1 背景&quot;&gt;&lt;/a&gt;1 背景&lt;/h2&gt;&lt;p&gt;这是一篇推荐算法领域经典的论文，它由 YouTube 在2016年发表在 RecSys 上的文章&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/2959100.2959190&quot;&gt;Deep Neural Networks for YouTube Recommendations&lt;/a&gt;。&lt;br&gt;这篇文章是诸多推荐算法工程师的必学经典，可能很多人多次重读都会有新的思考，本文也重点总结文章的核心内容与一些实战经验的思考。&lt;/p&gt;
&lt;h2 id=&quot;2-原理&quot;&gt;&lt;a href=&quot;#2-原理&quot; class=&quot;headerlink&quot; title=&quot;2 原理&quot;&gt;&lt;/a&gt;2 原理&lt;/h2&gt;&lt;p&gt;首先便是其展示的系统链路示意图，这块与大多主流方案没有什么区别。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/youtubednn0.png&quot; alt=&quot;youtubednn0&quot;&gt;&lt;/p&gt;
&lt;p&gt;论文分别介绍了在 &lt;code&gt;recall&lt;/code&gt; 和 &lt;code&gt;ranking&lt;/code&gt; 两个模块的方案，但可以说，recall 部分的重要性远大于 ranking。就此文章发表后的几年而言，&lt;em&gt;recall 往往还在工业界主流召回的候选方案中，但 ranking 的方案基本已经成为历史，很少再使用了&lt;/em&gt;，不过其思想还是值得学习的。&lt;/p&gt;</summary>
    
    
    
    <category term="精排模型" scheme="https://www.xiemingzhao.com/categories/%E7%B2%BE%E6%8E%92%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="算法总结" scheme="https://www.xiemingzhao.com/categories/%E7%B2%BE%E6%8E%92%E6%A8%A1%E5%9E%8B/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="精排" scheme="https://www.xiemingzhao.com/tags/%E7%B2%BE%E6%8E%92/"/>
    
    <category term="YouTubeDNN" scheme="https://www.xiemingzhao.com/tags/YouTubeDNN/"/>
    
    <category term="WCE" scheme="https://www.xiemingzhao.com/tags/WCE/"/>
    
  </entry>
  
  <entry>
    <title>从 MMOE 到 PLE 模型</title>
    <link href="https://www.xiemingzhao.com/posts/frommmoetople.html"/>
    <id>https://www.xiemingzhao.com/posts/frommmoetople.html</id>
    <published>2021-06-20T16:00:00.000Z</published>
    <updated>2025-04-02T16:08:59.762Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>在当下以深度学习为基调的推荐系统中，传统的单目标优化往往会带来一些不健康的生态指标变化，例如仅优化 CTR 可能会使得用户深层行为下降，往往推出一些博眼球和标题党等内容或商品。所以就催生了利用模型对 CLICK 后的各种深层行为的学习，最常用的便是 CVR（转化率），当然还有 cfr（收藏率）以及 viewtime（浏览时长）等等目标，视具体场景的业务指标而定。</p><p>为了解决上述问题，最原始的方法便是<code>多模型多分数融合</code>，也即对不同目标单独构建模型，线上独立打分进行融合，但是这样带来的问题便是深度行为的样本一般不足，难以使得模型学习的很好，而且独立建模成本高昂。为了进一步提效，目前主流的方法便是统一模型进行不同目标的<code>联合训练</code>，而模型内部不同任务之间存在一定信息共享。如此，</p><ul><li>一方面使得相关任务之间能够互相分享和补充信息，促进各任务学习能力；</li><li>另一方面，梯度在反向传播的时候，能够兼顾多个任务模式，提高模型的泛化能力。</li></ul><span id="more"></span><p>多任务学习模型有很多种，例如阿里的 ESSM 模型，谷歌的 MMOE 模型，包括本文重点介绍的 CGC 和 PLE 模型，来自由腾讯团队发表的获得 RecSys2020 最佳长论文奖得文章：<a href="https://dl.acm.org/doi/abs/10.1145/3383313.3412236">Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations</a>。当然还有很多其他变种的多任务模型，而在接下来得内容将聚焦介绍模型结构得迭代变化。</p><h2 id="2-目标"><a href="#2-目标" class="headerlink" title="2 目标"></a>2 目标</h2><p>实践中上述的理想目标往往会不及预期，主要原因在于, 当任务之间相关性较高的时候，能够一定程度通过信息共享来促进模型的学习效率，但不太相关时会产生<code>负迁移（negative transfer）</code>，即网络表现变差。前面提到谷歌提出的<code>MMOE</code>模型就是为了缓解负迁移现象。但，另一方面，工业实践中往往还面临一个普遍的问题，那就是<code>跷跷板现象（seesaw phenomenon）</code>。也就是在多任务联合训练中，往往部分任务能够相对于独立训练获得提升，但同时伴随着其他个别任务效果下降。</p><p>本人在实际工作中也遇到了上述问题，<code>跷跷板现象</code>是比较明显的结果导向，例如 cfr（收藏率）提升了，但 ctr（点击率）下降了。而对于<code>负迁移</code>问题，比较好验证其触因，可以直接计算样本中不同任务 label 之间的相关性。以博主自己的一个实际工作场景为例：</p><blockquote><p>在某电商瀑布流的推荐中，样本中核心 label 有 ctr（点击）、ccr（查看细分sku）、cfr(收藏)、cvr（下单）。当然不仅限于这些，对于场景核心 label 的选取主要取决于业务目标和 label 的覆盖度。部分同学在实际中可能会认为行为之间应该具有较强的相关性，然而实际上结果可能大相径庭。如下表所示是实际样本的一个统计结果（大约一天几亿量级的数据），可以发现在渗透漏斗链路上，离 ctr 越远的行为相关性越低。</p></blockquote><p>本文介绍的从 MMOE 迭代到 PLE 的模型，其提出者目的正是为了缓解上述问题。PLE 模型也是被部署在了腾讯的视频推荐系统中，其线上多目标<code>分数融合</code>方式如下：</p><script type="math/tex; mode=display">\begin{array}{r}score = pVTR^{w_{VTR}} \times pVCR^{w_{VCR}} \times pSHR^{w_{SHR}} \times \dots \times pCMR^{w_{CMR}} \times f(video\_len)\end{array}</script><p>其中，公式中每一项右上角的 $w$ 都是权重超参数。video_len 表示视频的原始时长，其有一个映射函数 $f$，是一个 non-linear 函数，可取 sigmoid 或者 log 函数。模型结构如下所示：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/mmoetople0.png" alt="mmoetople0"></p><p>在这里，<code>VCR (View Completion Rate)</code>和<code>VTR (View-Through-Rate)</code>是最重要的2个指标。VCR 是指视频完成度，例如1min的视频看了0.5min，便有VCR=0.5。以此作为 Label 可以构建一个回归问题，以 MSE 作为评估指标。VTR 则是指是否是一次有效观看，这一般可以构建成一个二分类模型，AUC 作为评估目标。</p><p>值得注意的是，此Label的打标签一般因业务场景不同而有所区别:</p><ul><li>需要通过列表主动点击到落地详情页的时候，一般 VTR 对应的 Label 就是用户是否主动点击；</li><li>如果是单列自动播放的视频流的时候，就会存在视频的默认播放问题，需要进行一定的阈值截断来进行 Label 的打标签；</li></ul><p>如上所述，实际上两个任务的关系很复杂。以腾讯实际场景为例，VTR是播放动作和VCR的耦合结果，因为在有wifi下，部分场景有自动播放机制，播放率相对就较高，就需要设定有效播放时长的阈值。没有自动播放机制时候，相对更为简单，且播放率会较低。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/mmoetople1.png" alt="mmoetople1"></p><p>如上图所示，是其团队对比了一些主流 MTL 模型在 VCR 和 VTR 任务上相对单任务模型的离线对比结果。从图中可以看到，大多 MTL 模型都是一个任务好于单任务模型另一个则较差，这便是前文提到的<code>跷跷板现象（seesaw phenomenon）</code>。以<code>MMOE</code>为例，其在 VTR 上有一定收益，但是在 VCR 上几乎无收益。核心原因在于，其 Experts 是被所有任务共享，会有噪声，且他们之间没有交互，联合训练有折扣。</p><p>而该团队提出的 PLE 模型在实验对比中最好，其线上实验也取得了2.23%的view-count和1.84%的阅读时长提升效果。我们可以先将其<strong>核心优化点</strong>总结成如下：</p><ul><li>解耦 Experts 网络，改进了模型结构；</li><li>优化了多目标 loss 融合的方法，提高了训练的效率；</li></ul><h2 id="3-MMOE"><a href="#3-MMOE" class="headerlink" title="3 MMOE"></a>3 MMOE</h2><p>前文提到很多与<code>MMOE</code>(Multi-gate Mixture-of-Experts)模型的对比，咱们先来回顾一下该模型的结构。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/mmoetople2.png" alt="mmoetople2"></p><p>如上图所示，该模型实际上是在多个 Expert 基础上，对每一个任务的 Tower 都构建一个 Gate 网络。整个模型可以用数理表达式：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;array&#125;&#123;l&#125; </span><br><span class="line">f^k(x)=\sum_&#123;i=1&#125;^n g^k(x)_&#123;i&#125; f_&#123;i&#125;(x)</span><br><span class="line">\\</span><br><span class="line">g^k(x)=Softmax(W_&#123;gk&#125;x)</span><br><span class="line">\end&#123;array&#125; </span><br></pre></td></tr></table></figure><p>其中，$g^k$ 表示第 k 个任务中的用来控制 experts 结果的门控网络。</p><p><strong>该网络的目的是使得每个 Task 通过自己独立的 Gate 网络来学习不同的 Experts 网络的组合模式</strong>。模型的 loss 一般是各个任务的 loss 加和，如果其中某个任务的 loss 占比过高，则梯度主要沿着其下降的方向更新，有可能降低其他任务的精度。</p><h2 id="4-CGC"><a href="#4-CGC" class="headerlink" title="4 CGC"></a>4 CGC</h2><p>这里介绍的<code>CGC</code>（Customized Gate Control）模型是一种单层多任务网络结构，它是本文介绍的最终版本PLE模型的简单版本或者说其组成部分。下面展示的是其网络结构：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/mmoetople3.png" alt="mmoetople3"></p><p>如上图所示，各模块结构的含义如下：</p><ul><li><code>shared experts</code>：共享的专家网络组，即上图 Experts Shared 中的$E_{S,1}$等；</li><li><code>task-specific expert</code>：各个任务专享的专家网络组，例如 Experts A；</li><li><code>task-specific tower</code>：各个任务的输出塔，例如 Tower A；</li><li><code>task-specific Gating</code>：各个任务的门控网络，例如 Tower A的入口与Experts的连接处；</li></ul><p>从上述结构可以看得出来，<code>shared experts</code> 会与所有任务链接，学习共享信息，而 <code>task-specific expert</code> 只会受到自己任务的影响，<code>task-specific tower</code> 则是由 <code>task-specificGating</code> 将对应的 <code>task-specific expert</code> 和 <code>shared experts</code> 组合后作为其输入的。这一过程可以表述称如下数理公式：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;array&#125;&#123;l&#125; </span><br><span class="line">y^k(x)=t^k(g^k(x))</span><br><span class="line">\\</span><br><span class="line">g^k(x)=w^k(x)S^k(x)</span><br><span class="line">\\</span><br><span class="line">w^k(x)=Softmax(W^k_g x)</span><br><span class="line">\\</span><br><span class="line">S^k(x)=[E^T_&#123;(k,1)&#125;,E^T_&#123;(k,2)&#125;,\dots ,E^T_&#123;(k,m_k)&#125;,E^T_&#123;(s,1)&#125;,E^T_&#123;(s,2)&#125;,\dots ,E^T_&#123;(s,m_s)&#125;]</span><br><span class="line">\end&#123;array&#125; </span><br></pre></td></tr></table></figure><br>其中，公式(1)表示任务k的输出结果，公式(3)表示门控网络的结构，公式(2)则表示基于门控网络将公式(4)中的专家网络组融合的过程。可以发现其与MMOE最大的区别便是不同Task之间除了共享的 <code>Shared Experts</code> 网络组之外还有各自独享的 <code>Task-specific Experts</code>，这也是接下来的 PLE 模型的核心组成模块。</p><h2 id="5-PLE"><a href="#5-PLE" class="headerlink" title="5 PLE"></a>5 PLE</h2><p>基于 <code>CGC</code> 的结构，<code>PLE</code>（Progressive Layered Extraction）则是一个升级版本的结构，它扩增了 Experts 之间的交互，结构如下所示：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/mmoetople4.png" alt="mmoetople4"></p><p>可以清晰的看到，其与CGC不同的是增加了多级的 <code>Extraction Networks</code>，而每一级的<code>Extraction Networks</code> 基本与CGC一致，旨在提取更高级别的共享信息。可以发现，每层 <code>Shared Experts</code> 吸收了下层的所有网络结构信息，而任务独享的 <code>Task-specific Experts</code> 则仅从其自己对应模块和<code>Shared Experts</code>中获取共享信息。整个过程可以简化成：</p><script type="math/tex; mode=display">y^k(X)=t^k(g^{k,N}(x))</script><h2 id="6-MTL-类型总结"><a href="#6-MTL-类型总结" class="headerlink" title="6 MTL 类型总结"></a>6 MTL 类型总结</h2><h3 id="6-1-Single-Level-MTL"><a href="#6-1-Single-Level-MTL" class="headerlink" title="6.1 Single-Level MTL"></a>6.1 Single-Level MTL</h3><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/mmoetople5.png" alt="mmoetople5"></p><p>如上图所示，是5个经典的Single-Level MTL模型：</p><ol><li><p><code>Hard Parameter Sharing</code>：最常见的MTL模型，底层的模块是share的，然后共享层的输出分别进入到每个Task的Tower中。当两个Task相关性较高时，用这种结构效果一般不错，但任务相关性不高时，会存在负迁移现象，导致效果不理想。</p></li><li><p><code>Asymmetry Sharing</code>（不对称共享）：不同Task的底层模块有各自对应的输出，但其中部分任务的输出会被其他Task所使用，而部分任务则使用自己独有的输出。交叉共享的部分需要认为定义，变数较多。</p></li><li><p><code>Customized Sharing</code>（自定义共享）：不同Task的底层模块不仅有各自独立的输出，还有共享的输出。2和3这两种结构同样是论文提出的，但相对不重点。</p></li><li><p><code>MMOE</code>：是大家比较熟悉的经典 MTL。底层包含多个 Expert，然后基于门控机制，不同任务会对不同 Expert 的输出进行过滤。</p></li><li><p><code>CGC</code>：这就是本文重点介绍的一个，与 MMOE 的区别就是每个 Task 有自己独享的 Experts。</p></li></ol><h3 id="6-2-Multi-Level-MTL"><a href="#6-2-Multi-Level-MTL" class="headerlink" title="6.2 Multi-Level MTL"></a>6.2 Multi-Level MTL</h3><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/mmoetople6.png" alt="mmoetople6"></p><p>如上图所示，是4个经典的 <code>Single-Level MTL</code> 模型：</p><ol><li><p><code>Cross-Stitch Network</code>（”十字绣”网络）：出自论文<a href="https://arxiv.org/pdf/1604.03539.pdf">《Cross-stitch Networks for Multi-task Learning》</a>。</p></li><li><p><code>Sluice Network</code>（水闸网络）：出自论文《<a href="https://arxiv.org/pdf/1705.08142v1.pdf">Sluice networks: Learning what to share between loosely related tasks</a>》.</p></li><li><p><code>ML-MMOE</code>：前文已经有介绍。</p></li><li><p><code>PLE</code>：这便是本文重点介绍的对象。</p></li></ol><h2 id="7-MTL-的-Loss-优化"><a href="#7-MTL-的-Loss-优化" class="headerlink" title="7 MTL 的 Loss 优化"></a>7 MTL 的 Loss 优化</h2><p>在传统的MTL任务中，一般设定各个任务样本空间一致，然后训练的时候将各个任务的 loss 加权求和作为模型优化的总 loss 即可：</p><script type="math/tex; mode=display">L(\theta_1, \dots , \theta_K, \theta_s)=\sum_{k=1}^K w_k L_k(\theta_k, \theta_s)</script><p>但实际上，<strong>不同Task的样本空间可能是不一致的</strong>。如下图所示。例如，假设 item 的 share 按钮在详情页，那么用户必须先 click 后，才能进行  share的动作，所以 share 的样本应该是 click 的一个子集，而不能粗暴的将没有 click（自然没有share）的样本也作为 share 的负样本，如此是有偏的。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/mmoetople7.png" alt="mmoetople7"></p><p>故，最终PLE在训练的时候 loss 构建如下：</p><script type="math/tex; mode=display">L_k(\theta_k,\theta_s)=\frac{1}{\sum_i \delta_k^i} \sum_i \delta_k^i loss_k(\hat y_k^i (\theta_k, \theta_s),y_k^i)</script><p>其中，$\delta_k^i$是一个示性变量，表示第i个样本是否属于第k个 Task 的样本空间，实际上起到了样本  loss入场的筛选过滤作用。</p><p>除此之外，该团队还从 <code>Multi-task learning</code> 对 <code>loss weight</code> 敏感的角度出发，为了兼顾静态 weight 不如动态 weight 有效，并且不用像阿里提出的帕累托最优这种复杂的方式来优化。他们最终采用人共设置初始 loss weight，但是在不同的 train step 会进行 update，具体方式如下所示：</p><script type="math/tex; mode=display">w_k^{(t)}=w_{k,0} \times \gamma_k^t</script><p>其中，$w_{k,0}$是人工设置的初始 loss wight，$\gamma_k$也是权重衰减的超参数，$t$则是 training step。</p><p>此优化确实较为合理，笔者在实际中也取得了效果。从数理角度理解，拆分样本空间后，预测的是每个阶段行为的条件概率，而不是联合改了，相对模式更容易学习。但是，<strong>此优化不仅限于PLE模型，实际上对任何一个MTL模型都适配的。</strong></p><h2 id="8-Code"><a href="#8-Code" class="headerlink" title="8 Code"></a>8 Code</h2><p>由于本文重点讲解的是 PLE 模型，且 CGC 模型也是 PLE 的一个组成部分，所以 MMOE 和 CGC 的 code 在此就不提，咱们重点介绍一下 PLE 模型的结构模块代码。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> .multiTaskModel <span class="keyword">import</span> multiTaskModel</span><br><span class="line"><span class="keyword">from</span> modules.experts <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gate</span>(<span class="params"><span class="built_in">input</span>, unit, name = <span class="string">&quot;gate&quot;</span></span>):</span><br><span class="line">    net = tf.layers.dense(inputs=<span class="built_in">input</span>, units=unit, name=<span class="string">&#x27;%s/dense&#x27;</span> % name)</span><br><span class="line">    gate = tf.nn.softmax(net, axis=<span class="number">1</span>, name=<span class="string">&#x27;%s/softmax&#x27;</span> % name)</span><br><span class="line">    <span class="keyword">return</span> gate</span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pleLayer</span>(<span class="params">input_list, num_expert, num_task, dnn_dims, is_training, name = <span class="string">&quot;pleLayer&quot;</span></span>):</span><br><span class="line">    expert_feat_list = []</span><br><span class="line">    <span class="comment"># num_task + 1 experts</span></span><br><span class="line">    <span class="keyword">for</span> label_id <span class="keyword">in</span> <span class="built_in">range</span>(num_task + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> expert_id <span class="keyword">in</span> <span class="built_in">range</span>(num_expert):</span><br><span class="line">            <span class="comment"># buile expert</span></span><br><span class="line">            expert_dnn = dnn(input_list[label_id], dnn_dims, is_training=is_training, usebn=<span class="literal">True</span>, activation=<span class="string">&quot;tf.nn.leaky_relu&quot;</span>,</span><br><span class="line">                             name=<span class="string">&#x27;%s/label%d/dnn%d&#x27;</span> % (name, label_id, expert_id))</span><br><span class="line">            expert_feat_list.append(expert_dnn)</span><br><span class="line"></span><br><span class="line">    experts_output_list = []</span><br><span class="line">    <span class="keyword">for</span> task_id <span class="keyword">in</span> <span class="built_in">range</span>(num_task):</span><br><span class="line">        <span class="comment"># build gate, unit equals task &amp; share expert&#x27;s nums</span></span><br><span class="line">        gate_feat = gate(<span class="built_in">input</span> = input_list[task_id], unit = num_expert * <span class="number">2</span>, name = <span class="string">&#x27;%s/gate%d&#x27;</span> % (name,task_id))</span><br><span class="line">        gate_feat = tf.expand_dims(gate_feat, -<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># staking，task &amp; share experts</span></span><br><span class="line">        experts_feat = tf.stack(expert_feat_list[task_id*num_expert:(task_id+<span class="number">1</span>)*num_expert] +</span><br><span class="line">                                expert_feat_list[-num_expert:], axis=<span class="number">1</span>, name=<span class="string">&quot;%s/feat&quot;</span> % name)</span><br><span class="line">        <span class="comment"># attention</span></span><br><span class="line">        task_input = tf.multiply(experts_feat, gate_feat, name = <span class="string">&#x27;%s/task%d/multiply&#x27;</span> % (name,task_id))</span><br><span class="line">        <span class="comment"># reduce dim for tower input</span></span><br><span class="line">        task_input = tf.reduce_sum(task_input, axis=<span class="number">1</span>, name = <span class="string">&#x27;%s/task%d/output&#x27;</span> % (name,task_id))</span><br><span class="line">        experts_output_list.append(task_input)</span><br><span class="line">    <span class="comment"># share expert gate</span></span><br><span class="line">    gate_feat = gate(<span class="built_in">input</span>=input_list[num_task], unit=num_expert * (num_task+<span class="number">1</span>), name=<span class="string">&#x27;%s/gateshare&#x27;</span> % (name))</span><br><span class="line">    gate_feat = tf.expand_dims(gate_feat, -<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># staking， all experts</span></span><br><span class="line">    experts_feat = tf.stack(expert_feat_list, axis=<span class="number">1</span>, name=<span class="string">&quot;%s/featshare&quot;</span> % name)</span><br><span class="line">    <span class="comment"># attention</span></span><br><span class="line">    task_input = tf.multiply(experts_feat, gate_feat, name=<span class="string">&#x27;%s/taskshare/multiply&#x27;</span> % (name))</span><br><span class="line">    <span class="comment"># reduce dim for tower input</span></span><br><span class="line">    task_input = tf.reduce_sum(task_input, axis=<span class="number">1</span>, name=<span class="string">&#x27;%s/taskshare/output&#x27;</span> % (name))</span><br><span class="line">    experts_output_list.append(task_input)</span><br><span class="line">    <span class="keyword">return</span> experts_output_list    </span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ple</span>(<span class="title class_ inherited__">multiTaskModel</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, params</span>):</span><br><span class="line">        <span class="variable language_">self</span>.expertNum = <span class="number">4</span></span><br><span class="line">        <span class="variable language_">self</span>.expertDims = [<span class="number">512</span>,<span class="number">512</span>,<span class="number">512</span>]</span><br><span class="line">        <span class="variable language_">self</span>.extractLevel = <span class="number">3</span></span><br><span class="line">        <span class="built_in">super</span>(ple, <span class="variable language_">self</span>).__init__(params)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build_graph</span>(<span class="params">self, features, is_training, params</span>):</span><br><span class="line">        dnn_feats = params[<span class="string">&#x27;feature_columns&#x27;</span>][<span class="string">&#x27;dnn_feats&#x27;</span>]</span><br><span class="line">        <span class="built_in">input</span> = tf.feature_column.input_layer(features, dnn_feats)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># task_input_list[-1] is share experts input</span></span><br><span class="line">        task_input_list = [<span class="built_in">input</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.lable_size + <span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">for</span> level <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.extractLevel):</span><br><span class="line">            task_input_list = pleLayer(task_input_list, <span class="variable language_">self</span>.expertNum, <span class="variable language_">self</span>.lable_size, dnn_dims = <span class="variable language_">self</span>.expertDims,</span><br><span class="line">                                       is_training=is_training, name = <span class="string">&quot;pleLayer%d&quot;</span> % level)</span><br><span class="line"></span><br><span class="line">        tower_outputs = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.lable_size):</span><br><span class="line">            tower_dnn = dnn(task_input_list[i], <span class="variable language_">self</span>.dnnDims, name=<span class="string">&quot;tower_%d&quot;</span> % i, is_training=is_training)</span><br><span class="line">            tower_output = tf.layers.dense(inputs=tower_dnn, units=<span class="number">1</span>, name=<span class="string">&#x27;tower_output_%d&#x27;</span> % i,</span><br><span class="line">                                           activation=tf.nn.sigmoid)</span><br><span class="line">            tower_outputs[<span class="string">&#x27;tower_output_%d&#x27;</span> % i] = tower_output</span><br><span class="line">        <span class="variable language_">self</span>.ctr_pred = tf.reshape(tower_outputs[<span class="string">&quot;tower_output_0&quot;</span>], [-<span class="number">1</span>], name=<span class="string">&quot;ctr&quot;</span>)</span><br><span class="line">        <span class="variable language_">self</span>.cvr_pred = tf.reshape(tower_outputs[<span class="string">&quot;tower_output_1&quot;</span>], [-<span class="number">1</span>], name=<span class="string">&quot;cvr&quot;</span>)</span><br></pre></td></tr></table></figure><br>代码整体没有什么难度，基本上就按照模型结构图来实现即可。唯一需要注意便是关键聚合层的维度对齐，支持灵活控制模型各个 Experts 模块的结构调整即可。</p><p><strong>参考文献</strong><br><a href="https://mp.weixin.qq.com/s?__biz=MzU2ODA0NTUyOQ==&amp;mid=2247490093&amp;idx=1&amp;sn=58cec36693c33742d7f5673246b0813f&amp;chksm=fc92a09bcbe5298db648d48fefc90e173bdb13890c600538b0152bd7bd58971da02724e063e0&amp;scene=126&amp;sessionid=1605088596&amp;key=301d1d633ed7664ead5e72db2696e63bc0cc9f81eba3c1fa7ce072479ea99f43857c7776607b03640f45fdd4c6a0989118dcd674dfc926a6fc2baa36ed8a60dde9a196fdc04f8cd521d08bfac5c0f97b344563ee9cac3cd782d65fef03e0f14c9af2bf7c11622ea04661600b67c4f51f5aece7889b00f144c7a177883642d2f4&amp;ascene=1&amp;uin=Mjg1NTU5MTQxMA==&amp;devicetype=Windows+10+x64&amp;version=6300002f&amp;lang=zh_CN&amp;exportkey=A6HrIa8MEBjQ0POQs4ps3Pk=&amp;pass_ticket=8hNub+Fu4yLIlzlFzkmkkQMUkX4moojyuksiXcSdcWti8q5+iG2QZTCpgM1wGGdz&amp;wx_header=0">腾讯 at RecSys2020最佳长论文 - 多任务学习模型PLE</a><br><a href="https://zhuanlan.zhihu.com/p/52566508">深度神经网络中的多任务学习汇总</a><br><a href="https://zhuanlan.zhihu.com/p/354055223">【论文笔记日更10】腾讯PLE模型（RecSys 2020最佳论文）</a><br><a href="https://mp.weixin.qq.com/s/1ZZvEfQUDQat6nFnF67GcQ">多目标优化（三）recsys2020最佳长论文奖PLE</a><br><a href="https://github.com/ShaoQiBNU/Google_MTL">Google多任务模型</a><br><a href="https://zhuanlan.zhihu.com/p/291406172">多目标学习在推荐系统的应用(MMOE/ESMM/PLE)</a><br><a href="https://lumingdong.cn/multi-task-learning-in-recommendation-system.html">推荐系统中的多任务学习</a></p><hr>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-引言&quot;&gt;&lt;a href=&quot;#1-引言&quot; class=&quot;headerlink&quot; title=&quot;1 引言&quot;&gt;&lt;/a&gt;1 引言&lt;/h2&gt;&lt;p&gt;在当下以深度学习为基调的推荐系统中，传统的单目标优化往往会带来一些不健康的生态指标变化，例如仅优化 CTR 可能会使得用户深层行为下降，往往推出一些博眼球和标题党等内容或商品。所以就催生了利用模型对 CLICK 后的各种深层行为的学习，最常用的便是 CVR（转化率），当然还有 cfr（收藏率）以及 viewtime（浏览时长）等等目标，视具体场景的业务指标而定。&lt;/p&gt;
&lt;p&gt;为了解决上述问题，最原始的方法便是&lt;code&gt;多模型多分数融合&lt;/code&gt;，也即对不同目标单独构建模型，线上独立打分进行融合，但是这样带来的问题便是深度行为的样本一般不足，难以使得模型学习的很好，而且独立建模成本高昂。为了进一步提效，目前主流的方法便是统一模型进行不同目标的&lt;code&gt;联合训练&lt;/code&gt;，而模型内部不同任务之间存在一定信息共享。如此，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一方面使得相关任务之间能够互相分享和补充信息，促进各任务学习能力；&lt;/li&gt;
&lt;li&gt;另一方面，梯度在反向传播的时候，能够兼顾多个任务模式，提高模型的泛化能力。&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="精排模型" scheme="https://www.xiemingzhao.com/categories/%E7%B2%BE%E6%8E%92%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="算法总结" scheme="https://www.xiemingzhao.com/categories/%E7%B2%BE%E6%8E%92%E6%A8%A1%E5%9E%8B/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="精排" scheme="https://www.xiemingzhao.com/tags/%E7%B2%BE%E6%8E%92/"/>
    
    <category term="PLE" scheme="https://www.xiemingzhao.com/tags/PLE/"/>
    
    <category term="CGC" scheme="https://www.xiemingzhao.com/tags/CGC/"/>
    
    <category term="MTL" scheme="https://www.xiemingzhao.com/tags/MTL/"/>
    
  </entry>
  
  <entry>
    <title>MMOE 模型解析</title>
    <link href="https://www.xiemingzhao.com/posts/mmoemodel.html"/>
    <id>https://www.xiemingzhao.com/posts/mmoemodel.html</id>
    <published>2021-06-05T16:00:00.000Z</published>
    <updated>2025-04-02T16:07:31.268Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>本文介绍的<code>MMOE</code>模型全称是<code>Multi-gate Mixture-of-Experts</code>，来自 Google 在 2018 年 KDD 上发表的论文<a href="https://dl.acm.org/doi/pdf/10.1145/3219819.3220007">Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts</a>。核心目标是改善多任务学习在任务之间不太相关的时候效果不好的问题。下面有两个相关学习资源：</p><ul><li>视频简介的<a href="https://www.youtube.com/watch?v=Dweg47Tswxw">youtube地址</a>;</li><li>一个用keras框架实现的<a href="https://github.com/drawbridge/keras-mmoe">开源地址</a>。</li></ul><h2 id="2-动机"><a href="#2-动机" class="headerlink" title="2 动机"></a>2 动机</h2><p>多任务学习一般是为了在训练多个相关任务的时候可以使得它们之间能够共享信息，提高学习效率和模型的泛化能力。但实际应用中往往难以如此理想化，因为<strong>任务之间往往相关性不够强</strong>，这时候就容易产生以下两个问题：</p><blockquote><p><code>负迁移（negative transfer）</code>：即网络表现变差<br><code>跷跷板现象（seesaw phenomenon）</code>：也就是个别任务相对于独立训练获得提升，但其他任务效果下降。</p></blockquote><span id="more"></span><p>论文团队在实际中也做了这方面的实验，它们仿真了不同相关性的数据集，然后在他们上面测试模型的训练效果，如下图所示：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/mmoe0.png" alt="mmoe0"></p><p>上图中的 <code>correlation</code> 是 <code>Pearson相关性系数</code>，从中可以看出相关性越高 loss 收敛的速度越快，值越小，说明效果越好。</p><p>在实际中，大多数任务也不足够相关，例如笔者自身的经验，在某电商瀑布流推荐场景，其中点击与查看价格相关性尚可（约0.48），但是收藏乃至成交等越深的行为与其相关性就越差了（&lt;0.1）。有时候也不能够直接判断任务之间的相关性，但无论如何，解决相关性不高的多任务学习本身就是一个很有实际意义的工作。</p><h2 id="3-模MMOE型"><a href="#3-模MMOE型" class="headerlink" title="3 模MMOE型"></a>3 模MMOE型</h2><p>在<code>MMOE</code>模型提出来之前，也有一些相关的探索，论文作者也提到他们与<code>MMOE</code>之间的关系。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/mmoe1.png" alt="mmoe1"></p><p>入上图所示，(a) 和 (b)是两个较为基础的<code>shared embedding</code>模型。</p><p>图 (a) 表示的是<code>Shared-Bottom model</code>，input 层之后进入模型 shared 的 bottom 模块，一般也就是一个 DNN 结构。之后，分别进入每个任务自己的 Tower，例如图中的 Tower A 和 Tower B，得到最终每个 Task 的 output。整个过程可以简单的用下述公式来表述：</p><script type="math/tex; mode=display">y^k = h^k (f(x))</script><p>其中，</p><ul><li>k 表示第几个目标 Task;</li><li>h 表示每个 Task 各自的 Tower 网络;</li><li>f 则表示地步共享的 bootom 网络。</li></ul><p>图 (b) 表示的是<code>MOE模型</code>，其与（a）比较显著的区别有2处：</p><ul><li>input 层后共享的 bootom 结构改成了多个 Experts 网络，图中是3个，并且各自参数不共享；</li><li>input 层出了进入共享的 Experts 网络外，还用来生成 Task 各自的 Gate 网络，作为 Experts 进入每个 Task Tower 网络的门控权重。</li></ul><p>可以发现，MOE 通过 input 生成 Gate 是为了赋予每个 Experts 网络进入 Task Tower 的权重，也是一种 <code>Attention</code> 的思想。其模型结构也以简单的总结如下公式：</p><script type="math/tex; mode=display">y^k = h^k(\sum_{i=1}^n g_i f_i(x))</script><script type="math/tex; mode=display">g = Softmax(W_{g}x)</script><p>其中，g 便是 Gate网络，i 表示第 i 个 Experts 网络。可以发现在 Gate 网络对 Experts 的输出进行加权求和之后，直接进入每个 Task 的 Tower 网络，所以每个 Tower 的 input 是一样的。</p><p>图 (b) 表示的是<code>MMOE模型</code>，也是论文作者提出的结构。其与 <code>MOE</code> 最大的区别就是：</p><blockquote><p>对于每个 Task 都有用自己独享的 Gate 网络结构。</p></blockquote><p>如此，每个 Task 可以根据任务需要，利用自己的 Gate 网络计算每个 Experts 进入 Tower 的权重，Gate 部分可简述为下列公式：</p><script type="math/tex; mode=display">Gate^k = Softmax(W_{gk}x)</script><p>在实际中，作者实验也证明相关性较高的任务 MMOE 其实效果不明显，与 MOE 相差无异。<strong>但是在相关性较差的数据上，效果就与MOE拉开一定的差异</strong>，如下图所示。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/mmoe2.png" alt="mmoe2"></p><p>不过这也和数据集和操作过程有一定关系，笔者在实际工作中发现<strong>两个相关问题</strong>：</p><ul><li>在大数据集且控制其他变量条件下，MMOE 往往很难与一般的 Shared-bootom 网络结构显著的拉开差距；</li><li>在多场景建模中往往会有一定的的价值拟合不同空间的样本和目标；</li><li>还有一种<code>假提升</code>情况，也就是模型收敛速度较快，但是最终水位相差无几。<br>（举例，例如在实际新模型回刷数据中，将 base model 和 MMOE 都从某一天开始刷数据，会发现在 AUC 或者 loss 等指标上，MMOE 很快并且连续一段时间始终好于 base model，然而随着数据的积累，这种 diff 逐渐较小直至最终极其微小，这一般就是新模型收敛速度快的原因，会给算法工程师一个很大幅度的<code>假提升</code>现象。）</li></ul><h2 id="4-Code"><a href="#4-Code" class="headerlink" title="4 Code"></a>4 Code</h2><p>MMOE 这里有两个 coding 的矩阵提速的点：</p><ul><li>在构建Shared-Experts网络结构的时候，Expert 网络的个数可以作为一个维度，即直接生成一个<code>三维tensor</code>：$input_dim \times expert_unit \times experts_num$</li><li>Gate 网络也可以将 Task 的个数作为其一个维度来一次性生成一个三维 tensor:$input_dim \times experts_num \times tasks_num$</li></ul><p>以上技巧在前文提到的keras框架实现的<a href="https://github.com/drawbridge/keras-mmoe">开源地址</a>中有所体现。这里展示一个笔者自己按照图结构一个一个模块写出来的版本：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> .multiTaskModel <span class="keyword">import</span> multiTaskModel</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gate</span>(<span class="params"><span class="built_in">input</span>, unit, name = <span class="string">&quot;gate&quot;</span></span>):</span><br><span class="line">    net = tf.layers.dense(inputs=<span class="built_in">input</span>, units=unit, name=<span class="string">&#x27;%s/dense&#x27;</span> % name)</span><br><span class="line">    gate = tf.nn.softmax(net, axis=<span class="number">1</span>, name=<span class="string">&#x27;%s/softmax&#x27;</span> % name)</span><br><span class="line">    <span class="keyword">return</span> gate</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">experts</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_expert, num_task, dnn_dims, name = <span class="string">&quot;experts&quot;</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.num_expert = num_expert</span><br><span class="line">        <span class="variable language_">self</span>.num_task = num_task</span><br><span class="line">        <span class="variable language_">self</span>.dnn_dims = dnn_dims</span><br><span class="line">        <span class="variable language_">self</span>.name = name</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, <span class="built_in">input</span>, is_training</span>):</span><br><span class="line">        expert_feat_list = []</span><br><span class="line">        <span class="keyword">for</span> expert_id <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_expert):</span><br><span class="line">            <span class="comment"># buile expert</span></span><br><span class="line">            expert_dnn = dnn(<span class="built_in">input</span>, <span class="variable language_">self</span>.dnn_dims, is_training=is_training, usebn=<span class="literal">True</span>, activation=<span class="string">&quot;tf.nn.leaky_relu&quot;</span>,</span><br><span class="line">                             name=<span class="string">&#x27;%s/dnn%d&#x27;</span> % (<span class="variable language_">self</span>.name, expert_id))</span><br><span class="line">            expert_feat_list.append(expert_dnn)</span><br><span class="line">        <span class="comment"># staking，Bxnum_expertxdim not Bx(num_expertxdim)</span></span><br><span class="line">        experts_feat = tf.stack(expert_feat_list, axis=<span class="number">1</span>, name=<span class="string">&quot;%s/feat&quot;</span> % <span class="variable language_">self</span>.name)</span><br><span class="line"></span><br><span class="line">        experts_output_list = []</span><br><span class="line">        <span class="keyword">for</span> task_id <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_task):</span><br><span class="line">            <span class="comment"># build gate</span></span><br><span class="line">            gate_feat = gate(<span class="built_in">input</span>=<span class="built_in">input</span>, unit=<span class="variable language_">self</span>.num_expert, name=<span class="string">&#x27;%s/gate%d&#x27;</span> % (<span class="variable language_">self</span>.name, task_id))</span><br><span class="line">            gate_feat = tf.expand_dims(gate_feat, -<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># attention</span></span><br><span class="line">            task_input = tf.multiply(experts_feat, gate_feat, name=<span class="string">&#x27;%s/task%d/multiply&#x27;</span> % (<span class="variable language_">self</span>.name, task_id))</span><br><span class="line">            <span class="comment"># reduce dim for tower input</span></span><br><span class="line">            task_input = tf.reduce_sum(task_input, axis=<span class="number">1</span>, name=<span class="string">&#x27;%s/task%d/output&#x27;</span> % (<span class="variable language_">self</span>.name, task_id))</span><br><span class="line">            experts_output_list.append(task_input)</span><br><span class="line">        <span class="keyword">return</span> experts_output_list</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">mmoe</span>(<span class="title class_ inherited__">multiTaskModel</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, params</span>):</span><br><span class="line">        <span class="variable language_">self</span>.expertNum = <span class="number">3</span></span><br><span class="line">        <span class="variable language_">self</span>.expertDims = [<span class="number">512</span>,<span class="number">256</span>,<span class="number">128</span>]</span><br><span class="line">        <span class="built_in">super</span>(mmoe, <span class="variable language_">self</span>).__init__(params)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build_graph</span>(<span class="params">self, features, mode, params</span>):</span><br><span class="line">        is_training = (mode == tf.estimator.ModeKeys.TRAIN)</span><br><span class="line">        user_feats = params[<span class="string">&#x27;feature_columns&#x27;</span>][<span class="string">&#x27;user_feats&#x27;</span>]</span><br><span class="line">        item_feats = params[<span class="string">&#x27;feature_columns&#x27;</span>][<span class="string">&#x27;item_feats&#x27;</span>]</span><br><span class="line">        partitioner = partitioned_variables.min_max_variable_partitioner(max_partitions=<span class="variable language_">self</span>.ps_num,</span><br><span class="line">                                                                         min_slice_size=<span class="number">8</span> * <span class="number">1024</span> * <span class="number">1024</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;embedding_scope&quot;</span>, values=features.values(), partitioner=partitioner) <span class="keyword">as</span> scope:</span><br><span class="line">            user_inputs = tf.feature_column.input_layer(features, user_feats)</span><br><span class="line">            item_inputs = tf.feature_column.input_layer(features, item_feats)</span><br><span class="line">            tf.logging.info(<span class="string">&quot;=&quot;</span> * <span class="number">8</span> + <span class="string">&quot;user_inputs shape is %s&quot;</span> % <span class="built_in">str</span>(user_inputs.shape) + <span class="string">&quot;=&quot;</span> * <span class="number">8</span>)</span><br><span class="line">            tf.logging.info(<span class="string">&quot;=&quot;</span> * <span class="number">8</span> + <span class="string">&quot;item_inputs shape is %s&quot;</span> % <span class="built_in">str</span>(item_inputs.shape) + <span class="string">&quot;=&quot;</span> * <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.task_type != [<span class="string">&quot;predict&quot;</span>] <span class="keyword">and</span> mode == tf.estimator.ModeKeys.PREDICT:</span><br><span class="line">                batchSize = tf.reshape(features[<span class="string">&quot;batchSize&quot;</span>], [])</span><br><span class="line">                user_inputs = tf.tile(user_inputs, [batchSize, <span class="number">1</span>])</span><br><span class="line">            embed_inputs = tf.concat([user_inputs, item_inputs], axis=<span class="number">1</span>)</span><br><span class="line">            tf.logging.info(<span class="string">&quot;=&quot;</span> * <span class="number">8</span> + <span class="string">&quot;embed_inputs shape is %s&quot;</span> % <span class="built_in">str</span>(embed_inputs.shape) + <span class="string">&quot;=&quot;</span> * <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">        nn_partitioner = partitioned_variables.min_max_variable_partitioner(max_partitions=<span class="variable language_">self</span>.ps_num,</span><br><span class="line">                                                                            min_slice_size=<span class="number">1</span> * <span class="number">64</span> * <span class="number">1024</span>)</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">&quot;nn_scope&quot;</span>, partitioner=nn_partitioner) <span class="keyword">as</span> nn_scope:</span><br><span class="line">            expertsNet = experts(<span class="variable language_">self</span>.expertNum, <span class="variable language_">self</span>.lable_size, dnn_dims = <span class="variable language_">self</span>.expertDims)</span><br><span class="line">            task_input_list = expertsNet(embed_inputs, is_training=is_training)</span><br><span class="line">            tower_outputs = &#123;&#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.lable_size):</span><br><span class="line">                tower_dnn = dnn(task_input_list[i], <span class="variable language_">self</span>.dnnDims, name=<span class="string">&quot;tower_%d&quot;</span> % i, is_training=is_training)</span><br><span class="line">                tower_output = tf.layers.dense(inputs=tower_dnn, units=<span class="number">1</span>, name=<span class="string">&#x27;tower_output_%d&#x27;</span> % i,</span><br><span class="line">                                               activation=tf.nn.sigmoid)</span><br><span class="line">                tower_outputs[<span class="string">&#x27;tower_output_%d&#x27;</span> % i] = tower_output</span><br><span class="line">            <span class="variable language_">self</span>.ctr_pred = tf.reshape(tower_outputs[<span class="string">&quot;tower_output_0&quot;</span>], [-<span class="number">1</span>], name = <span class="string">&quot;ctr&quot;</span>)</span><br><span class="line">            <span class="variable language_">self</span>.cvr_pred = tf.reshape(tower_outputs[<span class="string">&quot;tower_output_1&quot;</span>], [-<span class="number">1</span>], name = <span class="string">&quot;cvr&quot;</span>)</span><br></pre></td></tr></table></figure><p><strong>参考文献</strong><br><a href="https://zhuanlan.zhihu.com/p/145288000">多任务学习之MMOE模型</a><br><a href="https://mp.weixin.qq.com/s/EuJ2BOdMqR0zyRtUcdn0kA">多任务学习模型详解：Multi-gate Mixture-of-Experts（MMoE ，Google，KDD2018）</a></p><hr>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-引言&quot;&gt;&lt;a href=&quot;#1-引言&quot; class=&quot;headerlink&quot; title=&quot;1 引言&quot;&gt;&lt;/a&gt;1 引言&lt;/h2&gt;&lt;p&gt;本文介绍的&lt;code&gt;MMOE&lt;/code&gt;模型全称是&lt;code&gt;Multi-gate Mixture-of-Experts&lt;/code&gt;，来自 Google 在 2018 年 KDD 上发表的论文&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/3219819.3220007&quot;&gt;Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts&lt;/a&gt;。核心目标是改善多任务学习在任务之间不太相关的时候效果不好的问题。下面有两个相关学习资源：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;视频简介的&lt;a href=&quot;https://www.youtube.com/watch?v=Dweg47Tswxw&quot;&gt;youtube地址&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;一个用keras框架实现的&lt;a href=&quot;https://github.com/drawbridge/keras-mmoe&quot;&gt;开源地址&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;2-动机&quot;&gt;&lt;a href=&quot;#2-动机&quot; class=&quot;headerlink&quot; title=&quot;2 动机&quot;&gt;&lt;/a&gt;2 动机&lt;/h2&gt;&lt;p&gt;多任务学习一般是为了在训练多个相关任务的时候可以使得它们之间能够共享信息，提高学习效率和模型的泛化能力。但实际应用中往往难以如此理想化，因为&lt;strong&gt;任务之间往往相关性不够强&lt;/strong&gt;，这时候就容易产生以下两个问题：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;负迁移（negative transfer）&lt;/code&gt;：即网络表现变差&lt;br&gt;&lt;code&gt;跷跷板现象（seesaw phenomenon）&lt;/code&gt;：也就是个别任务相对于独立训练获得提升，但其他任务效果下降。&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="精排模型" scheme="https://www.xiemingzhao.com/categories/%E7%B2%BE%E6%8E%92%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="算法总结" scheme="https://www.xiemingzhao.com/categories/%E7%B2%BE%E6%8E%92%E6%A8%A1%E5%9E%8B/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="MMOE" scheme="https://www.xiemingzhao.com/tags/MMOE/"/>
    
    <category term="精排" scheme="https://www.xiemingzhao.com/tags/%E7%B2%BE%E6%8E%92/"/>
    
  </entry>
  
  <entry>
    <title>深度学习的常用损失函数</title>
    <link href="https://www.xiemingzhao.com/posts/popularlossfuncs.html"/>
    <id>https://www.xiemingzhao.com/posts/popularlossfuncs.html</id>
    <published>2021-04-22T16:00:00.000Z</published>
    <updated>2025-04-01T16:44:34.707Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>在深度学习中，<code>损失函数</code>（Loss Function）至关重要，它决定着深度模型的训练学习的方式，其设计的恰当与否，往往会影响到最终模型的有效性。<br>虽然在很多通用型任务上，业内逐渐形成使用惯例，（如点击率建模，默认都用对数损失，logloss），但对损失函数的认识越清楚，会有助于算法工程师在面临新任务时，在模型设计上事半功倍。</p><h2 id="2-常用损失函数"><a href="#2-常用损失函数" class="headerlink" title="2 常用损失函数"></a>2 常用损失函数</h2><blockquote><p>损失函数的任务是：针对一个样本，度量模型的预估值 logit 即$\hat y$和对应真实 Label 即$y$之间的差异。</p></blockquote><p>不同的损失函数有不同的含义，主要是模型学习逼近样本分布的方式。所以它是一个非负实值函数，主要<strong>特点为：恒非负；误差越小，函数值越小；收敛快。</strong></p><span id="more"></span><p>基于<strong>距离度量</strong>的损失函数</p><ul><li>均方差损失函数（MSE）；</li><li>L2 损失函数；</li><li>L1 损失函数；</li><li>Smooth L1损失函数；</li><li>huber 损失函数；</li></ul><p>基于<strong>概率分布度量</strong>的损失函数</p><ul><li>Logloss；</li><li>KL 散度函数（相对熵）；</li><li>Cross Entropy 损失；</li><li>Softmax 损失函数；</li><li>Focal loss。</li></ul><h3 id="2-1-均方差损失函数（MSE）"><a href="#2-1-均方差损失函数（MSE）" class="headerlink" title="2.1 均方差损失函数（MSE）"></a>2.1 均方差损失函数（MSE）</h3><script type="math/tex; mode=display">MSE = \frac{1}{n} \sum_{i=1}^n (y_i - p_i)^2</script><p>其中，n 是样本量，$y_i$是第 i 样本的真实 label，$p_i$是模型的预测结果。</p><p>该损失函数一般是用在回归问题中，用于度量样本点到回归曲线的距离。<strong>它对离群点比较敏感，所以它不适合离群点较多的数据集。</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">MSE_Loss</span>(<span class="params">y_true:<span class="built_in">list</span>,y_pred:<span class="built_in">list</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    y_pred:list，代表模型预测的一组数据</span></span><br><span class="line"><span class="string">    y_true:list，代表真实样本对应的一组数据</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(y_pred)==<span class="built_in">len</span>(y_true)</span><br><span class="line">    y_pred=np.array(y_pred)</span><br><span class="line">    y_true=np.array(y_true)</span><br><span class="line">    loss=np.<span class="built_in">sum</span>(np.square(y_pred - y_true)) / <span class="built_in">len</span>(y_pred)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure></p><h3 id="2-2-L2-损失函数"><a href="#2-2-L2-损失函数" class="headerlink" title="2.2 L2 损失函数"></a>2.2 L2 损失函数</h3><script type="math/tex; mode=display">L2 = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - p_i)^2}</script><p>其中，n 是样本量，$y_i$是第 i 样本的真实 label，$p_i$是模型的预测结果。</p><p>L2 函数，即<code>最小平方误差</code>（(Least Square Error(LSE))，也叫作<code>欧氏距离</code>。其在独立、同分布的高斯噪声情况下，它能提供最大似然估计，所以常用在回归、模式识别、图像任务中。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">L2_Loss</span>(<span class="params">y_true:<span class="built_in">list</span>,y_pred:<span class="built_in">list</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    y_pred:list，代表模型预测的一组数据</span></span><br><span class="line"><span class="string">    y_true:list，代表真实样本对应的一组数据</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(y_pred)==<span class="built_in">len</span>(y_true)</span><br><span class="line">    y_pred=np.array(y_pred)</span><br><span class="line">    y_true=np.array(y_true)</span><br><span class="line">    loss=np.sqrt(np.<span class="built_in">sum</span>(np.square(y_pred - y_true)) / <span class="built_in">len</span>(y_pred))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure></p><h3 id="2-3-L1-损失函数"><a href="#2-3-L1-损失函数" class="headerlink" title="2.3 L1 损失函数"></a>2.3 L1 损失函数</h3><script type="math/tex; mode=display">L1 = \sum_{i=1}^n |y_i - p_i|</script><p>其中，n 是样本量，$y_i$是第 i 样本的真实 label，$p_i$是模型的预测结果。</p><p>L1 Loss 即<code>最小绝对误差</code>(Least Abosulote Error(LAE))，又称为<code>曼哈顿距离</code>，表示残差的绝对值之和。</p><p>但它有2个缺点：</p><ul><li>残差为零处却不可导；</li><li>梯度始终相同。</li></ul><p>更适用于有较多离群点的数据集，但由于上述的缺点使得不利于模型的收敛。为了缓解此问题，实际中如果使用的话，往往使用后文介绍的优化版 Smooth L1 Loss。</p><p>这里顺便提一个与其非常接近的 MAE，即平均绝对误差：</p><script type="math/tex; mode=display">MAE = \frac{1}{n} \sum_{i=1}^n |y_i - p_i|</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">L1Loss</span>(<span class="params">y_true:<span class="built_in">list</span>,y_pred:<span class="built_in">list</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    y_pred:list，代表模型预测的一组数据</span></span><br><span class="line"><span class="string">    y_true:list，代表真实样本对应的一组数据</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(y_pred)==<span class="built_in">len</span>(y_true)</span><br><span class="line">    y_pred=np.array(y_pred)</span><br><span class="line">    y_true=np.array(y_true)</span><br><span class="line">    loss=np.<span class="built_in">sum</span>(np.<span class="built_in">abs</span>(y_pred - y_true)) / <span class="built_in">len</span>(y_pred)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h3 id="2-4-Smooth-L1损失函数"><a href="#2-4-Smooth-L1损失函数" class="headerlink" title="2.4 Smooth L1损失函数"></a>2.4 Smooth L1损失函数</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Smooth L1 =</span><br><span class="line">\begin&#123;cases&#125;</span><br><span class="line">\frac&#123;1&#125;&#123;2&#125;(Y-f(x))^2 &amp; \quad\text&#123;|Y-f(x)|&lt;1&#125; \\</span><br><span class="line">|Y-f(x)|-\frac&#123;1&#125;&#123;2&#125; &amp; \quad\text&#123;|Y-f(x)|&gt;=1&#125; &amp; </span><br><span class="line">\end&#123;cases&#125;</span><br></pre></td></tr></table></figure><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/lossfunc0.png" alt="lossfunc0"></p><p>该函数是由 Girshick R 在 Fast R-CNN 中提出的，<strong>主要用在目标检测中防止梯度爆炸</strong>。其实际上是一个分段函数：</p><ul><li>在[-1,1]，等价L2损失，解决了L1的不光滑问题；</li><li>在[-1,1]区间外，是L1损失，解决了L2离群点梯度易爆炸的问题。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Smooth_L1_Loss</span>(<span class="params">y_true_pred,y_true</span>):</span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(y_true_pred)==<span class="built_in">len</span>(y_true)</span><br><span class="line">    loss=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i_y_true_pred,i_y_true <span class="keyword">in</span> <span class="built_in">zip</span>(y_true_pred,y_true):</span><br><span class="line">        tmp = <span class="built_in">abs</span>(i_y_true-i_y_true_pred)</span><br><span class="line">        <span class="keyword">if</span> tmp&lt;<span class="number">1</span>:</span><br><span class="line">            loss+=<span class="number">0.5</span>*(tmp**<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            loss+=tmp-<span class="number">0.5</span></span><br><span class="line">     <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h3 id="2-5-huber-损失函数"><a href="#2-5-huber-损失函数" class="headerlink" title="2.5 huber 损失函数"></a>2.5 huber 损失函数</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">\left.\left.\left. Huber=\left\&#123;</span><br><span class="line">\begin&#123;array&#125;</span><br><span class="line">&#123;ll&#125;\frac&#123;1&#125;&#123;2&#125;(Y-f(x))^2 &amp; |\mathrm&#123;Y-f(x)&#125;|&lt;=\delta \\</span><br><span class="line">\delta|Y-f(x)|-\frac&#123;1&#125;&#123;2&#125;\delta^2 &amp; |\mathrm&#123;Y-f(x)&#125;|&gt;\delta</span><br><span class="line">\end&#123;array&#125;\right.\right.\right.\right.</span><br></pre></td></tr></table></figure><p><code>Huber</code> 损失是 MSE 和 MAE 的结合，又称作 Smooth Mean Absolute Error Loss。</p><p>它克服了L1和L2的缺点：</p><ul><li>不仅使损失函数具有连续的导数；</li><li>而且利用MSE梯度随误差减小的特性，可取得更精确的最小值。</li></ul><p>但是，它有自己的缺点，不仅引入了额外的参数，而且选择合适的参数比较困难。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">huber_loss</span>(<span class="params">y_pred,y_true,delta=<span class="number">1.0</span></span>):</span><br><span class="line">   <span class="keyword">assert</span> <span class="built_in">len</span>(y_pred)==<span class="built_in">len</span>(y_true)</span><br><span class="line">   loss=<span class="number">0</span></span><br><span class="line">   <span class="keyword">for</span> i_y_pred,i_y_true <span class="keyword">in</span> <span class="built_in">zip</span>(y_pred,y_true):</span><br><span class="line">       tmp = <span class="built_in">abs</span>(i_y_true-i_y_pred)</span><br><span class="line">       <span class="keyword">if</span> tmp&lt;=delta:</span><br><span class="line">           loss+=<span class="number">0.5</span>*(tmp**<span class="number">2</span>)</span><br><span class="line">       <span class="keyword">else</span>:</span><br><span class="line">           loss+=tmp*delta-<span class="number">0.5</span>*delta**<span class="number">2</span></span><br><span class="line">   <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h3 id="2-6-KL-散度函数（相对熵）"><a href="#2-6-KL-散度函数（相对熵）" class="headerlink" title="2.6 KL 散度函数（相对熵）"></a>2.6 KL 散度函数（相对熵）</h3><script type="math/tex; mode=display">KL = \sum_{i=1}^n y_i \times log(\frac{y_i}{p_i})</script><p>其中，n 是样本量，$y_i$是第 i 样本的真实 label，$p_i$是模型的预测结果。</p><p><code>KL散度</code>（ Kullback-Leibler divergence）也被称为<code>相对熵</code>，是一种<strong>非对称度量方法，即A、B两个分布，A对比B计算和B对A计算结果不一样。</strong></p><p>相对熵是恒大于等于0的，当且仅当两分布相同时，相对熵等于0。KL散度可以用于比较文本标签或图像的相似性。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">kl_loss</span>(<span class="params">y_true:<span class="built_in">list</span>,y_pred:<span class="built_in">list</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    y_true,y_pred，分别是两个概率分布</span></span><br><span class="line"><span class="string">    比如：y_true=[0.1,0.2,0.8]</span></span><br><span class="line"><span class="string">        y_pred=[0.3,0.3,0.4]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(y_true)==<span class="built_in">len</span>(y_pred)</span><br><span class="line">    KL=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> y,fx <span class="keyword">in</span> <span class="built_in">zip</span>(y_true,y_pred):</span><br><span class="line">        KL+=y*np.log(y/fx)</span><br><span class="line">    <span class="keyword">return</span> KL</span><br></pre></td></tr></table></figure></p><h3 id="2-7-Cross-Entropy-损失"><a href="#2-7-Cross-Entropy-损失" class="headerlink" title="2.7 Cross Entropy 损失"></a>2.7 Cross Entropy 损失</h3><script type="math/tex; mode=display">CE = -\frac{1}{n} \sum_{i=1}^n \sum_{j = 1}^m y_{ij} log(p_{ij})</script><p>其中，n 是样本数，m 是类别数，$y<em>{ij}$是样本 i 所属类别 j 的示性变量，即属于时位1，否则为0，$p</em>{ij}$表示预测的样本 i 属于类别 j 的概率。</p><p><code>交叉熵</code>是信息论中的一个概念，最初用于估算平均编码长度，在深度学习中往往用于评估当前训练得到的概率分布与真实分布的差异情况。</p><p>为了使神经网络的每一层输出从线性组合转为<strong>非线性逼近，以提高模型的预测精度</strong>，一般配合softmax激活函数，在多分类问题中常常被使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">CrossEntropy_loss</span>(<span class="params">y_true:<span class="built_in">list</span>,y_pred:<span class="built_in">list</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    y_true,y_pred，分别是两个概率分布list</span></span><br><span class="line"><span class="string">    比如：y_true=[[0.1,0.9],[0.2,0.8],[0.4,0.6]]</span></span><br><span class="line"><span class="string">         y_pred=[[0.3,0.7],[0.1,0.9],[0.4,0.6]]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(y_true)==<span class="built_in">len</span>(y_pred)</span><br><span class="line">    cate_size = <span class="built_in">len</span>(y_true[<span class="number">0</span>])</span><br><span class="line">    loss=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> y,fx <span class="keyword">in</span> <span class="built_in">zip</span>(y_true,y_pred):</span><br><span class="line">        loss+=-<span class="built_in">sum</span>([y[i] * np.log(fx[i]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(cate_size)]</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h3 id="2-8-LogLoss-函数"><a href="#2-8-LogLoss-函数" class="headerlink" title="2.8 LogLoss 函数"></a>2.8 LogLoss 函数</h3><script type="math/tex; mode=display">LogLoss = -\frac{1}{n}\sum_{i=1}^n (y_i log(p_i) + (1 - y_i) log(1 - p_i))</script><p>其中，n 是样本量，$y_i$是第 i 样本的真实 label，$p_i$是模型的预测结果。</p><p><code>对数损失</code>（logarithm loss）也被称为<code>对数似然损失</code>，实际上<strong>是交叉熵损失在二分类任务下的特例</strong>。其假设样本服从伯努利分布，利用极大似然估计的思想求得极值，它常作为二分类问题的损失函数，一般结合 sigmoid 输出函数使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Log_loss</span>(<span class="params">y_true:<span class="built_in">list</span>,y_pred:<span class="built_in">list</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    y_true,y_pred，分别是两个概率分布</span></span><br><span class="line"><span class="string">    比如：y_true=[0.1,0.2,0.8]</span></span><br><span class="line"><span class="string">         y_pred=[0.3,0.3,0.4]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(y_true)==<span class="built_in">len</span>(y_pred)</span><br><span class="line">    loss=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> y,fx <span class="keyword">in</span> <span class="built_in">zip</span>(y_true,y_pred):</span><br><span class="line">        loss+=-y * np.log(fx)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h3 id="2-9-Focal-loss"><a href="#2-9-Focal-loss" class="headerlink" title="2.9 Focal loss"></a>2.9 Focal loss</h3><script type="math/tex; mode=display">FL = -\frac{1}{n}\sum_{i=1}^n (\alpha (1-p_i)^{\gamma} y_i log(p_i) + (1 - \alpha) p_i^{\gamma} (1 - y_i) log(1 - p_i))</script><p>其中，n 是样本量，$y_i$是第 i 样本的真实 label，$p_i$是模型的预测结果。</p><p><code>Focal Loss</code> 的引入主要是为了<strong>解决难易样本不均衡的问题，注意有区别于正负样本不均衡的问题</strong>。易分样本虽然损失很低，但是数量太多，对模型的效果提升贡献很小，模型应该重点关注那些难分样本.<br>因此需要把置信度高的损失再降低一些性质：</p><ul><li>当样本分类错误时，$p_t$趋于0，调变因子趋于1，使得损失函数几乎不受影响;</li><li>如果正确分类，$p_t$将趋于1，调变因子将趋向于0，使得损耗非常接近于0，从而降低了该特定示例的权重。</li></ul><p>如下图，聚焦参数（γ）平滑地调整易于分类的样本向下加权的速率。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/lossfunc1.png" alt="lossfunc1"></p><p>其中：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">p_&#123;\mathbf&#123;t&#125;&#125;=</span><br><span class="line">\begin&#123;cases&#125;</span><br><span class="line">p &amp; \mathrm&#123;if~&#125;y=1 \\</span><br><span class="line">1-p &amp; \text&#123;otherwise&#125; &amp; &amp; </span><br><span class="line">\end&#123;cases&#125;</span><br></pre></td></tr></table></figure></p><h2 id="3-交叉熵的渊源"><a href="#3-交叉熵的渊源" class="headerlink" title="3 交叉熵的渊源"></a>3 交叉熵的渊源</h2><h3 id="3-1-交叉熵与信息论"><a href="#3-1-交叉熵与信息论" class="headerlink" title="3.1 交叉熵与信息论"></a>3.1 交叉熵与信息论</h3><p>交叉熵（Cross Entropy）出自 Shannon <code>信息论</code>，主要<strong>用于度量两个概率分布间的差异性信息。</strong></p><p>假设 p 表示真实分布，q 表示预估分布，那么 $H(p,q)$ 就称为交叉熵：</p><script type="math/tex; mode=display">H(p,q) = \sum_i p_i \cdot log \frac{1}{q_i} = -\sum_i p_ilog(q_i)</script><p>要追溯它的源头，需要再回到信息论中，<code>信息量</code>的表示方式：$I(x) = -log(p(x))$<br>$x$表示一个事件，$p(x)$表示事件 x 发生的概率，$I(x)$则表示信息量。<br>根据$log$函数的性质，<strong>事件发生概率越小时，它一旦发生后的信息量就越大。</strong></p><p>假设随机变量 x，有两个独立的概率分布 $P(x)$ 和 $Q(x)$，怎么度量两个分布的差异呢？</p><p>使用 <code>KL 散度</code>（Kullback-Leibler (KL) divergence），又称<code>相对熵</code>。</p><p>KL散度的计算公式：</p><script type="math/tex; mode=display">D_{KL}(q||p) = \sum_{i = 1}^n q(x_i) log \frac{q(x_i)}{p(x_i)}</script><p>其中，n为事件的所有可能性种类，D 的值越小，表示 Q 分布和 P 分布越接近。</p><p>我们简单做一下分解：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;aligned&#125;</span><br><span class="line"> &amp; \mathrm&#123;&#125; \\</span><br><span class="line">D_&#123;KL&#125;(q||p) &amp; =\sum_&#123;i=1&#125;^n q(x_&#123;i&#125;)\log q(x_&#123;i&#125;)-\sum_&#123;i=1&#125;^n q(x_&#123;i&#125;)\log p(x_&#123;i&#125;) \\</span><br><span class="line"> &amp; =-H(q(x))+H(q,p)</span><br><span class="line">\end&#123;aligned&#125;</span><br></pre></td></tr></table></figure></p><p>到这里，你可能已经发现他们之间的关联了。</p><p>在深度学习中的分类任务，我们<strong>想要度量模型预估是否准确，就可以通过度量样本的真实分布（Label）与预估分布（Predict）之间的距离来判断</strong>。我们令：</p><ul><li>真实分布（Label）：Q(x)</li><li>预估分布（Predict）：P(x)</li></ul><p>真实分布往往就是训练样本，是给定不变的，所以在$D_{KL}(q||p)$中需要关注和优化的就只有分解后的第二项$H(q,p)$，即<code>交叉熵</code>。</p><p>如果我们令 n 表示样本数，m 表示分类数，$y<em>{ij}$是样本 label，$p</em>{ij}$表示预测的概率。批量的交叉熵便是：</p><script type="math/tex; mode=display">CE = -\frac{1}{n} \sum_{i=1}^n \sum_{j = 1}^m y_{ij} log(p_{ij})</script><p>如果在<strong>二分类任务中</strong>，那么 $y_{ij}$ 就只有 0 和 1 两类，所以交叉熵可以简化为：</p><script type="math/tex; mode=display">CE = -\frac{1}{n} \sum_{i=1}^n (y_i log(p_{i}) + (1 - y_i)log(1 - p_{i}))</script><p>相信你已经看出来了，这就是 <code>LogLoss</code>。</p><p>我们以 Logloss 为例，接着用<strong>极大值点的方式（导数为0）来求解极大似然估计</strong>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;aligned&#125;</span><br><span class="line"> &amp; \mathrm&#123;&#125; \\</span><br><span class="line">\sum_&#123;i=1&#125;^n (y_i \frac&#123;1&#125;&#123;p_&#123;i&#125;&#125; + (1 - y_i)\frac&#123;1&#125;&#123;p_&#123;i&#125; - 1&#125;) = 0 \\</span><br><span class="line">\sum_&#123;i=1&#125;^n (p_i - y_i) = 0 \\</span><br><span class="line">\bar p = \frac&#123;1&#125;&#123;n&#125; \sum_&#123;i=1&#125;^n y_i</span><br><span class="line">\end&#123;aligned&#125;</span><br></pre></td></tr></table></figure><p>这就解释了实际应用中，对于<strong>二分类任务（比如ctr预估）模型的预估期望是等于 Label 的均值的</strong>，所以我们往往用 <code>pcoc</code> 来判断模型在局部样本上是高估还是低估。</p><h3 id="3-2-交叉熵与极大似然"><a href="#3-2-交叉熵与极大似然" class="headerlink" title="3.2 交叉熵与极大似然"></a>3.2 交叉熵与极大似然</h3><p>区别：</p><ul><li>交叉熵是度量分布差距的大小，越大代表越不相近；</li><li>似然函数是度量分布一样的概率，越大代表越相近。</li></ul><p><strong>实际上，最小化交叉熵函数的本质就是对数似然函数的最大化。</strong></p><blockquote><p>最大似然估计中采样需满足假设：独立同分布实验。</p></blockquote><p>我们假设独立采样 n 个样本，样本分布为 q，预估分布为 p，那么就有：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;gathered&#125;</span><br><span class="line">L=\frac&#123;1&#125;&#123;n&#125;\sum_&#123;i=1&#125;^&#123;n&#125;log\prod_&#123;j=1&#125;^&#123;m&#125;p_&#123;ij&#125;^&#123;q_&#123;ij&#125;&#125;=\frac&#123;1&#125;&#123;n&#125;\sum_&#123;i=1&#125;^&#123;n&#125;log(p_&#123;1&#125;^&#123;q_&#123;1&#125;&#125;.p_&#123;2&#125;^&#123;q_&#123;2&#125;&#125;...p_&#123;m&#125;^&#123;q_&#123;m&#125;&#125;) \\</span><br><span class="line">=\frac&#123;1&#125;&#123;n&#125;\sum_&#123;i=1&#125;^n(q_1log(p_1)+q_2log(p_2)+...+q_m log(p_m)) \\</span><br><span class="line">=\frac&#123;1&#125;&#123;n&#125;\sum_&#123;i=1&#125;^n\sum_&#123;j=1&#125;^m q_ilog(p_i)</span><br><span class="line">\end&#123;gathered&#125;</span><br></pre></td></tr></table></figure></p><p>可以发现，实际上就是交叉熵的绝对值。</p><h3 id="3-3-交叉熵损失的优势"><a href="#3-3-交叉熵损失的优势" class="headerlink" title="3.3 交叉熵损失的优势"></a>3.3 交叉熵损失的优势</h3><blockquote><p>为什么不能使用均方差做为分类问题的损失函数？</p></blockquote><p><strong>均方差损失适合回归</strong>：与激活函数叠加是个凸函数，即可以得到最优解。</p><p><strong>均方差损失不适合分类：</strong></p><ul><li>与激活函数（Sigmoid/Softmax）叠加不是凸函数，就很难得到最优解。</li><li>求导结果复杂，运算量比较大。</li></ul><p><strong>交叉熵适合分类：</strong></p><ul><li>可以保证区间内单调；</li><li>梯度计算简单，纯减法。</li></ul><p>正如上面所述，交叉上损失在梯度计算上也有优势。我们以常用的二分类任务为例，单条样本交叉熵公式为：</p><script type="math/tex; mode=display">C = -(y log(p) + (1 - y)log(1-p))</script><p>其中，y 是 label，p 是深度模型输出，如果结合输出层和激活函数，就有：</p><script type="math/tex; mode=display">p = \sigma{(z)} , z = w x + b</script><p>我们就可以分别推导出<strong>参数$w,b$的梯度，如下所示，可以发现非常简洁，只与$p - y$有关，即预估误差越大，梯度更新越快。</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;gathered&#125;</span><br><span class="line">\frac&#123;\partial C&#125;&#123;\partial p&#125; &amp;=&amp; -(y \frac&#123;1&#125;&#123;p&#125; + \frac&#123;y - 1&#125;&#123;1 - p&#125;)\\</span><br><span class="line">&amp;=&amp; -(y \frac&#123;1&#125;&#123;p(1 - p)&#125; - \frac&#123;1&#125;&#123;1 - p&#125;)\\</span><br><span class="line">\frac&#123;\partial C&#125;&#123;\partial z&#125; &amp;=&amp; \frac&#123;\partial C&#125;&#123;\partial p&#125; \frac&#123;\partial p&#125;&#123;\partial z&#125; \\</span><br><span class="line">&amp;=&amp; -(y \frac&#123;1&#125;&#123;p(1 - p)&#125; - \frac&#123;1&#125;&#123;1 - p&#125;) \cdot \sigma&#x27;(z) \\</span><br><span class="line">&amp;=&amp; -(y \frac&#123;1&#125;&#123;p(1 - p)&#125; - \frac&#123;1&#125;&#123;1 - p&#125;) \cdot \sigma(z)(1 - \sigma(z)) \\</span><br><span class="line">&amp;=&amp; p - y \\</span><br><span class="line"></span><br><span class="line">\frac&#123;\partial C&#125;&#123;\partial w&#125; &amp;=&amp; \frac&#123;\partial C&#125;&#123;\partial z&#125; \frac&#123;\partial z&#125;&#123;\partial w&#125; = (p - y)x \\</span><br><span class="line">\frac&#123;\partial C&#125;&#123;\partial b&#125; &amp;=&amp; \frac&#123;\partial C&#125;&#123;\partial z&#125; \frac&#123;\partial z&#125;&#123;\partial b&#125; = (p - y) </span><br><span class="line">\end&#123;gathered&#125;</span><br></pre></td></tr></table></figure><p>同样的，如果是多分类，使用 softmax 激活函数 + 交叉熵损失，也有类似的性质。虽然推导起来复杂一些，但结果也是只和$p - y$有关，这里不再赘述。</p><p><strong>参考文章：</strong><br><a href="https://blog.csdn.net/light169/article/details/124602481">深度学习之损失函数</a><br><a href="https://cloud.tencent.com/developer/article/1950150">六个深度学习常用损失函数总览</a><br><a href="https://www.nowcoder.com/discuss/353148846177984512">深度学习——损失函数</a><br><a href="https://mp.weixin.qq.com/s?subscene=23&amp;__biz=MzAxOTQ2NzUxOQ==&amp;mid=2651912913&amp;idx=1&amp;sn=68f62cb548b35ec93f18027458352434&amp;chksm=8022c741b7554e57616ac8dcd80a11acd7a8a6ada181f4584c602c9f242815f5863ed96b4529&amp;scene=7&amp;key=9c1744d1bffeab4b0c8d19d08eff803bc5bf556e7dad290931316859107869eae11c8d2ef08d1c445c32f9564b88b15a7ddefa9ecba47e9f30595d3194cb93202f018216f272734d0502965e5172c528b84d6f7ee4a2970563bd1594a1ee84cb78bedc97a8d73b1f2c1c8b796374470272fd4c44a5a5badebefe79f797dd9006&amp;ascene=0&amp;uin=NzY2MzMyNDAx&amp;devicetype=Windows+10+x64&amp;version=62090538&amp;lang=zh_CN&amp;exportkey=AbDFP%2Fy90lQvupKLsin%2BqIY%3D&amp;pass_ticket=xXJHSvXALrdXBNLifG37ggdjbeqxXtSOQYVggNqQRRey1Vm1lBPWCAlvlwRUGwZW&amp;wx_header=0">监督学习中的损失函数及应用研究</a></p><hr>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-引言&quot;&gt;&lt;a href=&quot;#1-引言&quot; class=&quot;headerlink&quot; title=&quot;1 引言&quot;&gt;&lt;/a&gt;1 引言&lt;/h2&gt;&lt;p&gt;在深度学习中，&lt;code&gt;损失函数&lt;/code&gt;（Loss Function）至关重要，它决定着深度模型的训练学习的方式，其设计的恰当与否，往往会影响到最终模型的有效性。&lt;br&gt;虽然在很多通用型任务上，业内逐渐形成使用惯例，（如点击率建模，默认都用对数损失，logloss），但对损失函数的认识越清楚，会有助于算法工程师在面临新任务时，在模型设计上事半功倍。&lt;/p&gt;
&lt;h2 id=&quot;2-常用损失函数&quot;&gt;&lt;a href=&quot;#2-常用损失函数&quot; class=&quot;headerlink&quot; title=&quot;2 常用损失函数&quot;&gt;&lt;/a&gt;2 常用损失函数&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;损失函数的任务是：针对一个样本，度量模型的预估值 logit 即$&#92;hat y$和对应真实 Label 即$y$之间的差异。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;不同的损失函数有不同的含义，主要是模型学习逼近样本分布的方式。所以它是一个非负实值函数，主要&lt;strong&gt;特点为：恒非负；误差越小，函数值越小；收敛快。&lt;/strong&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="学习笔记" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="算法总结" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="损失函数" scheme="https://www.xiemingzhao.com/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    
    <category term="loss fuction" scheme="https://www.xiemingzhao.com/tags/loss-fuction/"/>
    
  </entry>
  
  <entry>
    <title>Batch Normalization 小记</title>
    <link href="https://www.xiemingzhao.com/posts/batchnormnotes.html"/>
    <id>https://www.xiemingzhao.com/posts/batchnormnotes.html</id>
    <published>2021-04-04T16:00:00.000Z</published>
    <updated>2025-04-01T16:37:00.621Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1 背景"></a>1 背景</h2><p>如果你是一个玩深度学习的算法工程师，那么相信你对批标准化（<code>Batch Normalization</code>）一定不陌生。在实际训练深度模型中，<strong>BN 往往用来加速模型收敛或者缓解梯度消失/爆炸的问题</strong>。笔者在实际使用过程中也有一定的收获和思考，收获是不同的使用姿势确实能够带来不一样的效果。思考就是，虽然大致知道BN的原理和公式，但是创建 BN 这个方法的出发点和一些边界问题的思考始终萦绕在周围。在此做一个汇总整理，旨在帮助和我一样有此困惑的朋友们。</p><h2 id="2-原理"><a href="#2-原理" class="headerlink" title="2 原理"></a>2 原理</h2><h3 id="2-1-优化原理"><a href="#2-1-优化原理" class="headerlink" title="2.1 优化原理"></a>2.1 优化原理</h3><p>训练网络深度的加深可能会带来梯度迭代上的<code>梯度消失</code>（Gradient Vanishing)或者<code>梯度爆炸</code>(Gradient Explore)问题。这两个问题的产生原理这里不做赘述，一般都是由于网络层数过深，梯度链式传导带来的结果。</p><span id="more"></span><p>网络训练过程中参数不断改变导致后续每一层输入的分布也发生变化，而学习的过程又要使每一层适应输入的分布，所以一般只能选取较低的学习率和较小值来初始化。这种分布的变化一般称之为<code>internal covariate shift</code>。</p><p><em><code>Convariate Shift</code>是指训练集的样本数据和目标样本集分布不一致时，训练得到的模型无法很好的<code>Generalization</code>。</em></p><p>在训练网络模型的时候经常会对输入做均值归0化，有的做白化，都是为了加速训练。<em>但是能够加速的原理是什么呢？</em></p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/batchnormnotes0.png" alt="batchnormnotes0"></p><p>如上图所示，假设初始的样本数据分布如图a所示。当我们进行模型拟合的时候，以单层网络$y = Wx+b$为例，由于参数初始化的时候都是0均值附近的很小值，所以拟合曲线一般都会过原点，如上图 b 红色虚线所示，想要达到收敛的情况就会比较慢。但是，如果将数据平移至原点附近，如图 c 所示，陷入可以加快拟合速度。更甚者对数据做去相关，如图 d 所示，样本间的区分度就会更高。</p><p>而做标准化的方式也有多种，<strong>效果比较好的是PCA，但是在复杂的网络中需要计算协方差矩阵、求逆等操作，计算量很大，此外，反向传播时，标准化操作不一定可导</strong>。这时候<code>Batch Normalization</code>的优势就体现了出来。</p><p><code>Bactch Normalization</code>是来标准化某些层或者所有层的输入，从而<strong>固定每层输入信息的均值和方差</strong>。一般就是要让数据具有0均值和单位方差:</p><script type="math/tex; mode=display">\hat x ^{(k)} = \frac{x^{(k)} - E[x^{(k)}]}{\sqrt {Var[x^{(k)}]}}</script><p><strong>公式中的均值和方差，用一个Batch的均值和方差作为对整个数据集均值和方差的估计。</strong></p><blockquote><p>但是，如果仅仅这么简单的做是有问题的。</p></blockquote><p>我们以下图常用的激活函数<code>sigmoid</code>为例，如果把数据限制到0均值单位方差，那么相当于只使用了激活函数中近似线性的部分，这显然会降低模型表达能力。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/batchnormnotes1.jpg" alt="batchnormnotes1"></p><p>所以，就可以在上述的基础上，增加一个平移和缩放的变换，用来保持模型的表达能力：</p><script type="math/tex; mode=display">y^{(k)} = \gamma^{(k)} \hat x^{(k)} + \beta ^{(k)}</script><h3 id="2-2-模型推理"><a href="#2-2-模型推理" class="headerlink" title="2.2 模型推理"></a>2.2 模型推理</h3><p>实际使用的时候，模型前向传输网络依然使用下列的公式进行数据的标准化：</p><script type="math/tex; mode=display">\hat x = \frac{x - E[x]}{\sqrt{Var[x]+\epsilon}}</script><p><strong>注意：这里的$E[x]$和$Var[x]$不同于训练时候的值，并不是当前batch的统计结果，而是针对整个数据集的统计值。</strong></p><blockquote><p>但是，怎么获取呢？</p></blockquote><ul><li>训练时，均值、方差分别是该批次内数据相应维度的均值与方差；</li><li>推理时，均值、方差是基于所有批次的期望计算所得：</li></ul><p>为了最后在模型 infer 过程中更加准确，需要<strong>记录每一个训练的Batch的均值和方差</strong>，其实就是一个<code>无偏估计</code>：</p><script type="math/tex; mode=display">E[x] \epsilon \gets E_{B}[u_B]</script><script type="math/tex; mode=display">Var[x] \epsilon \gets \frac{m}{m-1}E_B[\sigma^2_B]</script><p>大部分经验说应该把BN放在激活函数之前，这是因为：</p><ul><li>本身就是为了解决梯度消失/爆炸的问题；</li><li>$Wx+b$具有更加一致和非稀疏的分布。</li></ul><p><em>但是也有人做实验表明放在激活函数后面效果更好。</em></p><h2 id="3-tf实战"><a href="#3-tf实战" class="headerlink" title="3 tf实战"></a>3 tf实战</h2><p>介绍了那么多理论，那实际中如何在网络中使用BN层呢？这里将介绍一些对应的tensorflow的API以及使用的小Tips。</p><h3 id="3-1-BN-的-API"><a href="#3-1-BN-的-API" class="headerlink" title="3.1 BN 的 API"></a>3.1 BN 的 API</h3><p>首先Batch Normalization在TensorFlow中有三个接口调用 (不包括slim、Keras模块中的)，分别是：</p><ul><li><a href="https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization">tf.layers.batch_normalization</a></li><li><a href="https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization">tf.nn.batch_normalization</a></li><li><a href="https://www.tensorflow.org/api_docs/python/tf/compat/v1/layers/batch_normalization">tf.contrib.layers.batch_norm</a></li></ul><p><code>tf.layers.batch_normalization</code>和<code>tf.contrib.layers.batch_norm</code>可以用来构建待训练的神经网络模型，而<code>tf.nn.batch_normalization</code>一般只用来构建推理模型，原因是后者只定义了初始的网络结构，没有考虑训练和推理时候的参数更新问题。由于<code>tf.contrib</code>包的不稳定性，一般实际中使用最多的就是<code>tf.layers.batch_normalization</code>。</p><p>首先，看一下<code>tf.layers.batch_normalization</code>接口方法的定义：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">tf.layers.batch_normalization(</span><br><span class="line">    inputs,</span><br><span class="line">    axis=-<span class="number">1</span>,</span><br><span class="line">    momentum=<span class="number">0.99</span>,</span><br><span class="line">    epsilon=<span class="number">0.001</span>,</span><br><span class="line">    center=<span class="literal">True</span>,</span><br><span class="line">    scale=<span class="literal">True</span>,</span><br><span class="line">    beta_initializer=tf.zeros_initializer(),</span><br><span class="line">    gamma_initializer=tf.ones_initializer(),</span><br><span class="line">    moving_mean_initializer=tf.zeros_initializer(),</span><br><span class="line">    moving_variance_initializer=tf.ones_initializer(),</span><br><span class="line">    beta_regularizer=<span class="literal">None</span>,</span><br><span class="line">    gamma_regularizer=<span class="literal">None</span>,</span><br><span class="line">    beta_constraint=<span class="literal">None</span>,</span><br><span class="line">    gamma_constraint=<span class="literal">None</span>,</span><br><span class="line">    training=<span class="literal">False</span>,</span><br><span class="line">    trainable=<span class="literal">True</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    reuse=<span class="literal">None</span>,</span><br><span class="line">    renorm=<span class="literal">False</span>,</span><br><span class="line">    renorm_clipping=<span class="literal">None</span>,</span><br><span class="line">    renorm_momentum=<span class="number">0.99</span>,</span><br><span class="line">    fused=<span class="literal">None</span>,</span><br><span class="line">    virtual_batch_size=<span class="literal">None</span>,</span><br><span class="line">    adjustment=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><p>其中，有几个主要参数需要了解一下：</p><ul><li><code>axis</code>的值取决于按照<code>input</code>的哪一个维度进行BN，例如输入为<code>channel_last format</code>，即<code>[batch_size, height, width, channel]</code>，则<code>axis</code>应该设定为4，如果为<code>channel_first format</code>，则<code>axis</code>应该设定为1.</li><li><code>momentum</code>的值用在训练时，滑动平均的方式计算滑动平均值<code>moving_mean</code>和滑动方差<code>moving_variance</code>。</li><li><code>center</code>为<code>True</code>时，添加位移因子<code>beta</code>到该BN层，否则不添加。添加<code>beta</code>是对BN层的变换加入位移操作。<strong>注意，<code>beta</code>一般设定为可训练参数，即trainable=True</strong>。</li><li><code>scale</code>为True是，添加缩放因子<code>gamma</code>到该BN层，否则不添加。添加gamma是对BN层的变化加入缩放操作。<strong>注意，gamma一般设定为可训练参数，即trainable=True</strong>。</li><li><code>training</code>表示模型当前的模式，如果为True，则模型在训练模式，否则为推理模式。<strong>要非常注意这个模式的设定!!!</strong>，这个参数默认值为False。如果在训练时采用了默认值False，则滑动均值<code>moving_mean</code>和滑动方差<code>moving_variance</code>都不会根据当前batch的数据更新，这就意味着<strong>在推理模式下，均值和方差都是其初始值，因为这两个值并没有在训练迭代过程中滑动更新</strong>。</li></ul><h3 id="3-2-BN-的-code"><a href="#3-2-BN-的-code" class="headerlink" title="3.2 BN 的 code"></a>3.2 BN 的 code</h3><p>TensorFlow中模型训练时的梯度计算、参数优化等<code>train_op</code>并没有依赖滑动均值<code>moving_mean</code>和滑动方差<code>moving_variance</code>，则moving_mean和moving_variance不会自动更新，只能在<code>tf.GraphKeys.GLOBAL_VARIABLES</code>中，所以<strong>必须加入负责更新这些参数的<code>update_ops</code>到依赖中</strong>，且应该在执行前向计算结束后、后向计算开始前执行update_ops，所以添加依赖的位置不能出错。在前文提到的$\beta$和$\gamma$是可训练变量，存放于<code>tf.GraphKeys.TRAINABLE_VARIABLES</code>。实际中，只需要在构建模型代码中，添加完所有BN层之后获取update_ops就不会出错!!！这是TensorFlow的图计算模式造成的编程影响，在其他深度学习框架中可能会有差别。</p><p><strong>训练</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x_norm = tf.layers.batch_normalization(x, training=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)</span><br><span class="line">train_op = optimizer.minimize(loss)</span><br><span class="line">train_op = tf.group([train_op, update_ops])</span><br></pre></td></tr></table></figure></p><p><strong>模型保存</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line">saver = tf.train.Saver(tf.global_variables())</span><br><span class="line">saver.save(sess, <span class="string">&quot;your_path&quot;</span>)</span><br></pre></td></tr></table></figure></p><p><strong>预测</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_norm = tf.layers.batch_normalization(x, training=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">saver = tf.train.Saver(tf.global_variables())</span><br><span class="line">saver.restore(sess, <span class="string">&quot;your_path&quot;</span>)</span><br></pre></td></tr></table></figure></p><p><strong>estimator</strong><br>如果你使用的是高阶API：estimator进行训练的话，那么就比较麻烦，因为它的session没有暴露出来，你没办法直接使用，需要换个方式：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">model_fn_build</span>(<span class="params">init_checkpoint=<span class="literal">None</span>, lr=<span class="number">0.001</span>, model_dir=<span class="literal">None</span></span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_model_fn</span>(<span class="params">features, labels, mode, params</span>):</span><br><span class="line"></span><br><span class="line">        x = features[<span class="string">&#x27;inputs&#x27;</span>]</span><br><span class="line">        y = features[<span class="string">&#x27;labels&#x27;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment">#####################在这里定义你自己的网络模型###################</span></span><br><span class="line">        x_norm = tf.layers.batch_normalization(x, training=mode == tf.estimator.ModeKeys.TRAIN)</span><br><span class="line">        pre = tf.layers.dense(x_norm, <span class="number">1</span>)</span><br><span class="line">        loss = tf.reduce_mean(tf.<span class="built_in">pow</span>(pre - y, <span class="number">2</span>), name=<span class="string">&#x27;loss&#x27;</span>)</span><br><span class="line">        <span class="comment">######################在这里定义你自己的网络模型###################</span></span><br><span class="line"></span><br><span class="line">        lr = params[<span class="string">&#x27;lr&#x27;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment">######################进入eval和predict之前，都经过这一步加载过程###################</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 加载保存的模型</span></span><br><span class="line">        <span class="comment"># 为了加载batch_normalization的参数，需要global_variables</span></span><br><span class="line">        tvars = tf.global_variables()</span><br><span class="line">        initialized_variable_names = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> params[<span class="string">&#x27;init_checkpoint&#x27;</span>] <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">or</span> tf.train.latest_checkpoint(model_dir) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            checkpoint = params[<span class="string">&#x27;init_checkpoint&#x27;</span>] <span class="keyword">or</span> tf.train.latest_checkpoint(model_dir)</span><br><span class="line">            (assignment_map, initialized_variable_names</span><br><span class="line">             ) = get_assignment_map_from_checkpoint(tvars, checkpoint)</span><br><span class="line">            tf.train.init_from_checkpoint(checkpoint, assignment_map)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># tf.logging.info(&quot;**** Trainable Variables ****&quot;)</span></span><br><span class="line">        <span class="comment"># for var in tvars:</span></span><br><span class="line">        <span class="comment">#     init_string = &quot;&quot;</span></span><br><span class="line">        <span class="comment">#     if var.name in initialized_variable_names:</span></span><br><span class="line">        <span class="comment">#         init_string = &quot;, *INIT_FROM_CKPT*&quot;</span></span><br><span class="line">        <span class="comment">#     tf.logging.info(&quot;  name = %s, shape = %s%s&quot;, var.name, var.shape,</span></span><br><span class="line">        <span class="comment">#                     init_string)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">######################进入eval和predict之前，都经过这一步加载过程###################</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mode == tf.estimator.ModeKeys.TRAIN:</span><br><span class="line">            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)</span><br><span class="line">            train_op = optimizer.minimize(loss)</span><br><span class="line">            train_op = tf.group([train_op, update_ops])</span><br><span class="line">            <span class="keyword">return</span> tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mode == tf.estimator.ModeKeys.EVAL:</span><br><span class="line">            metrics = &#123;<span class="string">&quot;accuracy&quot;</span>: tf.metrics.accuracy(features[<span class="string">&#x27;label&#x27;</span>], pred)&#125;</span><br><span class="line">            <span class="keyword">return</span> tf.estimator.EstimatorSpec(mode, eval_metric_ops=metrics, loss=loss)</span><br><span class="line"></span><br><span class="line">        predictions = &#123;<span class="string">&#x27;predictions&#x27;</span>: pred&#125;</span><br><span class="line">        predictions.update(&#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> features.items()&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> tf.estimator.EstimatorSpec(mode, predictions=predictions)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tf.estimator.Estimator(_model_fn, model_dir=model_dir, config=config,</span><br><span class="line">                                  params=&#123;<span class="string">&quot;lr&quot;</span>: lr, <span class="string">&quot;init_checkpoint&quot;</span>: init_checkpoint&#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_assignment_map_from_checkpoint</span>(<span class="params">tvars, init_checkpoint</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Compute the union of the current variables and checkpoint variables.&quot;&quot;&quot;</span></span><br><span class="line">    assignment_map = &#123;&#125;</span><br><span class="line">    initialized_variable_names = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    name_to_variable = collections.OrderedDict()</span><br><span class="line">    <span class="keyword">for</span> var <span class="keyword">in</span> tvars:</span><br><span class="line">        name = var.name</span><br><span class="line">        m = re.<span class="keyword">match</span>(<span class="string">&quot;^(.*):\\d+$&quot;</span>, name)</span><br><span class="line">        <span class="keyword">if</span> m <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            name = m.group(<span class="number">1</span>)</span><br><span class="line">        name_to_variable[name] = var</span><br><span class="line"></span><br><span class="line">    init_vars = tf.train.list_variables(init_checkpoint)</span><br><span class="line"></span><br><span class="line">    assignment_map = collections.OrderedDict()</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> init_vars:</span><br><span class="line">        (name, var) = (x[<span class="number">0</span>], x[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">if</span> name <span class="keyword">not</span> <span class="keyword">in</span> name_to_variable:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        assignment_map[name] = name</span><br><span class="line">        initialized_variable_names[name] = <span class="number">1</span></span><br><span class="line">        initialized_variable_names[name + <span class="string">&quot;:0&quot;</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (assignment_map, initialized_variable_names)</span><br><span class="line"></span><br></pre></td></tr></table></figure></p><h3 id="3-3-一些总结"><a href="#3-3-一些总结" class="headerlink" title="3.3 一些总结"></a>3.3 一些总结</h3><p>笔者在使用中有一些对比结论：</p><ul><li>首先 bn 层往往放在 dense 层和 activation 层（一般 ReLU）之间，有助于加速收敛和防止过拟合；</li><li>尽量不在 sigmoid 的激活层前加，可能会使得模型难以收敛；</li><li>输出层之前一般不能加；</li><li>training 的参数及其重要。</li></ul><p><strong>参考文章</strong><br><a href="https://blog.csdn.net/shuzfan/article/details/50723877">解读Batch Normalization</a><br><a href="https://blog.csdn.net/caicaiatnbu/article/details/72742293?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-10.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-10.no_search_link">[TensorFlow 学习笔记-05]批标准化(Bacth Normalization，BN)</a><br><a href="https://blog.csdn.net/sgyuanshi/article/details/115268969">tensorflow中batch_normalization的正确使用姿势</a><br><a href="https://www.jianshu.com/p/437fb1a5823e">Batch Normalization的正确打开方式</a><br><a href="https://arxiv.org/pdf/1502.03167.pdf">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></p><hr>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-背景&quot;&gt;&lt;a href=&quot;#1-背景&quot; class=&quot;headerlink&quot; title=&quot;1 背景&quot;&gt;&lt;/a&gt;1 背景&lt;/h2&gt;&lt;p&gt;如果你是一个玩深度学习的算法工程师，那么相信你对批标准化（&lt;code&gt;Batch Normalization&lt;/code&gt;）一定不陌生。在实际训练深度模型中，&lt;strong&gt;BN 往往用来加速模型收敛或者缓解梯度消失/爆炸的问题&lt;/strong&gt;。笔者在实际使用过程中也有一定的收获和思考，收获是不同的使用姿势确实能够带来不一样的效果。思考就是，虽然大致知道BN的原理和公式，但是创建 BN 这个方法的出发点和一些边界问题的思考始终萦绕在周围。在此做一个汇总整理，旨在帮助和我一样有此困惑的朋友们。&lt;/p&gt;
&lt;h2 id=&quot;2-原理&quot;&gt;&lt;a href=&quot;#2-原理&quot; class=&quot;headerlink&quot; title=&quot;2 原理&quot;&gt;&lt;/a&gt;2 原理&lt;/h2&gt;&lt;h3 id=&quot;2-1-优化原理&quot;&gt;&lt;a href=&quot;#2-1-优化原理&quot; class=&quot;headerlink&quot; title=&quot;2.1 优化原理&quot;&gt;&lt;/a&gt;2.1 优化原理&lt;/h3&gt;&lt;p&gt;训练网络深度的加深可能会带来梯度迭代上的&lt;code&gt;梯度消失&lt;/code&gt;（Gradient Vanishing)或者&lt;code&gt;梯度爆炸&lt;/code&gt;(Gradient Explore)问题。这两个问题的产生原理这里不做赘述，一般都是由于网络层数过深，梯度链式传导带来的结果。&lt;/p&gt;</summary>
    
    
    
    <category term="学习笔记" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="算法总结" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="算法" scheme="https://www.xiemingzhao.com/tags/%E7%AE%97%E6%B3%95/"/>
    
    <category term="批标准化" scheme="https://www.xiemingzhao.com/tags/%E6%89%B9%E6%A0%87%E5%87%86%E5%8C%96/"/>
    
    <category term="BN" scheme="https://www.xiemingzhao.com/tags/BN/"/>
    
  </entry>
  
  <entry>
    <title>SNE 和 t-SNE 算法</title>
    <link href="https://www.xiemingzhao.com/posts/senalgo.html"/>
    <id>https://www.xiemingzhao.com/posts/senalgo.html</id>
    <published>2021-03-03T16:00:00.000Z</published>
    <updated>2025-04-01T16:33:14.807Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>实际研究中有很多的<code>降维算法</code>，例如经典的线性降维算法<code>PCA</code>，相信很多人都比较熟悉了。而在这里，我们要介绍的是一个经典的降维算法<code>t-SNE</code>，它往往用来对高维数据做非线性降维来进行可视化分析。参考了不少大牛的文章，加上一些自己的思考，从一个小白的角度来总结一下该算法的原理和使用姿势。</p><h3 id="1-1-维数灾难"><a href="#1-1-维数灾难" class="headerlink" title="1.1 维数灾难"></a>1.1 维数灾难</h3><p><code>维数灾难</code>（curse of dimensionality）：描述的是高维空间中若干迥异于低维空间、甚至反直觉的现象。</p><p>在这里我们要阐述两个理论：</p><ol><li>高维空间中数据样本极其稀疏。<br>如下图所示，高维数据降维到低维空间将发生“拥挤问题（Crowding Problem）。</li></ol><span id="more"></span><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/cluster/sen0.jpg" alt="sen0"></p><ol><li>高维单位空间中数据几乎全部位于超立方体的边缘。</li></ol><h3 id="1-2-数理佐证"><a href="#1-2-数理佐证" class="headerlink" title="1.2 数理佐证"></a>1.2 数理佐证</h3><p>以上两个理论并不是空口而谈，我们能够用几何学来证明。首先，在高维空间中的单位超立方体的体积是：</p><script type="math/tex; mode=display">V_{hypercube} = 1^d = 1</script><p>对应的内切球体积为：</p><script type="math/tex; mode=display">V_{hypersphere} = \frac{\pi^{n/2}}{\Gamma(n/2 + 1)} \cdot 0.5^d</script><p>两者的商取极限就有：</p><script type="math/tex; mode=display">lim_{d \to +\infty} \frac{V_{hypersphere}}{V_{hypercube}} = 0</script><p>上述表明：<strong>在极端的高维情况下，单元空间只有边角，而没有中心</strong>。数据也只能处于边缘上，而远离中心。</p><p>由此我们又能推出结论：<strong>欧式距离会失效</strong></p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/cluster/sen1.jpg" alt="sen1"></p><p>上图描述的是高维空间中大距离和小距离的差异越来越不明显：</p><script type="math/tex; mode=display">lim_{d \to +\infty} \frac{dist_{max} - dist_{min}}{d_{min}} = 0</script><p>所以<strong>降维的基本作用</strong>：</p><ul><li>缓解维数灾难，使得欧式距离重新生效；</li><li>数据预处理，降噪去冗余；</li><li>可视化分析。</li></ul><h3 id="1-3-SNE算法的思想基础"><a href="#1-3-SNE算法的思想基础" class="headerlink" title="1.3 SNE算法的思想基础"></a>1.3 SNE算法的思想基础</h3><p><strong>SNE 的两个思想要点</strong>：</p><ul><li>构建一个高维对象之间的概率分布，使得对象间的相似度和被选择的概率成正相关；</li><li>将高维的数据映射到低维空间，使得两个空间的概率分布尽可能相似；</li></ul><p>看下面的图，比较形似。高维空间中的数据点对应低维空间的数据点，通过一个链接关系牵引使得两个空间中对应数据点的分布类似。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/cluster/sen2.jpg" alt="sen2"></p><p>SNE 两个主要步骤：</p><ul><li>将欧氏距离转化为条件概率来表征点间相似度（pairwise similarity）。</li><li>使用梯度下降算法来使低维分布学习/拟合高维分布。</li></ul><h2 id="2-SNE的算法原理"><a href="#2-SNE的算法原理" class="headerlink" title="2 SNE的算法原理"></a>2 SNE的算法原理</h2><p>给定一组高维空间的数据样本：$(x_1, … , x_N)$，<strong>其中<code>N</code>表示的是样本数，并不是维度</strong>。前面我们提到，在高维空间中，欧式距离将会失效，所以这里会将其转化成<code>条件概率</code>来度量两个样本数据之间的相似度或者说距离。</p><h3 id="2-1-相似性度量-条件概率"><a href="#2-1-相似性度量-条件概率" class="headerlink" title="2.1 相似性度量-条件概率"></a>2.1 相似性度量-条件概率</h3><p>那么对于样本点$x<em>i$来说，上面提到的相似性度量$p</em>{j|i}$就是以高斯分布来选择$x_j$作为近邻点的条件概率：</p><script type="math/tex; mode=display">p_{j \mid i} = \frac{\exp(- \mid  \mid  x_i -x_j  \mid  \mid  ^2 / (2 \sigma^2_i ))} {\sum_{k \neq i} \exp(- \mid  \mid  x_i - x_k  \mid  \mid  ^2 / (2 \sigma^2_i))}</script><p>这里需要指出三点：</p><ol><li>对除 i 外其他所有 j 都计算一个条件概率后，形成一个概率分布列，所以分母需要归一化；</li><li>设定 $p_{i|i}=0$，因为我们关注的是两两之间的相似度。</li><li>有一个参数是 $\sigma_i$，对于不同的点 $x_i$ 取值不一样。</li></ol><p>另一方面，如前面所述，降维就是将高维映射到低维，那么我们对上述的高维样本点$(x_1, … , x_N)$，构造出低维空间中对应的样本点$(y_1, … , y_N)$。同样的，该空间中也有对应的度量相似度的条件概率：</p><script type="math/tex; mode=display">q_ {j \mid i} = \frac{\exp(- \mid  \mid  x_i -x_j  \mid  \mid  ^2)} {\sum_{k \neq i} \exp(- \mid  \mid  x_i - x_k  \mid  \mid  ^2)}</script><p><strong>注意：这里我们令$\sigma_i = \frac{1}{\sqrt 2}$，若方差取其他值，对结果影响仅仅是缩放而已。</strong></p><h3 id="2-2-高低维空间分布一致性"><a href="#2-2-高低维空间分布一致性" class="headerlink" title="2.2 高低维空间分布一致性"></a>2.2 高低维空间分布一致性</h3><p>到这里，我们已经定义了原始高维空间的样本数据以及映射到低位空间后的对应数据点，以及两个空间中度量样本相似度的条件概率。基于前文的算法思想：<strong>我们需要做的就是让低维空间的数据分布尽可能的靠近或者拟合高维空间中的样本分布</strong>。提到度量分布的一致程度，很自然的能够想到<code>KL散度</code>。</p><script type="math/tex; mode=display">C = \sum_i KL(P_i  \mid  \mid  Q_i) = \sum_i \sum_j p_{j \mid i} \log \frac{p_{j \mid i}}{q_{j \mid i}}</script><p>其中，$P<em>i(k = j) = p</em>{j|i}$和$Q<em>i(k = j) = q</em>{j|i}$是两个分布列。所以，我们期望，在降维效果好的时候局部分布保留相似性，即$p<em>{i|j} = q</em>{i|j}$。</p><blockquote><p>这里需要注意：<strong>KL散度具有不对称性</strong>。</p></blockquote><p><em>则如果高维数据相邻而低维数据分开（即p大q小），则cost很大；相反，如果高维数据分开而低维数据相邻（即p小q大），则cost很小。</em></p><p>所以，<strong>SNE倾向于保留高维数据的局部结构</strong>。</p><h3 id="2-3-困惑度-perplexity"><a href="#2-3-困惑度-perplexity" class="headerlink" title="2.3 困惑度(perplexity)"></a>2.3 困惑度(perplexity)</h3><p>前面的公式中我们提到了不同的点具有不同的$\sigma_i$，而$P_i$的熵会随着$\sigma_i$的增加而增加。</p><script type="math/tex; mode=display">Perp(P_i) = 2^{H(P_i)} = 2^{-\sum_j p_{j \mid i} \log_2 p_{j \mid i}}</script><blockquote><p>注意：困惑度设的大，则显然σ_i也大。两者是单调关系，因此可以使用<code>二分查找</code>。</p></blockquote><p>虽然该取值对效果具有一定的鲁棒性，但一般<strong>建议困惑度设为5-50</strong>比较好，它可以解释为<strong>一个点附近的有效近邻点个数</strong>。</p><h3 id="2-4-梯度求解"><a href="#2-4-梯度求解" class="headerlink" title="2.4 梯度求解"></a>2.4 梯度求解</h3><p>前面已经介绍了 lossfunc，简单推导可知其梯度公式为：</p><script type="math/tex; mode=display">\frac{\partial C}{\partial y_i} = 2 \sum_j (p_{j \mid i} - q_{j \mid i} + p_{i \mid j} - q_{i \mid j})(y_i - y_j)</script><p>其结构与 softmax 类似。我们知道$\sum -y \log p$对应的梯度为$y-p$可以简单推导得知SNE的lossfunc中的i在j下的条件概率情况的梯度是$2(p<em>{i \mid j}-q</em>{i \mid j})(y<em>i-y_j)$， 同样j在i下的条件概率的梯度是$2(p</em>{j \mid i}-q_{j \mid i})(y_i-y_j)$.</p><p>为了加速优化过程和避免陷入局部最优解，我们需要引入动量，即之前的梯度累加的指数衰减项：</p><script type="math/tex; mode=display">y_i^{(t)} = y_i^{(t-1)} + \eta \frac{\partial C}{\partial y_i} + \alpha(t)(y_i^{(t-1)} - y_i^{(t-2)})</script><blockquote><p>在初始优化的阶段，每次迭代中可以引入一些高斯噪声，之后像模拟退火一样逐渐减小该噪声，可以用来避免陷入局部最优解。因此，SNE在选择高斯噪声，以及学习速率，什么时候开始衰减，动量选择等等超参数上，需要跑多次优化才可以。</p></blockquote><h3 id="2-5-SNE的问题"><a href="#2-5-SNE的问题" class="headerlink" title="2.5 SNE的问题"></a>2.5 SNE的问题</h3><p>虽然上述给出了SNE算法的原理和求解方式，但实际上其是比较难以优化的，而且存在<code>crowding problem</code>(拥挤问题)：</p><blockquote><p>由于降维后的空间压缩，会使得哪怕高维空间中离得较远的点，在低维空间中留不出这么多空间来映射。于是到最后高维空间中的点，尤其是远距离和中等距离的点，在低维空间中统统被塞在了一起。</p></blockquote><p>这里的原理在前文已经详细介绍过，是一个比较重要的问题。所以，Hinton 等人又提出了 t-SNE 的方法。与 SNE 不同，主要如下:</p><ul><li>使用对称版的SNE，简化梯度公式</li><li>低维空间下，使用t分布替代高斯分布表达两点之间的相似度</li></ul><h2 id="3-对称-SNE-Symmetric-SNE"><a href="#3-对称-SNE-Symmetric-SNE" class="headerlink" title="3 对称 SNE(Symmetric SNE)"></a>3 对称 SNE(Symmetric SNE)</h2><p>我们首先简单介绍一下<code>对称SNE</code>，它也是一种缓解拥挤问题的办法。它的主要思想就是<strong>使用联合概率分布来替换条件概率分布</strong>。我们假设P是高维空间里的各个点的联合概率分布，Q是对应的低维空间，目前函数：</p><script type="math/tex; mode=display">C = KL(P \mid  \mid Q) = \sum_i \sum_j p_{i,j} \log \frac{p_{ij}}{q_{ij}}</script><p>这里的$p<em>{ii}, q</em>{ii}$为0，我们将这种SNE称之为 <code>symmetric SNE</code> (对称SNE)，因为他假设了对于任意i,$p<em>{ij} = p</em>{ji}, q<em>{ij} = q</em>{ji}$，因此概率分布可以改写为:</p><script type="math/tex; mode=display">p_{ij} = \frac{\exp(- \mid  \mid x_i - x_j \mid  \mid ^2 / 2\sigma^2)}{\sum_{k \neq l} \exp(- \mid  \mid x_k-x_l \mid  \mid ^2 / 2\sigma^2)}  \ \ \ \ q_{ij} = \frac{\exp(- \mid  \mid y_i - y_j \mid  \mid ^2)}{\sum_{k \neq l} \exp(- \mid  \mid y_k-y_l \mid  \mid ^2)}</script><p>公式整体简洁一些，但是如果$x_i$是异常值，将会使得$||x_i - x_j||^2$很大，那么对应的lossfunc就会很小，会使得训练不好。为了解决此问题，我们可以将联合概率分布修改为：</p><script type="math/tex; mode=display">p_{ij} = \frac{p_{i|j} + p_{j|i}}{2}</script><p>如此便可以保证$\sum<em>j p</em>{ij} &gt; \frac{1}{2n}$，即每一个样本都会贡献一定的lossfunc，并且使得梯度变成：</p><script type="math/tex; mode=display">\frac{\partial C}{\partial y_i} = 4 \sum_j (p_{ij} - q_{ij})(y_i - y_j)</script><p><strong>实际使用中<code>对称SNE</code>往往不会比<code>SNE</code>的效果差</strong>。</p><h2 id="4-t-SNE算法"><a href="#4-t-SNE算法" class="headerlink" title="4 t-SNE算法"></a>4 t-SNE算法</h2><h3 id="4-1-t分布的应用到SNE"><a href="#4-1-t分布的应用到SNE" class="headerlink" title="4.1 t分布的应用到SNE"></a>4.1 t分布的应用到SNE</h3><p>上面介绍了SNE的问题和对称SNE，更正统的做法便是<code>t-SNE</code>算法：</p><blockquote><p>在不同空间使用不同的分布来将距离转换成概率分布，高维空间中一般用<code>高斯分布</code>，而在对应的低维空间中我们一般使用更加长尾的<code>t-分布</code>，如此便可以使得高维度下中低等的距离在映射后能够有一个较大的距离。</p></blockquote><p>首先我们知道<code>t-分布</code>的概率密度函数（PDF）形式为：</p><script type="math/tex; mode=display">f(t) = \frac{\Gamma(\frac{v+1}{2})}{\sqrt{v \pi} \Gamma(\frac{v}{2})} (1 + \frac{t^2}{v})^{-\frac{v+1}{2}}</script><p>其中v代表数据的自由度，当$v=1$的时候一般称为<code>柯西分布</code>（Cauchy distribution），这就是我们在低维空间中将要使用的具体分布：</p><script type="math/tex; mode=display">f(t) = \frac{1}{\pi(1 + t^2)}</script><p>而当$v = \infty$的时候就称为<code>高斯/正态分布</code>(Guassian/Normal distribution)，也就是原始数据高维空间中使用的分布：</p><script type="math/tex; mode=display">f(t) = \frac{1}{\sqrt{2 \pi}} e^{- \frac{t^2}{2}}</script><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/cluster/sen3.png" alt="sen3"></p><p>上图展示了所介绍的两个分布的对比图，可以发现<code>t-分布</code>相对厚尾许多，能够更好的捕捉真实数据特征。</p><p>回到SNE求解那里，我们使用<code>t-分布</code>带入变换之后，将得到：</p><script type="math/tex; mode=display">q_{ij} = \frac{(1 +  \mid  \mid y_i -y_j \mid  \mid ^2)^{-1}}{\sum_{k \neq l} (1 +  \mid  \mid y_i -y_j \mid  \mid ^2)^{-1}}</script><p>对应的梯度为：</p><script type="math/tex; mode=display">\frac{\delta C}{\delta y_i} = 4 \sum_j(p_{ij}-q_{ij})(y_i-y_j)(1+ \mid  \mid y_i-y_j \mid  \mid ^2)^{-1}</script><p>为了更好的展示为什么使用<code>t-分布</code>可以通过“把尾巴抬高”来缓解SNE的拥挤问题，我们将两个分布的映射对比图画出如下所示。其中，横轴表示距离，纵轴表示相似度。我们可以发现t-分布很好的满足了我们的需求，即：</p><ul><li>对于较大相似度的点，即图中上方的红线，表明t分布在低维空间中的距离需要稍小一点；</li><li>对于低相似度的点，即图中下方的红线，表明t分布在低维空间中的距离需要更远。</li></ul><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/cluster/sen4.png" alt="sen4"></p><h3 id="4-2-t-SNE缺点"><a href="#4-2-t-SNE缺点" class="headerlink" title="4.2 t-SNE缺点"></a>4.2 t-SNE缺点</h3><ol><li>时间、空间复杂度为O(n^2)，计算代价昂贵。百万量级的数据需要几小时，对于PCA可能只需要几分钟。</li><li>升级版 Barnes-Hut t-SNE 可以让复杂度降为O(nlogn)，但只限于获得二维和三维的嵌入。（sklearn中可以直接使用参数method=’barnes_hut’）</li><li>由于代价函数非凸，多次执行算法的结果是随机的（名字中“Stochatsic”的由来？），需要多次运行选取最好的结果。</li><li>全局结构不能很清楚的保留。<strong>这个问题可以通过先用PCA降维到一个合理的维度（如50）后再用t-SNE来缓解</strong>，前置的PCA步骤也可以起到去除噪声等功能。（sklearn中可以直接使用参数init=’pca’）</li></ol><h3 id="4-3-小补充"><a href="#4-3-小补充" class="headerlink" title="4.3 小补充"></a>4.3 小补充</h3><p>优化过程中可以尝试的两个 trick:</p><ol><li><code>提前压缩</code>(early compression)：开始初始化的时候，各个点要离得近一点。这样小的距离，方便各个聚类中心的移动。可以通过引入L2正则项(距离的平方和)来实现。</li><li><code>提前夸大</code>(early exaggeration)：在开始优化阶段，pij 乘以一个大于1的数进行扩大，来避免因为 qij 太小导致优化太慢的问题。比如前50次迭代，pij 乘以4。</li></ol><p>最后附上一幅常见的t-SNE降维过程效果图：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/cluster/sen5.gif" alt="sen5"></p><h2 id="5-t-SNE实战coding"><a href="#5-t-SNE实战coding" class="headerlink" title="5 t-SNE实战coding"></a>5 t-SNE实战coding</h2><p>我们来看两个简单的例子。</p><ol><li><p>假设现在有一组3维数据，我需要将其降维到2维进行可视化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">hello_tSNE</span>():</span><br><span class="line">    X = np.array([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">    tsne = TSNE(n_components=<span class="number">2</span>)</span><br><span class="line">    tsne.fit_transform(X)</span><br><span class="line">    <span class="built_in">print</span>(tsne.embedding_)</span><br></pre></td></tr></table></figure></li><li><p>高维S曲线数据的降维可视化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">S curve visualization</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># # Next line to silence pyflakes. This import is needed.</span></span><br><span class="line"><span class="comment"># Axes3D</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tSNE_forS</span>():</span><br><span class="line">    n_points = <span class="number">1000</span></span><br><span class="line">    <span class="comment"># 生成S曲线的样本数据</span></span><br><span class="line">    <span class="comment"># X是一个(1000, 3)的2维数据，color是一个(1000,)的1维数据</span></span><br><span class="line">    X, color = datasets.make_s_curve(n_points, random_state=<span class="number">0</span>)</span><br><span class="line">    n_neighbors = <span class="number">10</span></span><br><span class="line">    n_components = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">    <span class="comment"># 创建了一个figure，标题为&quot;Manifold Learning with 1000 points, 10 neighbors&quot;</span></span><br><span class="line">    plt.suptitle(<span class="string">&quot;Manifold Learning with %i points, %i neighbors&quot;</span></span><br><span class="line">                 % (<span class="number">1000</span>, n_neighbors), fontsize=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;绘制S曲线的3D图像&#x27;&#x27;&#x27;</span></span><br><span class="line">    ax = fig.add_subplot(<span class="number">211</span>, projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line">    ax.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], X[:, <span class="number">2</span>], c=color, cmap=plt.cm.Spectral)</span><br><span class="line">    ax.view_init(<span class="number">4</span>, -<span class="number">72</span>)  <span class="comment"># 初始化视角</span></span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;t-SNE&#x27;&#x27;&#x27;</span></span><br><span class="line">    t0 = time()</span><br><span class="line">    tsne = manifold.TSNE(n_components=n_components, init=<span class="string">&#x27;pca&#x27;</span>, random_state=<span class="number">0</span>)</span><br><span class="line">    Y = tsne.fit_transform(X)  <span class="comment"># 转换后的输出</span></span><br><span class="line">    t1 = time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;t-SNE: %.2g sec&quot;</span> % (t1 - t0))  <span class="comment"># 算法用时</span></span><br><span class="line">    ax = fig.add_subplot(<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    plt.scatter(Y[:, <span class="number">0</span>], Y[:, <span class="number">1</span>], c=color, cmap=plt.cm.Spectral)</span><br><span class="line">    plt.title(<span class="string">&quot;t-SNE (%.2g sec)&quot;</span> % (t1 - t0))</span><br><span class="line">    ax.xaxis.set_major_formatter(NullFormatter())  <span class="comment"># 设置标签显示格式为空</span></span><br><span class="line">    ax.yaxis.set_major_formatter(NullFormatter())</span><br><span class="line">    <span class="comment"># plt.axis(&#x27;tight&#x27;)</span></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></li></ol><p>效果：<br><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/cluster/sen6.png" alt="sen6"></p><p><strong>参考文章</strong><br><a href="http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf">Visualizing Data using t-SNE</a><br><a href="http://www.datakit.cn/blog/2017/02/05/t_sne_full.html">t-SNE完整笔记</a><br><a href="https://blog.csdn.net/hustqb/article/details/78144384">数据降维与可视化——t-SNE</a><br><a href="https://www.jiqizhixin.com/articles/2017-11-13-7">详解可视化利器 t-SNE 算法：数无形时少直觉</a><br><a href="https://www.jianshu.com/p/700f017cd330">t-SNE降维原理</a><br><a href="https://kknews.cc/education/83ajqm4.html">t-SNE：最好的降维方法之一</a><br><a href="https://bindog.github.io/blog/2016/06/04/from-sne-to-tsne-to-largevis/">从SNE到t-SNE再到LargeVis</a></p><hr>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-引言&quot;&gt;&lt;a href=&quot;#1-引言&quot; class=&quot;headerlink&quot; title=&quot;1 引言&quot;&gt;&lt;/a&gt;1 引言&lt;/h2&gt;&lt;p&gt;实际研究中有很多的&lt;code&gt;降维算法&lt;/code&gt;，例如经典的线性降维算法&lt;code&gt;PCA&lt;/code&gt;，相信很多人都比较熟悉了。而在这里，我们要介绍的是一个经典的降维算法&lt;code&gt;t-SNE&lt;/code&gt;，它往往用来对高维数据做非线性降维来进行可视化分析。参考了不少大牛的文章，加上一些自己的思考，从一个小白的角度来总结一下该算法的原理和使用姿势。&lt;/p&gt;
&lt;h3 id=&quot;1-1-维数灾难&quot;&gt;&lt;a href=&quot;#1-1-维数灾难&quot; class=&quot;headerlink&quot; title=&quot;1.1 维数灾难&quot;&gt;&lt;/a&gt;1.1 维数灾难&lt;/h3&gt;&lt;p&gt;&lt;code&gt;维数灾难&lt;/code&gt;（curse of dimensionality）：描述的是高维空间中若干迥异于低维空间、甚至反直觉的现象。&lt;/p&gt;
&lt;p&gt;在这里我们要阐述两个理论：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;高维空间中数据样本极其稀疏。&lt;br&gt;如下图所示，高维数据降维到低维空间将发生“拥挤问题（Crowding Problem）。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="学习笔记" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="算法总结" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="算法" scheme="https://www.xiemingzhao.com/tags/%E7%AE%97%E6%B3%95/"/>
    
    <category term="降维" scheme="https://www.xiemingzhao.com/tags/%E9%99%8D%E7%BB%B4/"/>
    
    <category term="SNE" scheme="https://www.xiemingzhao.com/tags/SNE/"/>
    
  </entry>
  
  <entry>
    <title>word2vec 详解</title>
    <link href="https://www.xiemingzhao.com/posts/word2vec.html"/>
    <id>https://www.xiemingzhao.com/posts/word2vec.html</id>
    <published>2020-12-12T16:00:00.000Z</published>
    <updated>2025-04-01T17:10:48.260Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>很多算法工程师认为 Embedding 技术是机器学习中最迷人的一种思想，在过去的近10年中，该技术在各个深度学习领域大放异彩。已经逐步进化到了近几年基于 BERT 和 GPT2 等模型的语境化嵌入。本文重点基于原始论文<a href="https://arxiv.org/pdf/1301.3781v3.pdf">Efficient Estimation of Word Representations in Vector Space</a>，整理 word2vec 相关技术的基础原理和应用经验，旨在利于自己回溯巩固和他人参考学习。</p><p>首先 Embedding 的思想是如何来的呢？我们知道计算机底层只能识别数字，并基于其进行逻辑等计算。而世间大多的实体或概念都不是以数据形式存在的，如何让计算机能够记住甚至理解是一件很难的事情。</p><span id="more"></span><p>如果我们能够将实体或概念以一种有意义的代数向量的形式输入给计算机，那么计算机对于它们的存储、理解和计算将会极大的友好。比如，对于一个人，如果我们重点关注他的性别、年龄、身高、体重、胸围、存款这些信息，那么我们可以将其记为以下形式：</p><p>[1,18,180,70,90,100]</p><p>其中每个维度的数值对应该维度的信息，也即性别=男（1）、年龄=18、身高=180cm、体重=70kg、胸围=90cm、存款=100W。当然你可以继续扩增更多的维度，维度信息越多，计算机对这个对象认识的更全面。</p><h2 id="2-Word-Embedding"><a href="#2-Word-Embedding" class="headerlink" title="2 Word Embedding"></a>2 Word Embedding</h2><p>在 NLP 领域，计算对于词的理解一直是一个很重要的问题。如前文所述，Word Embedding <code>目的</code>就是<strong>把词汇表中的单词或者短语（words or phrases）映射成由实数构成的向量</strong>上，而其<code>方法</code>一般是<strong>从数据中自动学习输入空间到 Distributed representation 空间的映射 f</strong>。</p><h3 id="2-1-One-hot"><a href="#2-1-One-hot" class="headerlink" title="2.1 One-hot"></a>2.1 One-hot</h3><p><code>One-hot</code> 编码又称<code>独热编码</code>，具体方法是：用一个N位状态寄存器来对N个状态进行编码，N是指所编码特征的空间大小。例如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gender:[&quot;male&quot;, &quot;female&quot;]</span><br><span class="line">country:[&quot;US&quot;, &quot;China&quot;,&quot;Japan&quot;,&quot;France&quot;,&quot;Italy&quot;]</span><br></pre></td></tr></table></figure><p>这两个特征的每一个取值可以被 One-hot 编码为：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gender = [[1,0], [0,1]]</span><br><span class="line">country = [[1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1]]</span><br></pre></td></tr></table></figure></p><p>如此，One-hot 编码的优缺点还是很明显的。</p><ul><li>优点：解决了分类器不好处理离散数据的问题，在一定程度上也起到了扩充特征的作用。</li><li>缺点：1. 不考虑词的顺序；2. 假设词之间相互独立；3. 向量是高度稀疏的；4. 容易出现维度爆炸。</li></ul><h3 id="2-2-Dristributed-representation"><a href="#2-2-Dristributed-representation" class="headerlink" title="2.2 Dristributed representation"></a>2.2 Dristributed representation</h3><p>根据One-hot的缺点，我们更希望用诸如“语义”，“复数”，“时态”等维度去描述一个单词。每一个维度不再是0或1，而是连续的实数，表示不同的程度。</p><p>Dristributed representation 可以解决 One hot representation 的问题，它的<strong>思路是通过训练，将每个词都映射到一个较短的稠密词向量上来。</strong></p><p>例如，king 这个词可能从一个非常稀疏的空间映射到一个稠密的四维空间，假设[0.99,0.99,0.05,0.7]。那这个映射一般要满足：</p><ul><li>这个映射是一一映射；</li><li>映射后的向量不会丢失之前所包含的信息。</li></ul><p>这个过程就成为 <code>Word Embedding</code> （词嵌入），而一个好的词嵌入一般能够获得有意义的词向量，例如一个经典的case，即可以从词向量上发现:</p><script type="math/tex; mode=display">\vec King - \vec Man + \vec Womman = \vec Queen</script><h3 id="2-3-Cocurrence-matrix"><a href="#2-3-Cocurrence-matrix" class="headerlink" title="2.3 Cocurrence matrix"></a>2.3 Cocurrence matrix</h3><p><strong>一般认为某个词的意思跟它临近的单词是紧密相关的</strong>。这时可以设定一个窗口（大小一般是5~10），如下窗口大小是2，那么在这个窗口内，与 rests 共同出现的单词就有 life、he、in、peace。然后我们就利用这种共现关系来生成词向量。</p><blockquote><p>… Bereft of life he rests in peace! If you hadn’t nailed him …</p></blockquote><p>假设窗口大小为1，此时，将得到一个对称矩阵——<code>共现矩阵</code>，如此就可以实现将 word 变成向量的设想，在共现矩阵每一行（或每一列）都是对应单词的一个向量表示。如下所示：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/word2vec0.png" alt="word2vec"></p><h2 id="3-word2vec"><a href="#3-word2vec" class="headerlink" title="3. word2vec"></a>3. word2vec</h2><h3 id="3-1-基本模型结构"><a href="#3-1-基本模型结构" class="headerlink" title="3.1 基本模型结构"></a>3.1 基本模型结构</h3><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/word2vec1.png" alt="word2vec1"></p><p>如上图所示，是 word2vec 的基本模型结构，其目的是：</p><p><strong>利用单词本身的 one-hot 编码来完成的预测（即不同场景下的context），然后利用训练过程中映射矩阵中对应的向量来作为单词的表示。</strong></p><p>简述一下上图的流程：</p><ol><li>输入是 One-hot 编码，通过与映射矩阵$W_{V \times N}$得到隐层的行向量；</li><li>从隐层到输出层，有另一个映射矩阵$W’_{N \times V}$，与前面的行向量相乘得到输出向量；</li><li>之后经过 softmax 层，便得到每个词的概率。</li></ol><p>整个过程用数学来表达就是：</p><script type="math/tex; mode=display">u_j = W'W^T x = { w'_{ij} } = {v'}_{w_j}^{T} {v}_{w_i}^T</script><script type="math/tex; mode=display">p{w_j | w_i} = y_i = \frac{exp(u_j)}{\sum_{ {j}'=1}^V exp(u'_j)} = \frac{exp({v'}_{w_j}^T v_{w_i})}{\sum_{ {j}'=1}^V exp({v'}_{w_{j'} }^T  v_{w_i}) )}</script><p>其中 $u<em>i$ 代表了输出向量中第 i 个单词的概率， $v</em>{w<em>i}$ 和 ${v’}</em>{w_{j’ } }^T$ 分别代表了 $W$ 中对应的行向量和 $W’$ 中对应的列向量。</p><h3 id="3-2-CBOW-Continuous-Bags-of-word"><a href="#3-2-CBOW-Continuous-Bags-of-word" class="headerlink" title="3.2 CBOW(Continuous Bags-of-word)"></a>3.2 CBOW(Continuous Bags-of-word)</h3><p>基于上述，我们来看一个经典的模型结构，<code>CBOW</code>，即连续词袋模型。与基准模型结构不同的是，<strong>CBOW 模型利用输入 context 多个词的向量均值作为输入</strong>。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/word2vec2.jpg" alt="word2vec2"></p><p>如上图所示，用数学描述即：</p><script type="math/tex; mode=display">h = \frac{1}{C} W^T (x_1 ++ x_2 + \dots + x_C) = \frac{1}{C}(v_{w_1} + v_{w_2} + \dots + v_{w_C})</script><p>其中，C 为 context 的词语数量，所以CBOW的损失函数为：</p><script type="math/tex; mode=display">\begin{array}{l}E & = & -log p(w_O|w_{I,1}, \dots , w_{I,C}) \\& = & - u_{j^*} + log \sum_{ {j}' = 1} ^ V exp(u_{ {j}'}) \\& = & - v'_{w_O} \cdot h + log \sum_{ {j}' = 1} ^ V exp({v'}_{w_j}^T) \cdot h\end{array}</script><h3 id="3-3-Skip-Gram-Model"><a href="#3-3-Skip-Gram-Model" class="headerlink" title="3.3 Skip-Gram Model"></a>3.3 Skip-Gram Model</h3><blockquote><p><code>核心区别</code>：Skip-gram 是预测一个词的上下文，而 CBOW 是用上下文预测这个词，即 y 有多个词。</p></blockquote><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/word2vec3.png" alt="word2vec3"></p><p>在传递过程中，其不再输出单个的多项分布（即一个词语的 one-hot 编码），而是利用共享的参数输出映射矩阵输出 C 个多项分布（此处  C 为context词语的数量）：</p><script type="math/tex; mode=display">p(w_{c,j} = w_{O,c}|w_I) = y_{c,j} = \frac{exp(u_{c,j})}{\sum_{ {j'} - 1}^V exp(u_{j'})}</script><p>其中：</p><ul><li>$w_{c,j}$是输出层第 c 部分中的第 j 个数字；</li><li>$w_{O,c}$是输出 context 词中第 c 个数字；</li><li>$w_I$ 是输入的唯一单词；</li><li>$y_{c,j}$ 是输出层第 c 部分中的第 j 个单元；</li><li>$u_{c,j}$是输出层第 c 部分上第 j 个单元的净输入。</li></ul><p>由于输出时映射矩阵的参数共享，所以有：</p><script type="math/tex; mode=display">u_{c,j} = u_j = {v'}_{w_j}^T \cdot h, \quad for  c = 1, 2, \dots , C</script><p>在Skip-Gram中的 loss function 为：</p><script type="math/tex; mode=display">\begin{array}{l}E & = & -log p(w_{O,1}, w_{O,2}, \dots , w_{O,C} | w_I) \\& = & - log \prod_{c = 1} ^C \frac{exp(u_{j'})}{\sum_{ {j'}-1}^V exp(u_{j'})} \\& = & -\sum_{c = 1}^C u_{j_c^*} + C \cdot log \sum_{ {j'}-1}^V exp(u_{j'})\end{array}</script><h2 id="4-输出层-softmax-优化"><a href="#4-输出层-softmax-优化" class="headerlink" title="4 输出层 softmax 优化"></a>4 输出层 softmax 优化</h2><p>我们回顾 <code>word2vec</code> 算法，容易发现其在输出层为了预估词的概率，需要经过 sotmax 层。而，当词典空间很大的时候，<strong>softmax层容易成为整个算法的计算瓶颈</strong>。一般会有两种方法可以解决，<code>Hierarchical SoftMax</code> 和 <code>Negative Sampling</code>。</p><h3 id="4-1-Hierarchical-SoftMax"><a href="#4-1-Hierarchical-SoftMax" class="headerlink" title="4.1 Hierarchical SoftMax"></a>4.1 Hierarchical SoftMax</h3><p><code>哈夫曼（Huffman）树</code>是一种二叉树数据结构，基于其衍生的 <code>Hierarchical SoftMax</code> 能够有效地的降低 Softmax 的计算复杂度。我们首先介绍一下如何构建一颗哈夫曼（Huffman）树。</p><p>假设待构建的 n 个权值（一般是词频）为 ${w_1, w_2, \dots , w_n}$，可以通过以下步骤来构建 Huffman 树：</p><ol><li>将 ${w_1, w_2, \dots , w_n}$作为森林中 n 棵树的根节点；</li><li>选取森林中根权值最小的2棵树，分别作为左右子树合成新树，且新根节点的权值为左右子树根节点权值之和；</li><li>用新合成的数替换森林中原来的2个子树，重复上述过程直至仅剩一棵树。</li></ol><p>假设，有下面的一句：</p><blockquote><p>I love data science. I love big data. I am who I am.</p></blockquote><p>对应的词频表为：</p><div class="table-container"><table><thead><tr><th>word</th><th>code</th><th>freq</th><th>bits</th></tr></thead><tbody><tr><td>I</td><td>10</td><td>4</td><td>8</td></tr><tr><td>love</td><td>110</td><td>2</td><td>6</td></tr><tr><td>data</td><td>010</td><td>2</td><td>6</td></tr><tr><td>science</td><td>11110</td><td>1</td><td>5</td></tr><tr><td>big</td><td>11111</td><td>1</td><td>5</td></tr><tr><td>am</td><td>011</td><td>2</td><td>6</td></tr><tr><td>who</td><td>1110</td><td>1</td><td>4</td></tr><tr><td>.</td><td>00</td><td>3</td><td>6</td></tr></tbody></table></div><p>根据构建流程，实际上经过7次合并后完成 Huffman 树的构建：</p><ol><li>T1: (science, big) = 2</li><li>T2: (who, T1) = 3</li><li>T3: (data, am) = 4</li><li>T4: (love, T2) = 5</li><li>T5: (., T3) = 7</li><li>T6: (I, T4) = 9</li><li>T7: (T5, T6) = 15</li></ol><p>按照上述步骤构建完成的<code>Huffman树</code>一般如下图所示：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/word2vec4.png" alt="word2vec4"></p><p>可以发现：<strong>每个词都为树中的叶子节点，即高频词计算路径短，低频词计算路劲长</strong></p><p>剩下的步骤：</p><ol><li>将每个节点的选择构建成一个简单的二分类问题（比如LR），这样每个词语输入节点时都面临一个二项分布的问题。</li><li>通过Skip-Gram中“一预测多”的思想，利用根节点的输入词来预测多个输出的叶子节点，这样每个节点输出的概率为对应路径中的多项分布概率相乘。</li><li>最后遍历词典中的所有输入，完成整个数的节点参数确定，最后利用每个节点路径上的概率来形成对应单词的隐向量。</li></ol><p>由于在构建 Huffman 树的时候，保证了数的深度为 $log{|V|}$ ，因此在分层Softmax中只需做 $log{|V|}$ 次二分类即可求得最后预测单词的概率大小。</p><h3 id="4-2-Negative-Sampling"><a href="#4-2-Negative-Sampling" class="headerlink" title="4.2 Negative Sampling"></a>4.2 Negative Sampling</h3><p>除了上述的<code>分层Softmax</code>方法，另一个更为经典和常用的便是<code>Negative Sampling</code>（负采样）方法。它的核心思想是：</p><blockquote><p>放弃全局 Softmax 计算的过程，按照固定概率采样一定量的子集作为负例，从而转化成计算这些负例的sigmoid二分类过程，可以大大降低计算复杂度。</p></blockquote><script type="math/tex; mode=display">E = -\log{\sigma{(v_{w_o}^T h) } } - \sum_{w_j \in W_{neg } } \log{\sigma{(-v_{w_j}^T h) } }</script><p>上述便是新的 Loss 函数，公式中前者是 input 词，后部分为负采样得到的负样本词。容易发现，网络的计算空间从$|V|$降低到了$|w<em>O \cup W</em>{neg}|$。<strong>而这本质上是对训练集进行了采样，从而减小了训练集的大小。</strong></p><h2 id="5-问题思考"><a href="#5-问题思考" class="headerlink" title="5 问题思考"></a>5 问题思考</h2><h3 id="5-1-负采样方式"><a href="#5-1-负采样方式" class="headerlink" title="5.1 负采样方式"></a>5.1 负采样方式</h3><blockquote><p>算法的采样要求：高频词被采到的概率要大于低频词。<br>所以答案是非均匀采样，而是<code>带权采样</code>。</p></blockquote><p>之所以如此，是因为在大语料数据集中，有很多高频但信息量少的词，例如”the, a”等。对它们的下采样不仅可以加速还可以提高词向量的质量。为了平衡高低频词，一般采用如下权重：</p><script type="math/tex; mode=display">P(w_i) = 1 - \sqrt{\frac{t}{f(w_i) } }</script><p>其中，$f(w_i)$是单词$w_i$出现频率，参数$t$根据经验值一般取$10^{-5}$。如此可以确保频率超过$t$的词可以被欠采样，且不会影响原单词的频率相对大小。</p><h3 id="5-2-模型中两个embedding的取舍"><a href="#5-2-模型中两个embedding的取舍" class="headerlink" title="5.2 模型中两个embedding的取舍"></a>5.2 模型中两个embedding的取舍</h3><p>在word2vec模型的训练阶段，一般创建2个词表矩阵，<strong>Embedding 矩阵和 Context 矩阵</strong>。它们的大小都是 vocab_size x embedding_size，其中 vocab_size 是词表大小，embedding_size 是词向量维度。如下图所示：</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/word2vec5.png" alt="word2vec5"></p><blockquote><p>训练结束后，一般丢弃 Context 矩阵，并使用 Embeddings 矩阵作为下一项任务的已被训练好的嵌入。那为什么这么做呢？</p></blockquote><p>在 Stack Overflow 上的问题<a href="https://stackoverflow.com/questions/29381505/why-does-word2vec-use-2-representations-for-each-word">Why does word2vec use 2 representations for each word?</a>中有提到一个直觉性的解释，核心就是<strong>前者是中心词下的 embedding ，后者是 context 时下的 embedding ，他所表征的是两种不同的分布。</strong></p><p>实际上，个人在实际应用中做了一些试错，这里记录共享一下：</p><ol><li>使用中心词表 embedding 一定程度符合问题的定义：为了获取中心词 emb，从而采样 context 来构建样本训练的；</li><li>中心词和上下文词的 embedding 表都可以单独使用，但是不能交叉使用，比如2个近义词在两个独立词表中 emb 有意义，跨表 embedding 的关联性会失去；</li><li>从 <code>CBOW</code> 和 <code>Skip-gram</code> 的算法逻辑看，中心词和上下文词 embedding 实际上是一个角色互换。</li></ol><h3 id="5-3-CBOW-amp-Skip-Gram-的优劣"><a href="#5-3-CBOW-amp-Skip-Gram-的优劣" class="headerlink" title="5.3 CBOW &amp; Skip-Gram 的优劣"></a>5.3 CBOW &amp; Skip-Gram 的优劣</h3><p>先总结一下结论：</p><ul><li>当语料较少时使用 CBOW 方法比较好，当语料较多时采用 skip-gram 表示比较好。</li><li>Skip-gram 训练时间长，但是对低频词(生僻词)效果好；</li><li>CBOW 训练时间短，对低频词效果比较差。</li></ul><p>对于上述的结论貌似业界较为统一，但是对于这个结论的原理解释众说纷纭，这里个人觉得下面这种逻辑分析更为合理。首先注意 CBOW 和 skip-gram 的训练形式区别：</p><ul><li>CBOW 是使用周围词预测中心词，周围词的emb表是最终使用的。其对于每个中心词的一组采样样本训练的 gradient 会同时反馈给周围词上；</li><li>skip-gram 则相反，使用中心词预测周围词，中心词的 emb 表是最重使用的。那么中心词每组采样样本训练的 gradient 都会调整中心词的 emb；</li></ul><p>如上情况，可能会认为两者虽然中心词和上下文词虽然角色不一样，但只是互换了位置，训练的次数和结果理应差不多。然而，当默认使用 embedding 词表的时候，情况是不一样的:</p><blockquote><p><strong>skip-gram 的主词表中每个 emb 的训练次数多于 CBOW 的</strong>。</p></blockquote><p>因为，在 skip-gram 中，中心词的 emb 表是主词表，其每次会抽样 K 个上下文词，这保证了主词表对应的每个上下文词都训练到 K 次。<br>而 CBOW 则不同，因为其上下文词是主词表，中心词是用来训练上下文词的  emb，而由于采样概率的问题，虽然也会采样 K 个上下文词，但依然不能保证下文词对应的主词表的每个emb 都能够至少训练 K 次。</p><h3 id="5-4-拓展应用"><a href="#5-4-拓展应用" class="headerlink" title="5.4 拓展应用"></a>5.4 拓展应用</h3><p>个人在实际工作和应用中，深刻的感受到 word2vec 的强大绝不止于预训练词向量这么简单，其算法原理的思想才是核心，可以应用在很多地方，也深深影响着我自己。</p><p>这里总结几个应用的场景：</p><ol><li>最传统的便是nlp中文本词向量的预训练；</li><li>搜索推荐场景做 item2vec，可以用来输入精排或直接做向量召回；</li><li>召回/粗排模型样本的采样逻辑，如样本的负采样完全可以参考此逻辑。</li></ol><p><strong>参考文献</strong><br><a href="https://blog.csdn.net/weixin_39910711/article/details/103696103">机器学习算法（十三）：word2vec</a><br><a href="https://zhuanlan.zhihu.com/p/447776752">Embedding知识点 —— word2Vec详解</a><br><a href="https://blog.csdn.net/weixin_42279926/article/details/106403211">word2vec对each word使用两个embedding的原因</a><br><a href="https://mp.weixin.qq.com/s/oIxCPNXEUEvnjC0ESNQvCg">图解Word2vec</a><br><a href="https://zhuanlan.zhihu.com/p/26306795">NLP秒懂词向量Word2vec的本质</a><br><a href="https://blog.csdn.net/lanyu_01/article/details/80097350">《word2vec Parameter Learning Explained》论文学习笔记</a></p><hr>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-引言&quot;&gt;&lt;a href=&quot;#1-引言&quot; class=&quot;headerlink&quot; title=&quot;1 引言&quot;&gt;&lt;/a&gt;1 引言&lt;/h2&gt;&lt;p&gt;很多算法工程师认为 Embedding 技术是机器学习中最迷人的一种思想，在过去的近10年中，该技术在各个深度学习领域大放异彩。已经逐步进化到了近几年基于 BERT 和 GPT2 等模型的语境化嵌入。本文重点基于原始论文&lt;a href=&quot;https://arxiv.org/pdf/1301.3781v3.pdf&quot;&gt;Efficient Estimation of Word Representations in Vector Space&lt;/a&gt;，整理 word2vec 相关技术的基础原理和应用经验，旨在利于自己回溯巩固和他人参考学习。&lt;/p&gt;
&lt;p&gt;首先 Embedding 的思想是如何来的呢？我们知道计算机底层只能识别数字，并基于其进行逻辑等计算。而世间大多的实体或概念都不是以数据形式存在的，如何让计算机能够记住甚至理解是一件很难的事情。&lt;/p&gt;</summary>
    
    
    
    <category term="学习笔记" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="算法总结" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="算法" scheme="https://www.xiemingzhao.com/tags/%E7%AE%97%E6%B3%95/"/>
    
    <category term="word2vec" scheme="https://www.xiemingzhao.com/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>深度学习中的激活函数们</title>
    <link href="https://www.xiemingzhao.com/posts/activefunc.html"/>
    <id>https://www.xiemingzhao.com/posts/activefunc.html</id>
    <published>2020-06-17T16:00:00.000Z</published>
    <updated>2025-03-31T16:31:34.948Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1 背景"></a>1 背景</h2><p>本文参考多方资料总结了一下当前在深度模型中常遇到的几种激活函数。</p><p>在神经网络中，激活函数主要有两个用途：</p><ul><li>引入非线性</li><li>充分组合特征</li></ul><p>其中<strong>非线性</strong>激活函数允许网络复制复杂的非线性行为。正如绝大多数神经网络借助某种形式的梯度下降进行优化，激活函数需要是<strong>可微分</strong>（或者至少是几乎完全可微分的）。此外，复杂的激活函数也许产生一些梯度消失或爆炸的问题。因此，神经网络倾向于部署若干个特定的激活函数（identity、sigmoid、ReLU 及其变体）。<br>因此，神经网络中激励函数的作用通俗上讲就是将多个线性输入转换为非线性的关系。如果不使用激励函数的话，神经网络的每层都只是做线性变换，即使是多层输入叠加后也还是线性变换。通过激励函数引入非线性因素后，使神经网络的表达能力更强了。</p><span id="more"></span><h2 id="2-常见激活函数"><a href="#2-常见激活函数" class="headerlink" title="2 常见激活函数"></a>2 常见激活函数</h2><p>下面是多个激活函数的图示及其一阶导数，图的右侧是一些与神经网络相关的属性。</p><p><code>单调性（Montonic）</code>： 单调性使得在激活函数处的梯度方向不会经常改变，从而让训练更容易收敛</p><p><code>连续性（Continuous）</code>：个人认为作者想表达可微性，可微性保证了在优化中梯度的可计算性</p><p><code>非饱和性（saturation）</code>：饱和指的是在某些区间梯度接近于零（即梯度消失），使得参数无法继续更新的问题。</p><p>在深度神经网络中，前面层上的梯度是来自于后面层上梯度的乘乘积。当存在过多的层次时，就出现了内在本质上的不稳定场景，如<code>梯度消失</code>和<code>梯度爆炸</code></p><p><code>梯度消失（Vanishing Gradient）</code>：某些区间梯度接近于零；前面的层比后面的层梯度变化更小，故变化更慢，从而引起了梯度消失问题</p><p><code>梯度爆炸(Exploding Gradient)</code>:  某些区间梯度接近于无穷大或者权重过大；前面层比后面层梯度变化更快，会引起梯度爆炸问题</p><h3 id="2-1-Step"><a href="#2-1-Step" class="headerlink" title="2.1 Step"></a>2.1 Step</h3><p>它的函数和倒数表达式是：</p><script type="math/tex; mode=display">f(x)=\left\{\begin{matrix}1 \quad for \ x \ge 0 \\  0 \quad for \ x < 0\end{matrix}\right.</script><script type="math/tex; mode=display">f'(x)=\left\{\begin{matrix}0 \quad for \ x \ne 0 \\  ? \quad for \ x = 0\end{matrix}\right.</script><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/activefunc1.png" alt="activefunc1"></p><p>激活函数 Step 更倾向于理论而不是实际，它模仿了生物神经元要么全有要么全无的属性。它无法应用于神经网络，因为其导数是 0（除了零点导数无定义以外），这意味着基于梯度的优化方法并不可行。</p><h3 id="2-2-Identity"><a href="#2-2-Identity" class="headerlink" title="2.2 Identity"></a>2.2 Identity</h3><script type="math/tex; mode=display">Identity(x)=x</script><script type="math/tex; mode=display">Identity'(x)=1</script><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/activefunc2.png" alt="activefunc2"><br>通过激活函数 Identity，节点的输入等于输出。它完美适合于潜在行为是线性（与线性回归相似）的任务。当存在非线性，单独使用该激活函数是不够的，但它依然可以在最终输出节点上作为激活函数用于回归任务。</p><h3 id="2-3-ReLU"><a href="#2-3-ReLU" class="headerlink" title="2.3 ReLU"></a>2.3 ReLU</h3><script type="math/tex; mode=display">ReLU(x)=\left\{\begin{matrix}x \quad for \ x \ge 0 \\  0 \quad for \ x < 0\end{matrix}\right.</script><script type="math/tex; mode=display">ReLU'(x)=\left\{\begin{matrix}1 \quad for \ x \ge 0 \\  0 \quad for \ x < 0\end{matrix}\right.</script><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/activefunc3.png" alt="activefunc3"></p><p>修正线性单元（Rectified linear unit，ReLU）是神经网络中最常用的激活函数。</p><p>优点：<br>1，解决了gradient vanishing （梯度消失）问题（在正区间）<br>2，计算方便，求导方便，计算速度非常快，只需要判断输入是否大于0<br>3，收敛速度远远大于 Sigmoid函数和 tanh函数，可以加速网络训练</p><p>缺点：</p><ol><li>由于负数部分恒为零，会导致一些神经元无法激活</li><li>输出不是以0为中心</li></ol><p>缺点的致因：</p><ol><li>非常不幸的参数初始化，这种情况比较少见</li><li>learning rate 太高，导致在训练过程中参数更新太大，不幸使网络进入这种状态。</li></ol><p>另，<strong>ReLU 激活函数在零点不可导</strong>，求导按左导数来计算，是0。</p><h3 id="2-4-Sigmoid"><a href="#2-4-Sigmoid" class="headerlink" title="2.4 Sigmoid"></a>2.4 Sigmoid</h3><script type="math/tex; mode=display">Sig(x)=\frac{1}{1 + e^{-x}}</script><script type="math/tex; mode=display">Sig'(x)=Sig(x)(1-Sig(x))</script><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/activefunc4.png" alt="activefunc4"></p><p>Sigmoid 因其在 logistic 回归中的重要地位而被人熟知，值域在 0 到 1 之间。Logistic Sigmoid（或者按通常的叫法，Sigmoid）激活函数给神经网络引进了概率的概念。它的导数是非零的，并且很容易计算（是其初始输出的函数）。然而，在分类任务中，sigmoid 正逐渐被 Tanh 函数取代作为标准的激活函数，因为后者为奇函数（关于原点对称）。</p><p>主要是其有一些缺点：</p><ul><li>容易出现梯度弥散或者梯度饱和；</li><li>Sigmoid函数的output不是0均值（zero-centered）；</li><li>对其解析式中含有幂函数，计算机求解时相对比较耗时。</li></ul><h3 id="2-5-Tanh"><a href="#2-5-Tanh" class="headerlink" title="2.5 Tanh"></a>2.5 Tanh</h3><script type="math/tex; mode=display">tanh(x)=\frac{e^x - e^{-x}}{e^x + e^{-x}}</script><script type="math/tex; mode=display">tanh'(x)=1-tanh^2(x)</script><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/activefunc5.png" alt="activefunc5"></p><p>在分类任务中，双曲正切函数（Tanh）逐渐取代 Sigmoid 函数作为标准的激活函数，其具有很多神经网络所钟爱的特征。它是完全可微分的，反对称，对称中心在原点。输出均值是0，使得其收敛速度要比Sigmoid快，减少迭代次数。为了解决学习缓慢和/或梯度消失问题，可以使用这个函数的更加平缓的变体（log-log、softsign、symmetrical sigmoid 等等）.</p><h3 id="2-6-Leaky-ReLU"><a href="#2-6-Leaky-ReLU" class="headerlink" title="2.6 Leaky ReLU"></a>2.6 Leaky ReLU</h3><script type="math/tex; mode=display">LeakyReLU(x)=\left\{\begin{matrix}x \quad &for \ x \ge 0 \\  0.01 x \quad &for \ x < 0\end{matrix}\right.</script><script type="math/tex; mode=display">LeakyReLU'(x)=\left\{\begin{matrix}1 \quad &for \ x \ge 0 \\  0.01 \quad &for \ x < 0\end{matrix}\right.</script><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/activefunc6.png" alt="activefunc6"></p><p>经典（以及广泛使用的）ReLU 激活函数的变体，带泄露修正线性单元（Leaky ReLU）的输出对负值输入有很小的坡度。由于导数总是不为零，这能减少静默神经元的出现，允许基于梯度的学习（虽然会很慢）。</p><h3 id="2-7-PReLU"><a href="#2-7-PReLU" class="headerlink" title="2.7 PReLU"></a>2.7 PReLU</h3><script type="math/tex; mode=display">PReLU(x)=\left\{\begin{matrix}x \quad for \ x \ge 0 \\  \alpha x \quad for \ x < 0\end{matrix}\right.</script><script type="math/tex; mode=display">PReLU'(x)=\left\{\begin{matrix}1 \quad for \ x \ge 0 \\  \alpha \quad for \ x < 0\end{matrix}\right.</script><p><strong>其中$\alpha$一般取根据数据来确定.</strong></p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/activefunc7.png" alt="activefunc7"></p><p>参数化修正线性单元（Parameteric Rectified Linear Unit，PReLU）属于 ReLU 修正类激活函数的一员。它和 RReLU 以及 Leaky ReLU 有一些共同点，即为负值输入添加了一个线性项。而最关键的区别是，这个线性项的斜率实际上是在模型训练中学习到的。如果$\alpha$是一个很小的固定值（如 ai=0.01），则PReLU 退化为 Leaky ReLU（LReLU）。有实验证明：与 ReLU 相比，LReLU 对最终结果几乎没有什么影响。</p><h3 id="2-8-RReLU"><a href="#2-8-RReLU" class="headerlink" title="2.8 RReLU"></a>2.8 RReLU</h3><script type="math/tex; mode=display">RReLU(x_{ji})=\left\{\begin{matrix}x_{ji} \quad &for \ x_{ji} \ge 0 \\  \alpha_{ji} x_{ji} \quad &for \ x_{ji} < 0\end{matrix}\right.</script><p>where</p><script type="math/tex; mode=display">\alpha_{ji} \sim U(l,u),l<u \ and \ l,u \in [0,1)</script><script type="math/tex; mode=display">RReLU'(x)=\left\{\begin{matrix}1 \quad for \ x \ge 0 \\  \alpha \quad for \ x < 0\end{matrix}\right.</script><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/activefunc8.png" alt="activefunc8"></p><p>随机带泄露的修正线性单元（Randomized Leaky Rectified Linear Unit，RReLU 也是 Leaky ReLU的一个变体。在 PReLU中，负值的斜率在训练中是随机的，在之后的测试中就变成了固定的了。RReLU 的亮点在于，在训练环节中，aji 是从一个均匀的分布 U(I, u) 中随机抽取的数值。</p><p>这里我们要上一个经典的三者对比图：<br><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/activefunc9.png" alt="activefunc10"><br>其中 PReLU 中的 ai 是根据数据变换的；Leaky ReLU中的 ai 是固定的；RReLU中的 aji 是在一个给定的范围内随机抽取的值，这个值在测试环境就会固定下来。</p><h3 id="2-9-ELU"><a href="#2-9-ELU" class="headerlink" title="2.9 ELU"></a>2.9 ELU</h3><script type="math/tex; mode=display">ELU(x)=\left\{\begin{matrix}x \quad &for \ x \ge 0 \\  \alpha (e^x-1) \quad &for \ x < 0\end{matrix}\right.</script><script type="math/tex; mode=display">ELU'(x)=\left\{\begin{matrix}1 \quad &for \ x \ge 0 \\  \alpha e^x \quad &for \ x < 0\end{matrix}\right.</script><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/activefunc10.png" alt="activefunc10"></p><p>指数线性单元（Exponential Linear Unit，ELU）也属于 ReLU 修正类激活函数的一员。和 PReLU 以及 RReLU 类似，为负值输入添加了一个非零输出。和其它修正类激活函数不同的是，它包括一个负指数项，从而防止静默神经元出现，导数收敛为零，从而提高学习效率。<br>根据一些研究，ELUs 分类精确度是高于 ReLUs的。ELU在正值区间的值为x本身，这样减轻了梯度弥散问题（x&gt;0区间导数处处为1），这点跟ReLU、Leaky ReLU相似。而在负值区间，ELU在输入取较小值类似于 Leaky ReLU ，理论上虽然好于 ReLU，但是实际使用中目前并没有好的证据 ELU 总是优于 ReLU。时具有软饱和的特性，提升了对噪声的鲁棒性。类似于 Leaky ReLU ，理论上虽然好于 ReLU，但是实际使用中目前并没有好的证据 ELU 总是优于 ReLU。</p><h3 id="2-10-SELU"><a href="#2-10-SELU" class="headerlink" title="2.10 SELU"></a>2.10 SELU</h3><script type="math/tex; mode=display">SELU(x)=\lambda\left\{\begin{matrix}x \quad &for \ x \ge 0 \\  \alpha (e^x-1) \quad &for \ x < 0\end{matrix}\right.</script><script type="math/tex; mode=display">with \lambda=1.0507, \alpha=1.67326</script><script type="math/tex; mode=display">SELU'(x)=\left\{\begin{matrix}\lambda \quad &for \ x \ge 0 \\  \lambda \alpha e^x \\ =\lambda(SELU(x)+\alpha) \quad &for \ x < 0\end{matrix}\right.</script><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/activefunc11.png" alt="activefunc11"><br>扩展指数线性单元（Scaled Exponential Linear Unit，SELU）是激活函数指数线性单元（ELU）的一个变种。其中λ和α是固定数值（分别为 1.0507 和 1.6726）。这些值背后的推论（零均值/单位方差）构成了自归一化神经网络的基础（SNN）。值看似是乱讲的，实际上是作者推导出来的，详情也可以看<a href="https://github.com/bioinf-jku/SNNs">作者的github</a>。</p><h3 id="2-11-SReLU"><a href="#2-11-SReLU" class="headerlink" title="2.11 SReLU"></a>2.11 SReLU</h3><script type="math/tex; mode=display">SReLU(x)=\left\{\begin{matrix}t_l + a_l(x-t_l) &for \ x \le t_l \\  x &for \ t_l < x < t_r \\ t_r + a_r(x - t_r) &for \ x \ge t_r\end{matrix}\right.</script><script type="math/tex; mode=display">SReLU'(x)=\left\{\begin{matrix}a_l &for \ x \le t_l \\  1 &for \ t_l < x < t_r \\a_r &for \ x \ge t_r\end{matrix}\right.</script><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/activefunc12.png" alt="activefunc12"><br>S 型整流线性激活单元（S-shaped Rectified Linear Activation Unit，SReLU）属于以 ReLU 为代表的整流激活函数族。它由三个分段线性函数组成。其中两种函数的斜度，以及函数相交的位置会在模型训练中被学习。</p><h3 id="2-12-Hard-Sigmoid"><a href="#2-12-Hard-Sigmoid" class="headerlink" title="2.12 Hard Sigmoid"></a>2.12 Hard Sigmoid</h3><script type="math/tex; mode=display">Hard Sigmoid(x)=max(0, min(1, \frac{x+1}{2}))</script><script type="math/tex; mode=display">Hard Sigmoid'(x)=\left\{\begin{matrix}0 &for \ x \le t_l \\  0.5 &for \ t_l < x < t_r \\0 &for \ x \ge t_r\end{matrix}\right.</script><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/activefunc13.png" alt="activefunc13"><br>Hard Sigmoid 是 Logistic Sigmoid 激活函数的分段线性近似。它更易计算，这使得学习计算的速度更快，尽管首次派生值为零可能导致静默神经元/过慢的学习速率（详见 ReLU）。</p><h3 id="2-13-Hard-Tanh"><a href="#2-13-Hard-Tanh" class="headerlink" title="2.13 Hard Tanh"></a>2.13 Hard Tanh</h3><script type="math/tex; mode=display">Hard Tanh(x)=\left\{\begin{matrix}-1 &for \ x < -1 \\  x &for \ -1 \le x \le 1 \\1 &for \ x > 1\end{matrix}\right.</script><script type="math/tex; mode=display">Hard Tanh'(x)=\left\{\begin{matrix}0 &for \ x < -1 \\  1 &for \ -1 \le x \le 1 \\0 &for \ x > 1\end{matrix}\right.</script><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/activefunc14.png" alt="activefunc14"></p><p>Hard Tanh 是 Tanh 激活函数的线性分段近似。相较而言，它更易计算，这使得学习计算的速度更快，尽管首次派生值为零可能导致静默神经元/过慢的学习速率（详见 ReLU）。</p><h3 id="2-14-LeCun-Tanh"><a href="#2-14-LeCun-Tanh" class="headerlink" title="2.14 LeCun Tanh"></a>2.14 LeCun Tanh</h3><script type="math/tex; mode=display">LeCun Tanh(x)=1.7519tanh(\frac{2}{3} x)</script><script type="math/tex; mode=display">LeCun Tanh'(x)=1.7519 * \frac{2}{3} (1 - tanh^2(\frac{2}{3} x))</script><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/activefunc15.png" alt="activefunc15"></p><p>LeCun Tanh（也被称作 Scaled Tanh）是 Tanh 激活函数的扩展版本。它具有以下几个可以改善学习的属性：f(± 1) = ±1；二阶导数在 x=1 最大化；且有效增益接近 1。</p><h3 id="2-15-ArcTan"><a href="#2-15-ArcTan" class="headerlink" title="2.15 ArcTan"></a>2.15 ArcTan</h3><script type="math/tex; mode=display">ArcTan(x)=tan^{-1}(x)</script><script type="math/tex; mode=display">ArcTan'(x)=\frac{1}{x^2 + 1}</script><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/activefunc16.png" alt="activefunc16"><br>视觉上类似于双曲正切（Tanh）函数，ArcTan 激活函数更加平坦，这让它比其他双曲线更加清晰。在默认情况下，其输出范围在-π/2 和π/2 之间。其导数趋向于零的速度也更慢，这意味着学习的效率更高。但这也意味着，导数的计算比 Tanh 更加昂贵。</p><h3 id="2-16-Softsign"><a href="#2-16-Softsign" class="headerlink" title="2.16 Softsign"></a>2.16 Softsign</h3><script type="math/tex; mode=display">Softsign(x)=\frac{x}{1 + |x|}</script><script type="math/tex; mode=display">Softsign'(x)=\frac{1}{(|x| + 1)^2}</script><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/activefunc17.png" alt="activefunc17"></p><p>Softsign 是 Tanh 激活函数的另一个替代选择。就像 Tanh 一样，Softsign 是反对称、去中心、可微分，并返回-1 和 1 之间的值。其更平坦的曲线与更慢的下降导数表明它可以更高效地学习。另一方面，导数的计算比 Tanh 更麻烦。</p><h3 id="2-17-SoftPlus"><a href="#2-17-SoftPlus" class="headerlink" title="2.17 SoftPlus"></a>2.17 SoftPlus</h3><script type="math/tex; mode=display">SoftPlus(x)=ln(1 + e^x)</script><script type="math/tex; mode=display">SoftPlus'(x)=\frac{1}{1 + e ^{-x}}</script><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/activefunc18.png" alt="activefunc18"></p><p>作为 ReLU 的一个不错的替代选择，SoftPlus 能够返回任何大于 0 的值。与 ReLU 不同，SoftPlus 的导数是连续的、非零的，无处不在，从而防止出现静默神经元。然而，SoftPlus 另一个不同于 ReLU 的地方在于其不对称性，不以零为中心，这兴许会妨碍学习。此外，由于导数常常小于 1，也可能出现梯度消失的问题。</p><h3 id="2-18-Signum"><a href="#2-18-Signum" class="headerlink" title="2.18 Signum"></a>2.18 Signum</h3><script type="math/tex; mode=display">Signum(x)=\left\{\begin{matrix}1 &for \ x > 0 \\  -1 &for \ x < 0 \\0 &for \ x = 0\end{matrix}\right.</script><script type="math/tex; mode=display">Signum'(x)=\left\{\begin{matrix}0 &for \ x \ne 0 \\  ? &for \ x = 0\end{matrix}\right.</script><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/activefunc19.png" alt="activefunc19"></p><p>激活函数 Signum（或者简写为 Sign）是二值阶跃激活函数的扩展版本。它的值域为 [-1,1]，原点值是 0。尽管缺少阶跃函数的生物动机，Signum 依然是反对称的，这对激活函数来说是一个有利的特征。</p><h3 id="2-19-Bent-Identity"><a href="#2-19-Bent-Identity" class="headerlink" title="2.19 Bent Identity"></a>2.19 Bent Identity</h3><script type="math/tex; mode=display">Bent Identity(x)=\frac{\sqrt{x ^ 2 + 1} - 1}{2} + x</script><script type="math/tex; mode=display">Bent Identity'(x)=\frac{x}{2 \sqrt{x ^ 2 + 1}} + 1</script><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/activefunc20.png" alt="activefunc20"><br>激活函数 Bent Identity 是介于 Identity 与 ReLU 之间的一种折衷选择。它允许非线性行为，尽管其非零导数有效提升了学习并克服了与 ReLU 相关的静默神经元的问题。由于其导数可在 1 的任意一侧返回值，因此它可能容易受到梯度爆炸和消失的影响。</p><h3 id="2-20-Symmetrical-Sigmoid"><a href="#2-20-Symmetrical-Sigmoid" class="headerlink" title="2.20 Symmetrical Sigmoid"></a>2.20 Symmetrical Sigmoid</h3><script type="math/tex; mode=display">Symmetrical Sigmoid(x)=tanh(x/2)=\frac{1 - e^{-x}}{1 + e^{-x}}</script><script type="math/tex; mode=display">Symmetrical Sigmoid'(x)=0.5 (1 - tanh^2(x/2))</script><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/activefunc21.png" alt="activefunc21"><br>Symmetrical Sigmoid 是另一个 Tanh 激活函数的变种（实际上，它相当于输入减半的 Tanh）。和 Tanh 一样，它是反对称的、零中心、可微分的，值域在 -1 到 1 之间。它更平坦的形状和更慢的下降派生表明它可以更有效地进行学习。</p><h3 id="2-21-Log-Log"><a href="#2-21-Log-Log" class="headerlink" title="2.21 Log Log"></a>2.21 Log Log</h3><script type="math/tex; mode=display">Log Log(x)=1 - e^{-e^x}</script><script type="math/tex; mode=display">Log Log'(x)=e^x(e^{-e^x})=e^{x-e^x}</script><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/activefunc22.png" alt="activefunc22"><br>Log Log 激活函数（由上图 f(x) 可知该函数为以 e 为底的嵌套指数函数）的值域为 [0,1]，Complementary Log Log 激活函数有潜力替代经典的 Sigmoid 激活函数。该函数饱和地更快，且零点值要高于 0.5。</p><h3 id="2-22-Gaussian"><a href="#2-22-Gaussian" class="headerlink" title="2.22 Gaussian"></a>2.22 Gaussian</h3><script type="math/tex; mode=display">Gaussian(x)=e^{-x^2}</script><script type="math/tex; mode=display">Gaussian'(x)=-2xe^{-x^2}</script><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/activefunc23.png" alt="activefunc23"><br>高斯激活函数（Gaussian）并不是径向基函数网络（RBFN）中常用的高斯核函数，高斯激活函数在多层感知机类的模型中并不是很流行。该函数处处可微且为偶函数，但一阶导会很快收敛到零。</p><h3 id="2-23-Absolute"><a href="#2-23-Absolute" class="headerlink" title="2.23 Absolute"></a>2.23 Absolute</h3><script type="math/tex; mode=display">Absolute(x)=|x|</script><script type="math/tex; mode=display">Absolute'(x)=\left\{\begin{matrix}-1 &for \ x < 0 \\  ? &for \ x = 0 \\1 &for x > 0\end{matrix}\right.</script><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/activefunc24.png" alt="activefunc24"><br>顾名思义，绝对值（Absolute）激活函数返回输入的绝对值。该函数的导数除了零点外处处有定义，且导数的量值处处为 1。这种激活函数一定不会出现梯度爆炸或消失的情况。</p><h3 id="2-24-Sinusoid"><a href="#2-24-Sinusoid" class="headerlink" title="2.24 Sinusoid"></a>2.24 Sinusoid</h3><script type="math/tex; mode=display">Sinusoid(x)=sin(x)</script><script type="math/tex; mode=display">Sinusoid'(x)=cos(x)</script><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/activefunc25.png" alt="activefunc25"></p><p>如同余弦函数，Sinusoid（或简单正弦函数）激活函数为神经网络引入了周期性。该函数的值域为 [-1,1]，且导数处处连续。此外，Sinusoid 激活函数为零点对称的奇函数。</p><h3 id="2-25-Cos"><a href="#2-25-Cos" class="headerlink" title="2.25 Cos"></a>2.25 Cos</h3><script type="math/tex; mode=display">Cos(x)=cos(x)</script><script type="math/tex; mode=display">Cos'(x)=sin(x)</script><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/activefunc26.png" alt="activefunc26"></p><p>如同正弦函数，余弦激活函数（Cos/Cosine）为神经网络引入了周期性。它的值域为 [-1,1]，且导数处处连续。和 Sinusoid 函数不同，余弦函数为不以零点对称的偶函数。</p><h3 id="2-26-Sinc"><a href="#2-26-Sinc" class="headerlink" title="2.26 Sinc"></a>2.26 Sinc</h3><script type="math/tex; mode=display">Sinc(x)=\left\{\begin{matrix}1 &for \ x = 0 \\\frac{sin(x)}{x} &for x \ne 0\end{matrix}\right.</script><script type="math/tex; mode=display">Sinc'(x)=\left\{\begin{matrix}0 &for \ x = 0 \\\frac{cos(x)}{x} - \frac{sin(x)}{x^2} &for x \ne 0\end{matrix}\right.</script><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/activefunc27.png" alt="activefunc27"><br>Sinc 函数（全称是 Cardinal Sine）在信号处理中尤为重要，因为它表征了矩形函数的傅立叶变换（Fourier transform）。作为一种激活函数，它的优势在于处处可微和对称的特性，不过它比较容易产生梯度消失的问题。</p><p><strong>参考文章</strong><br><a href="https://www.cnblogs.com/wj-1314/p/12015278.html">深度学习笔记——常用的激活（激励）函数</a><br><a href="https://dashee87.github.io/deep%20learning/visualising-activation-functions-in-neural-networks/">Visualising Activation Functions in Neural Networks</a></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-背景&quot;&gt;&lt;a href=&quot;#1-背景&quot; class=&quot;headerlink&quot; title=&quot;1 背景&quot;&gt;&lt;/a&gt;1 背景&lt;/h2&gt;&lt;p&gt;本文参考多方资料总结了一下当前在深度模型中常遇到的几种激活函数。&lt;/p&gt;
&lt;p&gt;在神经网络中，激活函数主要有两个用途：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;引入非线性&lt;/li&gt;
&lt;li&gt;充分组合特征&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其中&lt;strong&gt;非线性&lt;/strong&gt;激活函数允许网络复制复杂的非线性行为。正如绝大多数神经网络借助某种形式的梯度下降进行优化，激活函数需要是&lt;strong&gt;可微分&lt;/strong&gt;（或者至少是几乎完全可微分的）。此外，复杂的激活函数也许产生一些梯度消失或爆炸的问题。因此，神经网络倾向于部署若干个特定的激活函数（identity、sigmoid、ReLU 及其变体）。&lt;br&gt;因此，神经网络中激励函数的作用通俗上讲就是将多个线性输入转换为非线性的关系。如果不使用激励函数的话，神经网络的每层都只是做线性变换，即使是多层输入叠加后也还是线性变换。通过激励函数引入非线性因素后，使神经网络的表达能力更强了。&lt;/p&gt;</summary>
    
    
    
    <category term="学习笔记" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="算法总结" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="深度学习" scheme="https://www.xiemingzhao.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="激活函数" scheme="https://www.xiemingzhao.com/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>布谷鸟过滤器(Cuckcoo Filter)</title>
    <link href="https://www.xiemingzhao.com/posts/cuckooFilter.html"/>
    <id>https://www.xiemingzhao.com/posts/cuckooFilter.html</id>
    <published>2020-05-28T16:00:00.000Z</published>
    <updated>2025-03-31T16:42:01.792Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1 背景"></a>1 背景</h2><p>在了解了布隆过滤器的缺点之后，如果想要解决就可以来学习一下布谷鸟过滤器。其最早是在2001年的论文<a href="https://www.cs.cmu.edu/~dga/papers/cuckoo-conext2014.pdf">《Cuckoo Filter: Practically Better Than Bloom》</a>中提出的。论文中也很直接的抨击布隆过滤器的缺点，表明自己可以有效支持反向删除操作。</p><h2 id="2-原理"><a href="#2-原理" class="headerlink" title="2 原理"></a>2 原理</h2><p>先来介绍最简单的<code>布谷鸟过滤器</code>的工作原理。假设我们有：</p><ul><li>两个Hash表，T1和T2；</li><li>两个Hash函数，H1和H2。</li></ul><span id="more"></span><p>当一个不存在的元素需要进行插入的时候：</p><ol><li>先使用H1计算出其在T1的位置，如果空就插入；</li><li>如果不空，就再利用H2计算其在T2的位置，如果空就放入；</li><li>如果依然不空，那就<code>鸠占鹊巢</code>，把当前位置的元素踢出去，再把待插入的元素插入即可。这也是布谷鸟过滤器名称的来源。</li></ol><p>以上就是布谷鸟过滤器的主要原理，看到这里肯定各种疑问:</p><blockquote><p>被踢出的怎么办？会不会存在循环踢出？等等问题，研究人员也都进行了解决。</p></blockquote><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/cuckooFilter1.png" alt="cuckooFilter1"></p><p>首先，不同于布谷鸟的是，布谷鸟哈希算法会帮这些受害者（被挤走的数据）寻找其它的位置。参考上面论文中的图(a)：</p><blockquote><p>当x想要插入的时候，发现在T1和T2中对应的位置都存在了元素，那就随机把 T1 表对应位置上的 y 踢出去吧，而 y 的另一个位置被 z 元素占领了。于是 y 毫不留情把 z 也踢了出去。z 发现自己的备用位置还空着（虽然这个备用位置也是元素 v 的备用位置），赶紧就位。</p></blockquote><p>经过上述的插入过程后，整个数据结构就从(a)图中的左变成了右。这种类似于套娃的解决方式看是可行，但是总是有<code>出现循环踢出</code>导致放不进 x 的问题。比如上图中的(b)。</p><p>当遇到这种情况时候，说明<strong>布谷鸟 hash 已经到了极限情况，应该进行扩容，或者 hash 函数的优化</strong>。所以，你再次去看伪代码的时候，你会明白里面的 <code>MaxLoop</code> 的含义是什么了。这个 <code>MaxLoop</code> 的含义就是<strong>为了避免相互踢出的这个过程执行次数太多，设置的一个阈值</strong>。</p><h2 id="3-优化"><a href="#3-优化" class="headerlink" title="3 优化"></a>3 优化</h2><h3 id="3-1-思路"><a href="#3-1-思路" class="headerlink" title="3.1 思路"></a>3.1 思路</h3><p>看过上述的原理，相信依然对其性能和效果存在质疑。这里首先抛出论文中提到的短处。</p><blockquote><p>虽然<code>MaxLoop</code>能够避免太多循环踢出，但是这使得在完美的情况下，也就是没有发生哈希冲突之前，它的空间利用率最高只有 50%。</p></blockquote><p>对于上述的问题，一般会有下面的优化方法：</p><ol><li>增加 hash 函数，这样可以大大降低碰撞的概率，将空间利用率提高到 95% 左右。</li><li>在数组的每个位置上挂上多个座位，这样即使两个元素被 hash 在了同一个位置，也可以随意放一个。这种方案的空间利用率只有 85% 左右，但是查询效率会很高。</li></ol><h3 id="3-2-特殊的-hash-函数"><a href="#3-2-特殊的-hash-函数" class="headerlink" title="3.2 特殊的 hash 函数"></a>3.2 特殊的 hash 函数</h3><p>论文在实际的优化中，一个很重要的就是构建了特殊的 Hash 函数。回忆一下布谷鸟 Hash，它存储的是插入元素的原始值，比如 x 会经过两个 hash 函数，如果我们记数组的长度为 L，那么就是这样的：</p><ul><li>p1 = H1(x) % L</li><li>p2 = H2(x) % L</li></ul><p>而布谷鸟过滤器计算位置时候实际上是：</p><ul><li>H1(x) = Hash(x)</li><li>H2(x) = H1(x) $\oplus$ Hash(x’s fingerprint)</li></ul><p>可以看到，虽然有两个Hash函数，<strong>但实际上内部只有一个Hash函数构成</strong>，在H2中使用了H1和待插入元素 x 的指纹Hash结果，然后再做异或运算。</p><p><code>指纹</code>其实就是插入的元素进行一个 Hash 计算，而 Hash 计算的产物就是 几个 bit 位。<strong>布谷鸟过滤器里面存储的就是元素的“指纹”。删除数据的时候，也只是抹掉该位置上的“指纹”而已</strong>。</p><p><strong>注意：异或运算确保了一个重要的性质，这两个位置具有对偶性</strong>。</p><blockquote><p>只要保证 Hash(x’s fingerprint) !=0，那么就可以确保 H2!=H1，也就可以确保，不会出现自己踢自己的死循环问题。</p></blockquote><p><em>为什么要对“指纹”进行一个 hash 计算之后再进行异或运算呢？</em></p><blockquote><p>如果不进行 hash 计算，假设“指纹”的长度是 8bit，那么其对偶位置算出来，距离当前位置最远也才 256。所以，对“指纹”进行哈希处理可确保被踢出去的元素，可以重新定位到哈希表中完全不同的存储桶中，从而减少哈希冲突并提高表利用率。</p></blockquote><p><em>它没有对数组的长度进行取模，那么它怎么保证计算出来的下标一定是落在数组中的呢？</em></p><blockquote><p>其强制数组的长度必须是 2 的指数倍。一定是这样的：10000000…（n个0）。这个限制带来的好处就是，进行异或运算时，可以保证计算出来的下标一定是落在数组中的。</p></blockquote><h3 id="3-3-空间利用率"><a href="#3-3-空间利用率" class="headerlink" title="3.3 空间利用率"></a>3.3 空间利用率</h3><p>由于是对元素进行 hash 计算，那么必然会出现“指纹”相同的情况，也就是会出现误判的情况。没有存储原数据，所以牺牲了数据的准确性，但是只保存了几个 bit，因此提升了空间效率。</p><p>在完美的情况下，也就是没有发生哈希冲突之前，它的空间利用率最高只有 50%。因为没有发生冲突，说明至少有一半的位置是空着的。这个比率还是很低的，前面提到了优化方案，论文中也是基于只有2个Hash的条件下来进行优化的，也即<strong>增加数组的维度</strong>。</p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/cuckooFilter2.png" alt="cuckooFilter2"></p><p>参考上面论文中的图(c)，我们可以发现对数组进行展开，从一维变成了二维，这样每个位置可以放4个元素了。如此，论文中表述到，当 hash 函数固定为 2 个的时候，如果一个下标只能放一个元素，那么空间利用率是 50%。</p><blockquote><p>但是如果一个下标可以放 2，4，8 个元素的时候，空间利用率就会飙升到 84%，95%，98%。</p></blockquote><h3 id="3-4-最后的弊端"><a href="#3-4-最后的弊端" class="headerlink" title="3.4 最后的弊端"></a>3.4 最后的弊端</h3><p>布谷鸟过滤器整个了解下来，看起来一切都是这么的完美。各项指标都比布隆过滤器好，<strong>主打的是支持删除的操作</strong>。但实际上其依然存在不小的弊端。</p><blockquote><p>对重复数据进行限制：如果需要布谷鸟过滤器支持删除，它必须知道一个数据插入过多少次。不能让同一个数据插入 kb+1 次。其中 k 是 Hash 函数的个数，b 是一个下标的位置能放几个元素。</p></blockquote><p>举例：比如 2 个 hash 函数，一个二维数组，它的每个下标最多可以插入 4 个元素。那么对于同一个元素，最多支持插入 8 次。<br><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/cuckooFilter3.png" alt="cuckooFilter3"></p><p>why 已经插入了 8 次了，如果再次插入一个 why，则会出现循环踢出的问题，直到最大循环次数，然后返回一个 false。想要避免这个问题，就需要维护一个记录表，记录每个元素插入的次数就行了。但是这成本一下子就大了起来。</p><p>虽然布谷鸟过滤器也不算完美无暇，但是从技术上和实际应用上看，这无异又是人类的一次技术迭代。下面的图是各种过滤器的指标对比。</p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/cuckooFilter4.png" alt="cuckooFilter4"></p><p>补充一个冷知识，虽然布谷鸟过滤器在2001年就被大佬 <code>Burton Howard Bloom</code> 提出来了，不少海内外博主一直想要看看这位大佬是和容颜。但是，海内外全网都没有，是一个彻彻底底的低调技术大佬。</p><p><strong>参考文章</strong><br><a href="https://segmentfault.com/a/1190000039156246">布隆，牛逼！布谷鸟，牛逼！</a><br><a href="https://www.163.com/dy/article/G55C599D05372639.html">聊聊Redis布隆过滤器与布谷鸟过滤器？一文避坑</a></p><hr>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-背景&quot;&gt;&lt;a href=&quot;#1-背景&quot; class=&quot;headerlink&quot; title=&quot;1 背景&quot;&gt;&lt;/a&gt;1 背景&lt;/h2&gt;&lt;p&gt;在了解了布隆过滤器的缺点之后，如果想要解决就可以来学习一下布谷鸟过滤器。其最早是在2001年的论文&lt;a href=&quot;https://www.cs.cmu.edu/~dga/papers/cuckoo-conext2014.pdf&quot;&gt;《Cuckoo Filter: Practically Better Than Bloom》&lt;/a&gt;中提出的。论文中也很直接的抨击布隆过滤器的缺点，表明自己可以有效支持反向删除操作。&lt;/p&gt;
&lt;h2 id=&quot;2-原理&quot;&gt;&lt;a href=&quot;#2-原理&quot; class=&quot;headerlink&quot; title=&quot;2 原理&quot;&gt;&lt;/a&gt;2 原理&lt;/h2&gt;&lt;p&gt;先来介绍最简单的&lt;code&gt;布谷鸟过滤器&lt;/code&gt;的工作原理。假设我们有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;两个Hash表，T1和T2；&lt;/li&gt;
&lt;li&gt;两个Hash函数，H1和H2。&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="学习笔记" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="算法总结" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="算法" scheme="https://www.xiemingzhao.com/tags/%E7%AE%97%E6%B3%95/"/>
    
    <category term="数据结构" scheme="https://www.xiemingzhao.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>布隆过滤器(Bloom Filter)</title>
    <link href="https://www.xiemingzhao.com/posts/bloomFilter.html"/>
    <id>https://www.xiemingzhao.com/posts/bloomFilter.html</id>
    <published>2020-05-23T16:00:00.000Z</published>
    <updated>2025-03-31T16:39:27.048Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1 背景"></a>1 背景</h2><p>在实际工作中，我们经常涉及判断一个对象或者数据是否存在于内存或者数据库。往往大家会想到HashMap，但是这时候有一个问题,存储容量占比高，考虑到负载因子的存在，通常空间是不能被用满的，而一旦你的值很多例如上亿的时候，可行性就差了。<br>另一方面，如果很多请求是在请求数据库根本不存在的数据,那么数据库就要频繁响应这种不必要的IO查询,如果再多一些,数据库大多数IO都在响应这种毫无意义的请求操作,为了解决这一个问题，过滤器由此诞生！</p><h2 id="2-布隆过滤器"><a href="#2-布隆过滤器" class="headerlink" title="2 布隆过滤器"></a>2 布隆过滤器</h2><blockquote><p>过滤原理：布隆过滤器(Bloom Filter)大概的思路就是，当你请求的信息来的时候，先检查一下你查询的数据我这有没有，有的话将请求压给数据库,没有的话直接返回。</p></blockquote><span id="more"></span><p><strong>布隆过滤器是一个 bit 向量或者说 bit 数组</strong>。</p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/bloomFilter1.jpg" alt="bloomFilter1"></p><p>如图，一个<code>bitmap</code>用于记录，bitmap原始数值全都是0。<br>当一个数据存进来的时候：</p><ul><li>用三个Hash函数分别计算三次Hash值，并且将bitmap对应的位置设置为1，上图中 bitmap 的1,3,6位置被标记为1；</li><li>这时候如果一个数据请求过来，依然用之前的三个Hash函数计算Hash值，如果是同一个数据的话，势必依旧是映射到1，3，6位，那么就可以判断这个数据之前存储过；</li><li>如果新的数据映射的三个位置，有一个匹配不上，加入映射到1,3,7位，由于7位是0，也就是这个数据之前并没有加入进数据库，所以直接返回。</li></ul><h2 id="3-存在的问题"><a href="#3-存在的问题" class="headerlink" title="3 存在的问题"></a>3 存在的问题</h2><h3 id="3-1-误判"><a href="#3-1-误判" class="headerlink" title="3.1 误判"></a>3.1 误判</h3><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/bloomFilter2.jpg" alt="bloomFilter2"></p><p>如上图所示，假如有这么一个情景，放入数据包1，将bitmap的1,3,6位设置为了1。放入数据包2时将bitmap的3,6,7位设置为了1，此时一个并没有存过的数据包请求3。做三次哈希之后，对应的bitmap位点分别是1,6,7。这个数据之前并没有存进去过，但是由于数据包1和2存入时将对应的点设置为了1，所以请求3也会压到数据库上，这种情况会随着存入的数据增加而增加。</p><p>所以，布隆过滤器只能够得出<strong>两种结论</strong>：</p><ul><li>当hash对应的位置出现0的时候，就表明一定不存在；</li><li>当全是1的时候，由于误判的可能，只能表明可能存在。</li></ul><h3 id="3-2-无法删除"><a href="#3-2-无法删除" class="headerlink" title="3.2 无法删除"></a>3.2 无法删除</h3><p><strong>布隆过滤器无法删除的原因有二</strong>：</p><ol><li>由于有误判的可能，并不确定数据是否存在数据库里，例如数据包3。</li><li>当你删除某一个数据包对应位图上的标志后，可能影响其他的数据包。</li></ol><blockquote><p>例如上面例子中，如果删除数据包1，也就意味着会将 bitmap 1,3,6位设置为0。此时数据包2来请求时，会显示不存在，因为3,6两位已经被设置为0。</p></blockquote><p>为此还出现了一个<code>改进版的布隆过滤器</code>，即 <code>Counting Bloom filter</code>，可以用来测试元素计数个数是否绝对小于某个阈值，如下图所示。</p><p>这个过滤器的思路：将布隆过滤器的bitmap更换成数组，当数组某位置被映射一次时就+1，当删除时就-1，这样就避免了普通布隆过滤器删除数据后需要重新计算其余数据包Hash的问题。</p><p>但实际上也无法解决删除的问题，原因是由于一开始就存在误判的可能，如果在删除的时候，一个本来不存在的由于误判而进行了删除，就会使得其他原本正确的出现错误计数。</p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/bloomFilter3.jpg" alt="bloomFilter3"></p><p><em>这个问题造就了其软肋，布隆过滤器就好比是印迹，来过来就会有痕迹，就算走了也无法清理干净</em>。</p><p>比如你的系统里本来只留下 1kw 个元素，但是整体上来过了上亿的流水元素，布隆过滤器很无奈，它会将这些流失的元素的印迹也会永远存放在那里。随着时间的流失，这个过滤器会越来越拥挤，直到有一天你发现它的误判率太高了，不得不进行重建。</p><h3 id="3-3-其他问题"><a href="#3-3-其他问题" class="headerlink" title="3.3 其他问题"></a>3.3 其他问题</h3><p><strong>查询性能弱</strong><br>是因为布隆过滤器需要使用多个 hash 函数探测位图中多个不同的位点，这些位点在内存上跨度很大，会导致 CPU 缓存行命中率低。</p><p><strong>空间效率低</strong><br>是因为在相同的误判率下，布谷鸟过滤器的空间利用率要明显高于布隆，空间上大概能节省 40% 多。不过布隆过滤器并没有要求位图的长度必须是 2 的指数，而布谷鸟过滤器必须有这个要求。从这一点出发，似乎布隆过滤器的空间伸缩性更强一些。</p><h3 id="3-4-参数选择"><a href="#3-4-参数选择" class="headerlink" title="3.4 参数选择"></a>3.4 参数选择</h3><p>布隆过滤器在构建时，有<strong>两个重要的参数</strong>：</p><ul><li>一个是Hash函数的个数 k；</li><li>另一个是 bit 数组的大小 m。</li></ul><p>过小的布隆过滤器很快所有的 bit 位均为 1，那么查询任何值都会返回“可能存在”，起不到过滤的目的了。布隆过滤器的长度会直接影响误报率，布隆过滤器越长其误报率越小。<br>另外，哈希函数的个数也需要权衡，个数越多则布隆过滤器 bit 位置位 1 的速度越快，且布隆过滤器的效率越低；但是如果太少的话，那我们的误报率会变高。</p><p>我们参考如下一个图：<br><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/bloomFilter4.png" alt="bloomFilter4"></p><p>其中：</p><ul><li>k 是Hash函数的个数；</li><li>m 是布隆过滤器数组的长度；</li><li>n 是需要插入元素的个数；</li><li>p 是误报率。</li></ul><h3 id="3-5-误报率"><a href="#3-5-误报率" class="headerlink" title="3.5 误报率"></a>3.5 误报率</h3><p>如果 m 是位数组中的比特数，则在插入元素期间某一特定比特位不被某个哈希函数设置为 1 的概率是：</p><script type="math/tex; mode=display">1 - \frac{1}{m}</script><p>如果哈希函数的数量是 k，则通过 k 个哈希函数都未将该位设置为 1 的概率是：</p><script type="math/tex; mode=display">(1 - \frac{1}{m})^k</script><p>那么，如果我们插入了 n 个元素，某个位为 1 的概率，我们利用反向概率就可以求得为：</p><script type="math/tex; mode=display">1 - (1 - \frac{1}{m})^{kn}</script><p>现在我们要判断一个元素是否在集合中，假设这个元素本不在集合中，理论上来讲，经过 k 个哈希函数计算后得到的位数组的 k 个位置的值都应该是 0，如果发生了误判，即这 k 个位置的值都为 1，这就对应着<code>误判率</code>如下：</p><script type="math/tex; mode=display">p=(1 - [1 - \frac{1}{m}]^{kn})^k \approx (1 - e^{-\frac{kn}{m}})^k</script><p>参考极限公式：</p><script type="math/tex; mode=display">\lim_{x \to \infty} (1 - \frac{1}{x})^{-x}=e</script><h3 id="3-6-最优的k"><a href="#3-6-最优的k" class="headerlink" title="3.6 最优的k"></a>3.6 最优的k</h3><p>这里<strong>存在两个互斥</strong>：</p><ul><li>如果哈希函数的个数多，那么在对一个不属于集合的元素进行查询时得到0的概率就大；</li><li>如果哈希函数的个数少，那么位数组中的0就多。</li></ul><p>为了得到最优的哈希函数个数，我们需要根据上一节中的<code>错误率</code>公式进行计算。</p><p>我们首先对误判率两边取对数：</p><script type="math/tex; mode=display">ln(p) = k ln(1-e^{-\frac{kn}{m}})</script><p>我们的目的是求最优的k，且最优就表明误判率p要是最小，所以两边对k求导：</p><script type="math/tex; mode=display">\frac{1}{p} \cdot p' = ln(1 - e^{-\frac{nk}{m}}) + \frac{k e^{-\frac{nk}{m}} \frac{n}{m}}{1 - e^{-\frac{nk}{m}}}</script><p>另$p’=0$就有：</p><script type="math/tex; mode=display">ln(1 - e^{-\frac{nk}{m}}) + \frac{k e^{-\frac{nk}{m}} \frac{n}{m}}{1 - e^{-\frac{nk}{m}}} = 0</script><script type="math/tex; mode=display">(1 - e^{-\frac{nk}{m}}) \cdot ln(1 - e^{-\frac{nk}{m}}) = -k e^{-\frac{nk}{m}} \frac{n}{m}</script><script type="math/tex; mode=display">(1 - e^{-\frac{nk}{m}}) \cdot ln(1 - e^{-\frac{nk}{m}}) = e^{-\frac{nk}{m}}(-\frac{nk}{m})</script><p>所以：</p><script type="math/tex; mode=display">1 - e^{-\frac{nk}{m}} = e^{-\frac{nk}{m}}</script><script type="math/tex; mode=display">e^{-\frac{nk}{m}} = 1/2</script><script type="math/tex; mode=display">\frac{kn}{m} = ln2</script><script type="math/tex; mode=display">k = \frac{m}{n}ln2</script><h3 id="3-7-最优的m"><a href="#3-7-最优的m" class="headerlink" title="3.7 最优的m"></a>3.7 最优的m</h3><p>根据上面求出的最优 k，我们带入误判率 p 的公式就有：</p><script type="math/tex; mode=display">p=(1 - e^{-\frac{kn}{m}})^k=(1 - e^{-(\frac{m}{n}ln2)\frac{n}{m}})^k=\frac{1}{2}^k</script><p>将最优的 k 代入：</p><script type="math/tex; mode=display">p = 2^{-ln2 \cdot\frac{m}{n}}</script><p>两边同时取 ln 就有：</p><script type="math/tex; mode=display">lnp = ln2 \cdot (-ln2)\frac{m}{n}</script><script type="math/tex; mode=display">m = -\frac{n \cdot lnp}{(ln2)^2}</script><h3 id="3-8-估算-BF-的元素数量n"><a href="#3-8-估算-BF-的元素数量n" class="headerlink" title="3.8 估算 BF 的元素数量n"></a>3.8 估算 BF 的元素数量n</h3><script type="math/tex; mode=display">n = -\frac{m}{k}ln(1 - \frac{t}{m})</script><p>其中:</p><ul><li>n 是估计 BF 中的元素个数;</li><li>t 是位数组中被置为 1 的位的个数。</li></ul><h2 id="4-代码参考"><a href="#4-代码参考" class="headerlink" title="4 代码参考"></a>4 代码参考</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mmh3</span><br><span class="line"><span class="keyword">from</span> bitarray <span class="keyword">import</span> bitarray</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># zhihu_crawler.bloom_filter</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Implement a simple bloom filter with murmurhash algorithm.</span></span><br><span class="line"><span class="comment"># Bloom filter is used to check wether an element exists in a collection, and it has a good performance in big data situation.</span></span><br><span class="line"><span class="comment"># It may has positive rate depend on hash functions and elements count.</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">BIT_SIZE = <span class="number">5000000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BloomFilter</span>:</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># Initialize bloom filter, set size and all bits to 0</span></span><br><span class="line">        bit_array = bitarray(BIT_SIZE)</span><br><span class="line">        bit_array.setall(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.bit_array = bit_array</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, url</span>):</span><br><span class="line">        <span class="comment"># Add a url, and set points in bitarray to 1 (Points count is equal to hash funcs count.)</span></span><br><span class="line">        <span class="comment"># Here use 7 hash functions.</span></span><br><span class="line">        point_list = <span class="variable language_">self</span>.get_postions(url)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> b <span class="keyword">in</span> point_list:</span><br><span class="line">            <span class="variable language_">self</span>.bit_array[b] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">contains</span>(<span class="params">self, url</span>):</span><br><span class="line">        <span class="comment"># Check if a url is in a collection</span></span><br><span class="line">        point_list = <span class="variable language_">self</span>.get_postions(url)</span><br><span class="line"></span><br><span class="line">        result = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> b <span class="keyword">in</span> point_list:</span><br><span class="line">            result = result <span class="keyword">and</span> <span class="variable language_">self</span>.bit_array[b]</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_postions</span>(<span class="params">self, url</span>):</span><br><span class="line">        <span class="comment"># Get points positions in bit vector.</span></span><br><span class="line">        point1 = mmh3.<span class="built_in">hash</span>(url, <span class="number">41</span>) % BIT_SIZE</span><br><span class="line">        point2 = mmh3.<span class="built_in">hash</span>(url, <span class="number">42</span>) % BIT_SIZE</span><br><span class="line">        point3 = mmh3.<span class="built_in">hash</span>(url, <span class="number">43</span>) % BIT_SIZE</span><br><span class="line">        point4 = mmh3.<span class="built_in">hash</span>(url, <span class="number">44</span>) % BIT_SIZE</span><br><span class="line">        point5 = mmh3.<span class="built_in">hash</span>(url, <span class="number">45</span>) % BIT_SIZE</span><br><span class="line">        point6 = mmh3.<span class="built_in">hash</span>(url, <span class="number">46</span>) % BIT_SIZE</span><br><span class="line">        point7 = mmh3.<span class="built_in">hash</span>(url, <span class="number">47</span>) % BIT_SIZE</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> [point1, point2, point3, point4, point5, point6, point7]</span><br></pre></td></tr></table></figure><p><strong>参考文章：</strong><br><a href="https://www.cnblogs.com/cpselvis/p/6265825.html">布隆过滤器(Bloom Filter)的原理和实现</a><br><a href="https://www.163.com/dy/article/G55C599D05372639.html">聊聊Redis布隆过滤器与布谷鸟过滤器？一文避坑</a><br><a href="https://cloud.tencent.com/developer/article/1136056">Counting Bloom Filter 的原理和实现</a><br><a href="https://zhuanlan.zhihu.com/p/43263751">详解布隆过滤器的原理，使用场景和注意事项</a></p><hr>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-背景&quot;&gt;&lt;a href=&quot;#1-背景&quot; class=&quot;headerlink&quot; title=&quot;1 背景&quot;&gt;&lt;/a&gt;1 背景&lt;/h2&gt;&lt;p&gt;在实际工作中，我们经常涉及判断一个对象或者数据是否存在于内存或者数据库。往往大家会想到HashMap，但是这时候有一个问题,存储容量占比高，考虑到负载因子的存在，通常空间是不能被用满的，而一旦你的值很多例如上亿的时候，可行性就差了。&lt;br&gt;另一方面，如果很多请求是在请求数据库根本不存在的数据,那么数据库就要频繁响应这种不必要的IO查询,如果再多一些,数据库大多数IO都在响应这种毫无意义的请求操作,为了解决这一个问题，过滤器由此诞生！&lt;/p&gt;
&lt;h2 id=&quot;2-布隆过滤器&quot;&gt;&lt;a href=&quot;#2-布隆过滤器&quot; class=&quot;headerlink&quot; title=&quot;2 布隆过滤器&quot;&gt;&lt;/a&gt;2 布隆过滤器&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;过滤原理：布隆过滤器(Bloom Filter)大概的思路就是，当你请求的信息来的时候，先检查一下你查询的数据我这有没有，有的话将请求压给数据库,没有的话直接返回。&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="学习笔记" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="算法总结" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="算法" scheme="https://www.xiemingzhao.com/tags/%E7%AE%97%E6%B3%95/"/>
    
    <category term="数据结构" scheme="https://www.xiemingzhao.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Faiss 召回详解</title>
    <link href="https://www.xiemingzhao.com/posts/faissrecall.html"/>
    <id>https://www.xiemingzhao.com/posts/faissrecall.html</id>
    <published>2020-05-04T16:00:00.000Z</published>
    <updated>2025-04-01T16:00:36.024Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>向量检索普遍应用于搜索、推荐、以及CV领域，往往候选集合两集都在千万甚至上亿规模。那么，速度很容易成为瓶颈，这时候就需要牺牲一定的精度来换取速度。于是诞生了许多ANN（近邻检索）算法，例如HNSW就是其中一种。但是，不同的ANN具有各自的优劣势，本文主要介绍<code>faiss</code>这一工业界普遍使用的向量检索框架。</p><p>Faiss的是由FaceBook的AI团队公开的项目<a href="https://github.com/facebookresearch/faiss">Facebook AI Similarity Search</a>，是针对大规模相似度检索问题开发的一个工具，使用C++编写，有python接口，对10亿量级的索引可以做到毫秒级检索的性能。</p><p>其核心思想：<strong>把候选向量集封装成一个index数据库，加速检索TopK相似向量的过程，尽量维持召回率，其中部分索引支持GPU构建。</strong></p><span id="more"></span><h2 id="2-原理"><a href="#2-原理" class="headerlink" title="2 原理"></a>2 原理</h2><p>Faiss框架中，需要了解<code>k-means</code>、<code>PCA</code>以及<code>PQ</code>等算法。但最需要了解的2个核心原理便是：</p><ul><li>Product Quantizer, 简称<code>PQ</code>.</li><li>Inverted File System, 简称<code>IVF</code>.</li></ul><h3 id="2-1-乘积量化-PQ-原理"><a href="#2-1-乘积量化-PQ-原理" class="headerlink" title="2.1 乘积量化(PQ)原理"></a>2.1 乘积量化(PQ)原理</h3><p>矢量量化方法，即<code>vector quantization</code>，其具体定义为: <strong>将向量空间的点用一个有限子集来进行编码的过程</strong>。常见的聚类算法，都是一种矢量量化方法。向量量化方法又以<code>乘积量化</code>(PQ, Product Quantization)最为典型。</p><p>乘积量化的<code>核心思想</code>:<strong>分段（划分子空间）和聚类，KMeans是PQ乘积量化子空间数目为1的特例。</strong></p><p>PQ乘积量化生成码本和量化的过程可以用如下图示来说明：</p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/recall/faiss0.png" alt="faissrecall0"></p><h5 id="在训练阶段"><a href="#在训练阶段" class="headerlink" title="在训练阶段:"></a>在训练阶段:</h5><ol><li>针对N个训练样本，假设样本维度为128维，我们将其切分为4个子空间，则每一个子空间的维度为32维;</li><li>然后我们在每一个子空间中，对子向量采用K-Means对其进行聚类(图中示意聚成256类)，这样每一个子空间都能得到一个码本，这步称为<code>Clustering</code>。</li><li>这样训练样本的每个子段，都可以用子空间的聚类中心来近似，对应的编码即为类中心的ID(8bit)，这步称为 <code>Assign</code>。</li><li>如图所示，通过这样一种编码方式，训练样本仅使用的很短的一个编码得以表示，从而达到量化的目的。</li><li>对于待编码的样本，将它进行相同的操作，在各个子空间里使用距离它们最近类中心的id来表示它们，即完成了编码。</li></ol><p>通过下图可以看到，作者做了相应的实验，最终发现，压缩后的类中心ID设置为<strong>m=8bit（即上述的256类）效果最好</strong>。</p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/recall/faiss1.png" alt="faissrecall1"></p><h4 id="在查询阶段"><a href="#在查询阶段" class="headerlink" title="在查询阶段:"></a>在查询阶段:</h4><p>下面过程示意的是查询样本来到时，以<code>非对称距离</code>的方式(红框标识出来的部分)计算到dataset样本间的过程：</p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/recall/faiss2.png" alt="faissrecall2"></p><ol><li>按生成码本的过程，将其同样分成相同的子段，然后在每个子空间中，计算子段到该子空间中所有聚类中心得距离，得到距离表（维度4x256）。</li><li>在计算库中某个样本到查询向量的距离时，比如编码为(124, 56, 132, 222)这个样本，我们分别到距离表中取各个子段对应的距离即可。</li><li>所有子段对应的距离取出来后，将这些子段的距离求和相加，即得到该样本到查询样本间的<code>非对称距离</code>。</li><li>所有距离算好后，排序后即得到我们最终想要的结果。（实际上距离计算节省了，但是依然要遍历排序查找）</li></ol><p>PQ乘积量化能够<code>加速索引</code>的原理：<strong>即将全样本的距离计算，转化为到子空间类中心的距离计算。</strong></p><ul><li>比如上面所举的例子，原本brute-force search的方式计算距离的次数随样本数目N成线性增长，但是经过PQ编码后，对于耗时的距离计算，只要计算4x256次，<strong>几乎可以忽略此时间的消耗。</strong></li><li>另外，从上图也可以看出，对特征进行编码后，可以用一个相对比较短的编码来表示样本，自然对于<strong>内存的消耗要大大小于 brute-force search 的方式</strong>。</li></ul><p>在某些特殊的场合，我们总是希望获得精确的距离，而不是近似的距离，并且我们总是喜欢获取向量间的余弦相似度（余弦相似度距离范围在[-1,1]之间，便于设置固定的阈值），针对这种场景，<strong>可以针对PQ乘积量化得到的前top@K做一个brute-force search的排序</strong>。</p><h3 id="2-2-倒排乘积量化（IVFPQ）原理"><a href="#2-2-倒排乘积量化（IVFPQ）原理" class="headerlink" title="2.2  倒排乘积量化（IVFPQ）原理"></a>2.2  倒排乘积量化（IVFPQ）原理</h3><p>如果向量比较多，虽然降低了距离的计算复杂度，但是依然要便利所有的向量，需要进一步优化。<code>倒排PQ乘积量化(IVFPQ)</code>是PQ乘积量化的更进一步加速版。</p><p>其加速的本质依然是<strong>加速原理：为了加快查找的速度，几乎所有的ANN方法都是通过对全空间分割，将其分割成很多小的子空间，在搜索的时候，通过某种方式，快速锁定在某一（几）子空间，然后在该（几个）子空间里做遍历</strong>。</p><p>仔细观察PQ乘积量化存在一定的<code>优化空间</code>：</p><ul><li>实际上我们感兴趣的是那些跟查询样本相近的样本（姑且称这样的区域为<code>感兴趣区域</code>），也就是说老老实实挨个相加其实做了很多的无用功。如果能够通过某种手段<strong>快速将全局遍历锁定为感兴趣区域，则可以舍去不必要的全局计算以及排序。</strong></li><li>倒排PQ乘积量化的”倒排“，正是这样一种思想的体现，在具体实施手段上，采用的是通过<strong>聚类的方式实现感兴趣区域的快速定位</strong>，在倒排PQ乘积量化中，聚类可以说应用得淋漓尽致。</li></ul><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/recall/faiss3.png" alt="faissrecall3"></p><p>如上图所示：<strong>在PQ乘积量化之前，增加了一个粗量化过程。</strong></p><h4 id="具体地"><a href="#具体地" class="headerlink" title="具体地:"></a>具体地:</h4><ol><li>先对N个训练样本采用<code>KMeans聚类</code>，这里聚类的数目一般设置得不应过大，<strong>一般设置为1024差不多</strong>，这种可以以比较快的速度完成聚类过程。</li><li>得到了聚类中心后，针对每一个样本$x_i$，找到其距离最近的类中心$c_i$后，两者相减得到样本$x_i$的<code>残差向量(x_i-c_i)</code>;</li><li>后面剩下的过程，就是<strong>针对$(x_i-c_i)$的PQ乘积量化过程</strong>。</li></ol><p>在查询的时候，通过相同的粗量化，可以快速定位到查询向量属于哪个$c_i$（即在哪一个感兴趣区域），也可以是多个感兴趣聚类中心，然后在该感兴趣区域按上面所述的 P Q乘积量化距离计算方式计算距离。整体流程如下图所示。</p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/recall/faiss4.png" alt="faissrecall4"></p><h3 id="2-3-最优乘积量化（OPQ）"><a href="#2-3-最优乘积量化（OPQ）" class="headerlink" title="2.3 最优乘积量化（OPQ）"></a>2.3 最优乘积量化（OPQ）</h3><p>最优乘积量化（Optimal Product Quantization, OPQ）是PQ的一种改进版本。其改进体现在，<strong>致力于在子空间分割时，对各子空间的方差进行均衡</strong>。</p><p>用于检索的原始特征维度较高，所以实际在使用PQ等方法构建索引的时候，常会对高维的特征使用<strong>PCA等降维</strong>方法对特征先做降维处理。这样降维预处理，可以达到两个<code>目的</code>：</p><ul><li>一是<strong>降低特征维度</strong>；</li><li>二是利用PCA使得在对向量进行子段切分的时候要求特征<strong>各个维度尽可能不相关</strong>。</li></ul><p>但是这么做了后，在切分子段的时候，采用顺序切分子段仍然存在一定的问题，这个问题可以借用ITQ中的一个二维平面的例子加以说明：</p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/recall/faiss5.png" alt="faissrecall5"></p><p><strong>这个问题就是：</strong></p><ul><li>如上面a图所示，对于PCA降维后的二维空间，假设在做PQ的时候，将子段数目设置为2段，即切分成x和y两个子向量，然后分别在x和y上做聚类（假设聚类中心设置为2）。</li><li>对a图和c图聚类的结果进行比较，可以明显的发现，<strong>a图在y方向上聚类的效果明显差于c图</strong>，而PQ又是采用聚类中心来近似原始向量（这里指降维后的向量），也就是c图是我们需要的结果。</li></ul><blockquote><p>这个问题可以转化为数据方差来描述：<strong>在做PQ编码时，对于切分的各个子空间，我们应尽可能使得各个子空间的方差比较接近，最理想的情况是各个子空间的方差都相等。</strong></p></blockquote><p><strong>解决办法：</strong><br>OPQ致力于解决的问题正是对各个子空间方差的均衡。思想主要是<strong>在聚类的时候对聚类中心寻找对应的最优旋转矩阵，使得所有子空间中各个数据点到对应子空间的类中心的L2损失的求和最小</strong>。具体可以分为非参求解方法和带参求解方法，这里不再赘述。</p><h2 id="3-code"><a href="#3-code" class="headerlink" title="3 code"></a>3 code</h2><p>注意faiss包的安装：<code>conda install faiss-cpu -c pytorch</code></p><p><strong>Flat ：暴力检索</strong></p><blockquote><p>优点：最准确的，召回率最高；<br>缺点：速度慢，占内存大。<br>使用情况：向量候选集很少，在50万以内，并且内存不紧张。</p></blockquote><p><strong>IVFx Flat ：倒排暴力检索</strong></p><blockquote><p>优点：IVF主要利用倒排的思想，会大大减少了检索的时间。具体可以拿出每个聚类中心下的向量ID，每个中心ID后面挂上一堆非中心向量，每次查询向量的时候找到最近的几个中心ID，分别搜索这几个中心下的非中心向量。通过减小搜索范围，提升搜索效率。<br>缺点：速度也还不是很快。<br>使用情况：向量候选集很少，在50万以内，并且内存不紧张。</p></blockquote><p><strong>PQx ：乘积量化</strong></p><blockquote><p>优点：利用乘积量化的方法，改进了普通检索，将一个向量的维度切成x段，每段分别进行检索，每段向量的检索结果取交集后得出最后的TopK。因此速度很快，而且占用内存较小，召回率也相对较高。<br>缺点：召回率相较于暴力检索，下降较多。<br>使用情况：内存及其稀缺，并且需要较快的检索速度，不那么在意召回率</p></blockquote><p><strong>IVFxPQy 倒排乘积量化</strong></p><blockquote><p>优点：工业界大量使用此方法，各项指标都均可以接受，利用乘积量化的方法，改进了IVF的k-means，将一个向量的维度切成x段，每段分别进行k-means再检索。<br>缺点：集百家之长，自然也集百家之短<br>使用情况：一般来说，各方面没啥特殊的极端要求的话，最推荐使用该方法！</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> faiss</span><br><span class="line"><span class="keyword">from</span> scipy.cluster.vq <span class="keyword">import</span> vq, kmeans2</span><br><span class="line"><span class="keyword">from</span> scipy.spatial.distance <span class="keyword">import</span> cdist</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">vec, M, Ks</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param vec: 向量</span></span><br><span class="line"><span class="string">    :param M: 子向量组数</span></span><br><span class="line"><span class="string">    :param Ks: 每组向量聚类个数</span></span><br><span class="line"><span class="string">    :return: codeword: [M, Ks, Ds]，</span></span><br><span class="line"><span class="string">        codeword[m][k]表示第m组子向量第k个子向量所属的聚类中心向量</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    Ds = <span class="built_in">int</span>(vec.shape[<span class="number">1</span>] / M)</span><br><span class="line">    codeword = np.empty((M, Ks, Ds), np.float32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> <span class="built_in">range</span>(M):</span><br><span class="line">        vec_sub = vec[:, m * Ds: (m + <span class="number">1</span>) * Ds]</span><br><span class="line">        <span class="comment"># 第m组子向量vec_sub聚成Ks类</span></span><br><span class="line">        <span class="comment"># kmeans2返回两个结果，第一个是原始向量归属类目的中心向量，第二个是类目ID</span></span><br><span class="line">        codeword[m], label = kmeans2(vec_sub, Ks)</span><br><span class="line">    <span class="keyword">return</span> codeword</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">codeword, vec</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param codeword: 码本，shape为[M, Ks, Ds]</span></span><br><span class="line"><span class="string">    :param vec: 原始向量</span></span><br><span class="line"><span class="string">    :return: pqcode: pq编码结果,</span></span><br><span class="line"><span class="string">        shape为[N, M]，每个原始向量用M组子向量的聚类中心ID表示</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    M, Ks, Ds = codeword.shape</span><br><span class="line">    <span class="comment"># pq编码shape为[N, M]</span></span><br><span class="line">    pqcode = np.empty((vec.shape[<span class="number">0</span>], M), np.int64)</span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> <span class="built_in">range</span>(M):</span><br><span class="line">        vec_sub = vec[:, m * Ds: (m + <span class="number">1</span>) * Ds]</span><br><span class="line">        <span class="comment"># 第m组子向量</span></span><br><span class="line">        <span class="comment"># 第m组子向量中每个子向量在第m个码本中查找距离最近的</span></span><br><span class="line">        pqcode[:, m], dist = vq(vec_sub, codeword[m])</span><br><span class="line">    <span class="keyword">return</span> pqcode</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">search</span>(<span class="params">codeword, pqcode, query</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param codeword:</span></span><br><span class="line"><span class="string">    :param pqcode: pq编码结果, shape为[N, M]，每个原始向量用M组子向量的聚类中心ID表示</span></span><br><span class="line"><span class="string">    :param query: 查询向量[1, d]</span></span><br><span class="line"><span class="string">    :return: dist：查询向量与原始向量的距离，shape为[N,]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    M, Ks, Ds = codeword.shape</span><br><span class="line">    <span class="comment"># 距离向量表, [M, Ks]</span></span><br><span class="line">    dist_table = np.empty((M, Ks))</span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> <span class="built_in">range</span>(M):</span><br><span class="line">        query_sub = query[m * Ds: (m + <span class="number">1</span>) * Ds]</span><br><span class="line">        <span class="comment"># query_sub向量与第m个码本每个向量距离</span></span><br><span class="line">        dist_table[m, :] = cdist([query_sub], codeword[m], <span class="string">&#x27;sqeuclidean&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># dist_table[range(M), pqcode] 为 query向量与原始向量在每个子向量的聚类，shape为[N, M]</span></span><br><span class="line">    <span class="comment"># 每组子向量距离相加</span></span><br><span class="line">    dist = np.<span class="built_in">sum</span>(dist_table[<span class="built_in">range</span>(M), pqcode], axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> dist</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">PQmain</span>():</span><br><span class="line">    <span class="comment"># 数据量</span></span><br><span class="line">    N = <span class="number">50000</span></span><br><span class="line">    <span class="comment"># 向量维度</span></span><br><span class="line">    d = <span class="number">128</span></span><br><span class="line">    <span class="comment"># 每组子向量聚类个数</span></span><br><span class="line">    Ks = <span class="number">32</span></span><br><span class="line">    <span class="comment"># 训练向量[N, d]</span></span><br><span class="line">    vec_train = np.random.random((N, d))</span><br><span class="line">    <span class="comment"># 查询向量[1, d]</span></span><br><span class="line">    <span class="comment"># mock 第100个是距离查询向量最近的</span></span><br><span class="line">    selected_vec = vec_train[<span class="number">100</span>]</span><br><span class="line">    query_vec = selected_vec + [np.random.uniform(-<span class="number">0.001</span>, <span class="number">0.001</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(d)]</span><br><span class="line">    query = np.random.random((<span class="number">1</span>, d))</span><br><span class="line">    <span class="comment"># 子向量组数</span></span><br><span class="line">    M = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对原始向量划分子向量组，并对每组子向量进行聚类</span></span><br><span class="line">    codeword = train(vec_train, M, Ks)</span><br><span class="line">    <span class="comment"># pq编码</span></span><br><span class="line">    pqcode = encode(codeword, vec_train)</span><br><span class="line">    <span class="comment"># 查询向量</span></span><br><span class="line">    dist = search(codeword, pqcode, query_vec)</span><br><span class="line"></span><br><span class="line">    sorted_dist = <span class="built_in">sorted</span>(<span class="built_in">enumerate</span>(dist), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])</span><br><span class="line">    <span class="built_in">print</span>(sorted_dist[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_IndexFlatL2</span>(<span class="params">vec_train, query, top_k=<span class="number">5</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    暴力检索</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    N, d = vec_train.shape</span><br><span class="line">    <span class="comment"># 1. 创建索引</span></span><br><span class="line">    index = faiss.IndexFlatL2(d)</span><br><span class="line">    <span class="comment"># 2. 添加数据集</span></span><br><span class="line">    index.add(vec_train)</span><br><span class="line">    <span class="comment"># 3. 检索</span></span><br><span class="line">    dist_list, label_list = index.search(np.array([query]), k=top_k)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;=&quot;</span> * <span class="number">8</span> + <span class="string">&quot;FlatL2 recall top %d is&quot;</span> % top_k + <span class="string">&quot;=&quot;</span> * <span class="number">8</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;dist:&quot;</span> + <span class="built_in">str</span>(dist_list))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;index:&quot;</span> + <span class="built_in">str</span>(label_list))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_IndexIVFFlat</span>(<span class="params">vec_train, query_vec, top_k=<span class="number">5</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    通过创建倒排索引优化</span></span><br><span class="line"><span class="string">    流程：使用k-means对train向量进行聚类，查询时query_vec所归属的类目中进行检索</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    nlist = <span class="number">100</span>  <span class="comment"># 聚类中心的个数</span></span><br><span class="line">    N, d = vec_train.shape</span><br><span class="line">    quantizer = faiss.IndexFlatL2(d)  <span class="comment"># the other index</span></span><br><span class="line">    index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_L2)</span><br><span class="line">    <span class="comment"># 添加 训练集</span></span><br><span class="line">    index.train(vec_train)</span><br><span class="line">    index.add(vec_train)</span><br><span class="line">    <span class="comment"># 检索</span></span><br><span class="line">    query_vec = np.reshape(query_vec, [-<span class="number">1</span>,d])</span><br><span class="line">    D, I = index.search(query_vec, top_k)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;=&quot;</span> * <span class="number">8</span> + <span class="string">&quot;IVFFlat recall top %d is&quot;</span> % top_k + <span class="string">&quot;=&quot;</span> * <span class="number">8</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;dist:&quot;</span> + <span class="built_in">str</span>(D))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;index:&quot;</span> + <span class="built_in">str</span>(I))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_IndexIVFPQ</span>(<span class="params">vec_train, query_vec, top_k=<span class="number">10</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    通过创建倒排索引优化</span></span><br><span class="line"><span class="string">    流程：使用k-means对train向量进行聚类，查询时query_vec所归属的类目中进行检索</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    nlist = <span class="number">100</span>  <span class="comment"># 聚类中心的个数</span></span><br><span class="line">    m = <span class="number">8</span></span><br><span class="line">    N, d = vec_train.shape</span><br><span class="line">    quantizer = faiss.IndexFlatL2(d)  <span class="comment"># the other index</span></span><br><span class="line">    index = faiss.IndexIVFPQ(quantizer, d, nlist, m, <span class="number">8</span>)</span><br><span class="line">    <span class="comment"># 添加 训练集</span></span><br><span class="line">    index.train(vec_train)</span><br><span class="line">    index.add(vec_train)</span><br><span class="line">    <span class="comment"># 检索</span></span><br><span class="line">    query_vec = np.reshape(query_vec, [-<span class="number">1</span>,d])</span><br><span class="line">    D, I = index.search(query_vec, top_k)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;=&quot;</span>*<span class="number">8</span> + <span class="string">&quot;IVFPQ recall top %d is&quot;</span> % top_k+<span class="string">&quot;=&quot;</span>*<span class="number">8</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;dist:&quot;</span> + <span class="built_in">str</span>(D))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;index:&quot;</span> + <span class="built_in">str</span>(I))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">faissmain</span>():</span><br><span class="line">    <span class="comment"># 数据量</span></span><br><span class="line">    N = <span class="number">50000</span></span><br><span class="line">    <span class="comment"># 向量维度</span></span><br><span class="line">    d = <span class="number">128</span></span><br><span class="line">    vec_train = np.ascontiguousarray(np.random.random((N, d)), np.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># mock 第100个是距离查询向量最近的</span></span><br><span class="line">    selected_vec = vec_train[<span class="number">100</span>]</span><br><span class="line">    query_vec = selected_vec + [np.random.uniform(-<span class="number">0.001</span>, <span class="number">0.001</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(d)]</span><br><span class="line">    query_vec = np.ascontiguousarray(query_vec, np.float32)</span><br><span class="line">    <span class="comment"># 1. 暴力检索，全量检索</span></span><br><span class="line">    test_IndexFlatL2(vec_train, query_vec)</span><br><span class="line">    <span class="comment"># 2. 倒排索引</span></span><br><span class="line">    test_IndexIVFFlat(vec_train, query_vec)</span><br><span class="line">    <span class="comment"># 3. 倒排PQ</span></span><br><span class="line">    test_IndexIVFPQ(vec_train, query_vec)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    PQmain()</span><br><span class="line">    faissmain()</span><br></pre></td></tr></table></figure><h2 id="4-相关拓展"><a href="#4-相关拓展" class="headerlink" title="4 相关拓展"></a>4 相关拓展</h2><p>IndexIVFPQ（”IVFx,PQy”）的性能损失来自于向量压缩和倒排列表两部分。如果IndexIVFPQ的精度太低，可以：<strong>设置nprobe为nlist，以搜索整个数据集，然后查看其性能。请注意，默认nprobe值为1。</strong></p><p>单个向量检索的速度慢，Faiss针对批量搜索进行了优化：</p><ul><li>矩阵-矩阵乘法通常比对应数量的矩阵-向量乘法快得多</li><li>搜索并行化</li><li>采用多线程同时执行多个搜索，以完全占用计算机的cores</li></ul><p><strong>Faiss不支持字符串ID或in64以外的任何数据类型。</strong></p><p>不同框架的召回效果对比如下图，更全的对比可参考<a href="http://ann-benchmarks.com">ann-Benchmarking Results</a>：</p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/recall/faiss6.png" alt="faissrecall6"></p><p><strong>参考文献：</strong></p><p><a href="https://www.jstage.jst.go.jp/article/mta/6/1/6_2/_pdf">A Survey of Product Quantization</a><br><a href="https://lear.inrialpes.fr/pubs/2011/JDS11/jegou_searching_with_quantization.pdf">Product quantization for nearest neighbor search</a><br><a href="https://blog.csdn.net/weixin_42486623/article/details/121990806">Faiss原理介绍</a><br><a href="https://zhuanlan.zhihu.com/p/534004381">faiss原理（Product Quantization）</a><br><a href="https://zhuanlan.zhihu.com/p/432317877">搜索召回 | Facebook: 亿级向量相似度检索库Faiss原理+应用</a><br><a href="https://zhuanlan.zhihu.com/p/107241260">Fiass - 常见问题总结</a><br><a href="https://blog.csdn.net/sgyuanshi/article/details/119878434">推荐系统的向量检索工具: Annoy &amp; Faiss</a><br><a href="https://blog.csdn.net/luoyexuge/article/details/84235421?spm=1001.2101.3001.6650.3&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-3-84235421-blog-124058746.pc_relevant_multi_platform_featuressortv2removedup&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-3-84235421-blog-124058746.pc_relevant_multi_platform_featuressortv2removedup&amp;utm_relevant_index=6">topk相似度性能比较（kd-tree、kd-ball、faiss、annoy、线性搜索）</a><br><a href="https://zhongqiang.blog.csdn.net/article/details/122516942?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-122516942-blog-124058746.pc_relevant_multi_platform_featuressortv2removedup&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-122516942-blog-124058746.pc_relevant_multi_platform_featuressortv2removedup&amp;utm_relevant_index=2">annoy(快速近邻向量搜索包)学习小记 - pip命令学习与annoy基础使用</a></p><hr>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-引言&quot;&gt;&lt;a href=&quot;#1-引言&quot; class=&quot;headerlink&quot; title=&quot;1 引言&quot;&gt;&lt;/a&gt;1 引言&lt;/h2&gt;&lt;p&gt;向量检索普遍应用于搜索、推荐、以及CV领域，往往候选集合两集都在千万甚至上亿规模。那么，速度很容易成为瓶颈，这时候就需要牺牲一定的精度来换取速度。于是诞生了许多ANN（近邻检索）算法，例如HNSW就是其中一种。但是，不同的ANN具有各自的优劣势，本文主要介绍&lt;code&gt;faiss&lt;/code&gt;这一工业界普遍使用的向量检索框架。&lt;/p&gt;
&lt;p&gt;Faiss的是由FaceBook的AI团队公开的项目&lt;a href=&quot;https://github.com/facebookresearch/faiss&quot;&gt;Facebook AI Similarity Search&lt;/a&gt;，是针对大规模相似度检索问题开发的一个工具，使用C++编写，有python接口，对10亿量级的索引可以做到毫秒级检索的性能。&lt;/p&gt;
&lt;p&gt;其核心思想：&lt;strong&gt;把候选向量集封装成一个index数据库，加速检索TopK相似向量的过程，尽量维持召回率，其中部分索引支持GPU构建。&lt;/strong&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="学习笔记" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="算法总结" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="Faiss" scheme="https://www.xiemingzhao.com/tags/Faiss/"/>
    
    <category term="召回" scheme="https://www.xiemingzhao.com/tags/%E5%8F%AC%E5%9B%9E/"/>
    
  </entry>
  
  <entry>
    <title>一致性哈希算法(Consistent Hashing)</title>
    <link href="https://www.xiemingzhao.com/posts/consistentHash.html"/>
    <id>https://www.xiemingzhao.com/posts/consistentHash.html</id>
    <published>2020-04-07T16:00:00.000Z</published>
    <updated>2025-03-31T17:41:37.578Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1 背景"></a>1 背景</h2><p>一致性哈希算法(Consistent Hashing)在分布式系统的应用还是十分广泛的。例如随着业务的扩展，流量的剧增，单体项目逐渐划分为分布式系统。对于经常使用的数据，我们可以使用Redis作为缓存机制，减少数据层的压力。因此，重构后的系统架构如下图所示：</p><span id="more"></span><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/consistentHash1.png" alt="consistentHash1"></p><p>这个时候一般有两种方案：</p><p><strong>1. 每个机器缓存全量数据</strong>；</p><blockquote><p>如此虽然能保证请求打到任何一台机器都可以，但是冗余性巨高；</p></blockquote><p><strong>2. 每个机器只缓存一部分，分布式存储</strong>；</p><blockquote><p>如此需要保证对应的请求打到对应的机器上，否则查询结果为空，轮训查询的话效率极低不靠谱；</p></blockquote><h2 id="2-原始Hash方案"><a href="#2-原始Hash方案" class="headerlink" title="2 原始Hash方案"></a>2 原始Hash方案</h2><p>这时候，自然而然想到一个方案，那就是使用 hash 算法 例如，有三台 Redis，对于每次的访问都可以通过计算 hash 来求得 hash 值。 如公式 h=hash(key)%3，我们把 Redis 编号设置成0,1,2来保存对应 hash 计算出来的值，h的值等于Redis对应的编号。</p><p>但是，使用上述HASH算法进行缓存时，会出现一些<strong>缺陷</strong>。例如：</p><ul><li>缓存的3台缓存服务器由于故障使得机器数量减少；</li><li>缓存量的增加需要新增使得缓存机器增加。</li></ul><p>如此缓存位置必定会发生改变，以前缓存的图片也会失去缓存的作用与意义，由于大量缓存在同一时间失效，造成了缓存的雪崩，此时前端缓存已经无法起到承担部分压力的作用，后端服务器将会承受巨大的压力，整个系统很有可能被压垮，所以，我们应该想办法不让这种情况发生，但是由于上述HASH算法本身的缘故，使用取模法进行缓存时，这种情况是无法避免的，为了解决这些问题，一致性哈希算法诞生了。</p><h2 id="3-一致性Hash算法"><a href="#3-一致性Hash算法" class="headerlink" title="3 一致性Hash算法"></a>3 一致性Hash算法</h2><p>根据上述我们知道一致性Hash算法<strong>需要解决</strong>：</p><ol><li>当缓存服务器数量发生变化时，会引起缓存的雪崩，可能会引起整体系统压力过大而崩溃（大量缓存同一时间失效）。</li><li>当缓存服务器数量发生变化时，几乎所有缓存的位置都会发生改变，尽量减少受影响的缓存。</li></ol><p>其实，一致性哈希算法也是使用取模的方法，只是，刚才描述的取模法是对服务器的数量进行取模，而一致性哈希算法是对2^32取模。</p><p>首先，我们把二的三十二次方想象成一个圆，就像钟表一样，钟表的圆可以理解成由60个点组成的圆，而此处我们把这个圆想象成由2^32个点组成的圆，示意图如下：</p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/consistentHash2.png" alt="consistentHash2"></p><p>我们把这个由2的32次方个点组成的圆环称为<code>hash环</code>。</p><p>假设我们有3台缓存服务器，服务器A、服务器B、服务器C，那么，在生产环境中，<strong>这三台服务器肯定有自己的IP地址</strong>。</p><p><strong>hash（服务器A的IP地址） %  2^32</strong></p><p>通过上述公式算出的结果一定是一个0到2^32-1之间的一个整数，我们就用算出的这个整数，代表服务器A，既然这个整数肯定处于0到2^32-1之间。<br>那么，上图中的hash环上必定有一个点与这个整数对应，而我们刚才已经说明，使用这个整数代表服务器A，那么，服务器A就可以映射到这个环上，用下图示意：</p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/consistentHash3.png" alt="consistentHash3"></p><p>同理，服务器B与服务器C也可以通过相同的方法映射到上图中的 hash 环中</p><p><strong>hash（服务器B的IP地址） %  2^32</strong></p><p><strong>hash（服务器C的IP地址） %  2^32</strong></p><p>通过上述方法，可以将服务器B与服务器C映射到上图中的 hash 环上，示意图如下：</p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/consistentHash4.png" alt="consistentHash4"></p><p>我们通过上述方法，把缓存服务器映射到了hash环上，那么使用同样的方法，我们也可以将需要缓存的对象映射到hash环上。</p><p>假设，我们需要使用缓存服务器缓存图片，而且我们仍然使用图片的名称作为找到图片的key，那么我们使用如下公式可以将图片映射到上图中的hash环上。</p><p><strong>hash（图片名称） %  2^32</strong></p><p>映射后的示意图如下，下图中的橘黄色圆形表示图片：</p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/consistentHash5.png" alt="consistentHash5"></p><p>现在服务器与图片都被映射到了hash环上，上图中的图片将会被缓存到服务器A上，为什么呢？因为从图片的位置开始，沿顺时针方向遇到的第一个服务器就是A服务器，所以，上图中的图片将会被缓存到服务器A上，如下图所示：</p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/consistentHash6.png" alt="consistentHash6"></p><p>一致性哈希算法就是通过这种方法:</p><ol><li>判断一个对象应该被缓存到哪台服务器上的;</li><li>将缓存服务器与被缓存对象都映射到hash环上以后;</li><li>从被缓存对象的位置出发，沿顺时针方向遇到的第一个服务器，就是当前对象将要缓存于的服务器.</li></ol><p>由于被缓存对象与服务器hash后的值是固定的，所以，在服务器不变的情况下，一张图片必定会被缓存到固定的服务器上，那么，当下次想要访问这张图片时，只要再次使用相同的算法进行计算，即可算出这个图片被缓存在哪个服务器上，直接去对应的服务器查找对应的图片即可。</p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/consistentHash7.png" alt="consistentHash7"></p><p>1号、2号图片将会被缓存到服务器A上，3号图片将会被缓存到服务器B上，4号图片将会被缓存到服务器C上。</p><h2 id="4-一致性哈希算法的优点"><a href="#4-一致性哈希算法的优点" class="headerlink" title="4 一致性哈希算法的优点"></a>4 一致性哈希算法的优点</h2><blockquote><p>一致性哈希算法如何解决之前出现的问题呢？</p></blockquote><p>假设，服务器B出现了故障，我们现在需要将服务器B移除，那么，我们将上图中的服务器B从hash环上移除即可，移除服务器B以后示意图如下：</p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/consistentHash8.png" alt="consistentHash8"></p><p>在服务器B未移除时，图片3应该被缓存到服务器B中，可是当服务器B移除以后，按照之前描述的一致性哈希算法的规则，图片3应该被缓存到服务器C中，因为从图片3的位置出发，沿顺时针方向遇到的第一个缓存服务器节点就是服务器C，也就是说，如果服务器B出现故障被移除时，图片3的缓存位置会发生改变：</p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/consistentHash9.png" alt="consistentHash9"></p><p>但是，图片4仍然会被缓存到服务器C中，图片1与图片2仍然会被缓存到服务器A中，这与服务器B移除之前并没有任何区别，这就是一致性哈希算法的优点，如果使用之前的hash算法，服务器数量发生改变时，所有服务器的所有缓存在同一时间失效了，而使用一致性哈希算法时，服务器的数量如果发生改变，并不是所有缓存都会失效，而是只有部分缓存会失效，前端的缓存仍然能分担整个系统的压力，而不至于所有压力都在同一时间集中到后端服务器上。</p><h2 id="5-hash环的偏斜"><a href="#5-hash环的偏斜" class="headerlink" title="5 hash环的偏斜"></a>5 hash环的偏斜</h2><p>在介绍一致性哈希的概念时，我们理想化的将3台服务器均匀的映射到了hash环上，如下图所示：</p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/consistentHash4.png" alt="consistentHash10"></p><p>在实际的映射中，服务器可能会被映射成如下模样。</p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/consistentHash14.png" alt="consistentHash11"></p><p>如果服务器被映射成上图中的模样，那么被缓存的对象很有可能大部分集中缓存在某一台服务器上，如下图所示。</p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/consistentHash15.png" alt="consistentHash12"></p><p>上图中，1号、2号、3号、4号、6号图片均被缓存在了服务器A上，只有5号图片被缓存在了服务器B上，服务器C上甚至没有缓存任何图片，如果出现上图中的情况，A、B、C三台服务器并没有被合理的平均的充分利用，缓存分布的极度不均匀，而且，如果此时服务器A出现故障，那么失效缓存的数量也将达到最大值，在极端情况下，仍然有可能引起系统的崩溃，上图中的情况则被称之为hash环的偏斜。当然也有办法解决此问题的。</p><h2 id="6-虚拟节点"><a href="#6-虚拟节点" class="headerlink" title="6 虚拟节点"></a>6 虚拟节点</h2><p>话接上文，由于我们只有3台服务器，当我们把服务器映射到hash环上的时候，很有可能出现hash环偏斜的情况，当hash环偏斜以后，缓存往往会极度不均衡的分布在各服务器上。</p><p>聪明如你一定已经想到了，如果想要均衡的将缓存分布到3台服务器上，最好能让这3台服务器尽量多的、均匀的出现在hash环上。但是，真实的服务器资源只有3台，我们怎样凭空的让它们多起来呢。没错，<strong>就是凭空的让服务器节点多起来</strong>，既然没有多余的真正的物理服务器节点，我们就只能将现有的物理节点通过虚拟的方法复制出来，这些由实际节点虚拟复制而来的节点被称为<code>虚拟节点</code>。加入虚拟节点以后的hash环如下。</p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/consistentHash13.png" alt="consistentHash13"></p><p><strong>虚拟节点”是”实际节点”（实际的物理服务器）在hash环上的复制品，一个实际节点可以对应多个虚拟节点</strong>。</p><p>从上图可以看出，A、B、C三台服务器分别虚拟出了一个虚拟节点，当然，如果你需要，也可以虚拟出更多的虚拟节点。引入虚拟节点的概念后，缓存的分布就均衡多了，上图中，1号、3号图片被缓存在服务器A中，5号、4号图片被缓存在服务器B中，6号、2号图片被缓存在服务器C中，如果你还不放心，可以虚拟出更多的虚拟节点，以便减小hash环偏斜所带来的影响，虚拟节点越多，hash环上的节点就越多，缓存被均匀分布的概率就越大。</p><p><strong>参考文章</strong><br><a href="https://www.zsythink.net/archives/1182">白话解析：一致性哈希算法 consistent hashing</a><br><a href="https://juejin.cn/post/6844903750860013576">5分钟理解一致性哈希算法</a></p><hr>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-背景&quot;&gt;&lt;a href=&quot;#1-背景&quot; class=&quot;headerlink&quot; title=&quot;1 背景&quot;&gt;&lt;/a&gt;1 背景&lt;/h2&gt;&lt;p&gt;一致性哈希算法(Consistent Hashing)在分布式系统的应用还是十分广泛的。例如随着业务的扩展，流量的剧增，单体项目逐渐划分为分布式系统。对于经常使用的数据，我们可以使用Redis作为缓存机制，减少数据层的压力。因此，重构后的系统架构如下图所示：&lt;/p&gt;</summary>
    
    
    
    <category term="学习笔记" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="算法总结" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="算法" scheme="https://www.xiemingzhao.com/tags/%E7%AE%97%E6%B3%95/"/>
    
    <category term="Hash" scheme="https://www.xiemingzhao.com/tags/Hash/"/>
    
  </entry>
  
  <entry>
    <title>HNSW算法详解</title>
    <link href="https://www.xiemingzhao.com/posts/hnswAlgo.html"/>
    <id>https://www.xiemingzhao.com/posts/hnswAlgo.html</id>
    <published>2020-03-11T16:00:00.000Z</published>
    <updated>2025-03-31T16:46:23.237Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>在向量检索领域，<code>HNSW</code>算法算是有比较重要的一席之地，很多地方都会用到，那么本文主要是对其具体构建的细节逻辑和背后的思想进行一个阐述和介绍。</p><p><a href="https://arxiv.org/abs/1603.09320">Hierarchical Navigable Small World Graphs (HNSW)</a> 是Yury A. Malkov提出的一种基于图索引的方法，它是Yury A. Malkov在他本人之前工作<code>NSW</code>上一种改进。通过采用层状结构，将边按特征半径进行分层，使每个顶点在所有层中平均度数变为常数，从而将 <code>NSW</code> 的计算复杂度由多重对数(Polylogarithmic)复杂度降到了对数(logarithmic)复杂度。</p><h2 id="2-朴素思想"><a href="#2-朴素思想" class="headerlink" title="2 朴素思想"></a>2 朴素思想</h2><p>这里我们以一个小的场景为例来开始，假设我们现在有13个2维数据向量，我们把这些向量放在了一个平面直角坐标系内，隐去坐标系刻度，它们的位置关系如下图所示。</p><span id="more"></span><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/hnsw1.png" alt="hnsw1"></p><p><strong>朴素查找法</strong>：</p><ol><li>有一个很容易的的朴素想法，把某些点和点之间连上线，构成一个查找图，存下来备用；</li><li>当我想查找与粉色点最近的一点时，我从任意一个黑色点出发，计算它和粉色点的距离，与这个任意黑色点有连接关系的点我们称之为“友点”（直译）；</li><li>然后我要计算这个黑色点的所有“友点”与粉色点的距离，从所有“友点”中选出与粉色点最近的一个点，把这个点作为下一个进入点；</li><li>继续按照上面的步骤查找下去。如果当前黑色点对粉色点的距离比所有“友点”都近，终止查找，这个黑色点就是我们要找的离粉色点最近的点。</li></ol><p><strong>整个思想就是找出当前点及其邻近点雨目标点之间的最近的那个，然后把它当作当前点，再次循环</strong>。举个例子。</p><p><em>目标：我们要查找与粉色点最近的点。</em><br><strong>步骤</strong>：</p><ol><li>从任意一个黑色点出发，这里我们随便选个C点吧，计算一下C点和粉色点的距离，存下来备用;</li><li>再计算C点的所有友点（A，I，B）与粉色点的距离（计算距离和度量的方式有多种，这里我们采用欧氏距离，就是二维物理空间上的“近和远”），我们计算得出B与粉色点的距离最近，而且B点距离粉色点的距离比C点距离粉色点的距离（前面算过）更近l;</li><li>所以我们下面用B点继续查找。B点距离粉色点的距离保存下来，B点的友点是E，A，C，I，H，分别计算它们与粉色点的距离，得到E点与粉色点距离最近，且E点比B点距离粉色点还要近，所以我们选择E点作为下一个查找点。</li><li>E点的友点是J，B，D，G，这时我们发现J点的与粉色点的距离最近，但是，but，however，<em>J点的距离粉色点的距离比E点还要远，所以满足了终止查找的条件</em>，因此我们返回E点。</li></ol><p><strong>之所以叫朴素想法就是因为它的缺点非常多:</strong></p><ol><li>首先，我们发现图中的K点是无法被查询到的，因为K点没有友点</li><li>其次，如果我们要查找距离粉色点最近的两个点，而这两个近点之间如果没有连线，那么将大大影响效率（比如L和E点，如果L和E有连线，那么我们可以轻易用上述方法查出距离粉色点最近的两个点）;</li><li>最后D点真的需要这么多“友点”吗？谁是谁的友点应该怎么确定呢？</li></ol><p><strong>相关解决办法：</strong></p><ol><li>关于K点的问题，我们规定在构图时所有数据向量节点都必须有友点。</li><li>关于L和E的问题，我们规定在构图时所有距离相近（相似）到一定程度的向量必须互为友点。</li><li>关于D点问题，权衡构造这张图的时间复杂度，我们规定尽量减少每个节点的“友点”数量。</li></ol><h2 id="3-NSW算法"><a href="#3-NSW算法" class="headerlink" title="3 NSW算法"></a>3 NSW算法</h2><p>上述最后部分针对各个问题的解决办法促成了<code>NSW</code>算法的诞生。在图论中有一个很好的剖分法则专门解决上一节中提到的朴素想法的缺陷问题—-<strong>德劳内（Delaunay）三角剖分算法</strong>，这个算法可以达成如下要求：</p><ol><li>图中每个点都有“友点”。</li><li>相近的点都互为“友点”。</li><li>图中所有连接（线段）的数量最少。效果如下图。</li></ol><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/hnsw2.png" alt="hnsw2"></p><p>如上图所示就是一个经典的满足上述三个条件的<code>德劳内三角网图</code>。但<code>NSW</code>算法并没有采用德劳内三角剖分法来构成德劳内三角网图，原因是：</p><ul><li>德劳内三角剖分构图算法时间<strong>复杂度太高</strong>，换句话说，构图太耗时。</li><li>德劳内三角形的查找效率并不一定最高，如果初始点和查找点<br>距离很远的话我们需要进行多次跳转才能查到其临近点，需要“<strong>高速公路</strong>”机制（Expressway mechanism, 这里指部分远点之间拥有线段连接，以便于快速查找）。</li></ul><p><strong>在理想状态下，我们的算法不仅要满足上面三条需求，还要算法复杂度低，同时配有高速公路机制的构图法。</strong></p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/hnsw3.png" alt="hnsw3"></p><p>如上图所示，这是NSW论文中给出的一张满足条件的网络图，可以发现黑色是近邻点的连线，红色线就是<code>高速公路机制</code>了。我们从 enter point 点进入查找，查找绿色点临近节点的时候，就可以用过红色连线“高速公路机制”快速查找到结果。</p><p><strong>NSW朴素构图算法</strong>：</p><ol><li>向图中逐个插入点，点的选取上随机的；</li><li>插入一个全新点时，通过朴素想法中的朴素查找法查找到与这个全新点最近的 m 个点（通过计算“友点”和待插入点的距离来判断下一个进入点是哪个点，m 由用户设置）；</li><li>连接全新点到m个点的连线。</li></ol><p>仔细分析这个简单的构图方法，其中有一个精妙之处就是构图算法是<strong>逐点随机插入</strong>的，这就意味着<strong>在图构建的早期，很有可能构建出“高速公路”。</strong>一个点，越早插入就越容易形成与之相关的“高速公路”连接，越晚插入就越难形成与之相关的“高速公路”连接。这种方法<strong>不仅降低了构图算法时间复杂度的同时还带来了数量有限的“高速公路”，加速了查找。</strong></p><h3 id="NSW构图示例"><a href="#NSW构图示例" class="headerlink" title="NSW构图示例"></a>NSW构图示例</h3><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/hnsw4.png" alt="hnsw4"></p><p>如上图所示就是一个构建好的NSW网络图，我们说明一下过程：</p><ol><li>我们对7个二维点进行构图，用户设置m=3（每个点在插入时找3个紧邻友点）。</li><li>首先初始点是A点（随机出来的），A点插入图中只有它自己，所以无法挑选“友点”。</li><li>然后是B点，B点只有A点可选，所以连接BA，此为第1次构造。</li><li>然后插入F点，F只有A和B可以选，所以连接FA，FB，此为第2此构造。</li><li>然后插入了C点，同样地，C点只有A，B，F可选，连接CA，CB，CF，此为第3次构造。</li><li>重点来了，然后插入了E点，E点在A，B，F，C中只能选择3个点（m=3）作为“友点”，根据我们前面讲规则，要选最近的三个，怎么确定最近呢？<code>朴素查找</code>！从A，B，C，F任意一点出发，计算出发点与E的距离和出发点的所有“友点”和E的距离，选出最近的一点作为新的出发点，如果选出的点就是出发点本身，那么看我们的m等于几，如果不够数，就继续找第二近的点或者第三近的点，本着不找重复点的原则，直到找到3个近点为止。由此，我们找到了E的三个近点，连接EA，EC，EF，此为第四次构造。</li><li>第5次构造和第6次与E点的插入一模一样，都是在“现成”的图中查找到3个最近的节点作为“友点”，并做连接。</li></ol><p>当图构建完了，请关注E点和A点的连线，如果我再这个图的基础上再插入6个点，这6个点有3个和E很近，有3个和A很近，那么距离E最近的3个点中没有A，距离A最近的3个点中也没有E，但因为A和E是构图早期添加的点，A和E有了连线，我们管这种连线叫“高速公路”，在查找时可以提高查找效率</p><p><strong>NSW算法的优化</strong></p><ul><li>在查找的过程中，为了<strong>提高效率</strong>，我们可以建立一个废弃列表，在一次查找任务中遍历过的点不再遍历。在一次查找中，已经计算过这个点的所有友点距离查找点的距离，并且已经知道正确的跳转方向了，这些结果是唯一的，没有必要再去做走这个路径，因为这个路径会带给我们同样的重复结果，没有意义。</li><li>在查找过程中，为了<strong>提高准确度</strong>，我们可以建立一个动态列表，把距离查找点最近的n个点存储在表中，并行地对这n个点进行同时计算“友点”和待查找点的距离，在这些“友点”中选择n个点与动态列中的n个点进行并集操作，在并集中选出n个最近的友点，更新动态列表。</li></ul><p>由于插入过程之前会先进行查找，所以优化查找过程就是在优化插入过程。所以我们先来看<strong>NSW查找步骤</strong>。设待查找q点的m个近邻点：</p><ol><li>随机选一个点作为初始进入点，建立空废弃表g和动态列表c，g是变长的列表，c是定长为s的列表（s&gt;m）,将初始点放入动态列表c（附上初始点和待查找q的距离信息），制作动态列表的影子列表c’。</li><li>对动态列表c中的所有点并行找出其“友点”，查看这些“友点”是否存储在废弃表g中，如果存在，则丢弃，如不存在，将这些   剩余“友点”记录在废弃列表g中（以免后续重复查找，走冤枉路）。</li><li>并行计算这些剩余“友点”距离待查找点q的距离，将这些点及其各自的距离信息放入c。</li><li>对动态列表c去重，然后按距离排序（升序），储存前s个点及其距离信息。</li><li>查看动态列表c和c’是否一样，如果一样，结束本次查找，返回动态列表中前m个结果。如果不一样，将c’的内容更新为c的内容，执行第2步。</li></ol><blockquote><p>插入算法就是先用查找算法查找到m个（用户设置）与待插入点最近的点并连接。</p></blockquote><h2 id="4-跳表结构"><a href="#4-跳表结构" class="headerlink" title="4 跳表结构"></a>4 跳表结构</h2><p>设有有序链表，名叫<code>sorted_link</code>，里面有n个节点，每个节点是一个整数。</p><ul><li>我们从表头开始查找，查找第t（0&lt;t&lt;n）个节点需要跳转几次？答：t-1次（没错，我是从1开始数的）。</li><li>把n个节点分成n次查找的需求，都查找一遍，需要跳转几次？答：（0+1+2+3+…..+（n-1））次。</li></ul><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/hnsw5.png" alt="hnsw5"></p><p>但上图所示的已经不是一个有序链表了，这是三个有序链表+分层连接指针构成的跳表了。看这张示意图就能明白它的查找过程，先查第一层，然后查第二层，然后查第三层，然后找到结果。如果把上段所描述的名字叫 sorted_link 的链表建立成这样的跳表，那么把 sorted_link 中的所有元素都查一遍还需要花费（0+1+2+3+…..+（n-1））次吗？当然不需要。</p><blockquote><p><strong>跳表怎么构建呢？</strong></p></blockquote><p><em>抛硬币随机生成。</em></p><ol><li>对于 sorted_link 链表中的每个节点进行抛硬币，如抛正，则该节点进入上一层有序链表，每个 sorted_link 中的节点有50%的概率进入上一层有序链表。</li><li>将上一层有序链表中和 sorted_link 链表中相同的元素做一一对应的指针链接。</li><li>再从 sorted_link 上一层链表中再抛硬币，sorted_link 上一层链表中的节点有50%的可能进入最表层，相当于 sorted_link 中的每个节点有25%的概率进入最表层。以此类推。</li></ol><p>这种四件简单来看就是每层保留一定比例的节点，越往上层越少，相邻两层对应点相连。就保证了表层是“高速通道”，底层是精细查找，这个思想被应用到了NSW算法中，变成了HNSW。</p><h2 id="5-HNSW算法"><a href="#5-HNSW算法" class="headerlink" title="5 HNSW算法"></a>5 HNSW算法</h2><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/hnsw6.png" alt="hnsw6"></p><p>直接上图，现在看到这张图应该不陌生了。</p><ul><li>其中第0层中，是数据集中的所有点，你需要设置一个常数$m_l$，通过公式$floor(-ln(uniform(0,1)) \times m_l)$来计算这个点可以深入到第几层。</li><li>公式中，floor（）的含义是向下取整，uniform（0,1）的含义是在均匀分布中随机取出一个值，ln（）表示取对数。</li></ul><p><strong>梳理一下它的查找过程</strong>:</p><ol><li>从表层（上图中编号为Layer=2）<em>任意点开始</em>查找，选择进入点最邻近的一些友点，把它们存储在定长的动态列表中，别忘了把它们也同样在废弃表中存一份，以防后面走冤枉路。</li><li>一般地，在第 x 次查找时，先计算动态列表中所有点的友点距离待查找点（上图绿色点）的距离，在废弃列表中记录过的友点不要计算，计算完后更新废弃列表，不走冤枉路;</li><li>再把这些计算完的友点存入动态列表，去重排序，保留前k个点，看看这k个点和更新前的k个点是不是一样的，如果不是一样的，继续查找（说明还有更近的），如果是一样的，返回前 m 个结果。</li></ol><p><strong>插入构图的时候</strong>:先计算这个点可以深入到第几层，在每层的NSW图中查找 t 个最紧邻点，分别连接它们，对每层图都进行如此操作。</p><p><strong>算法参数</strong></p><ol><li>首先，插入时的动态列表c的大小，它的大小直接影响了插入效率，和构图的质量，size 越大，图的质量越高，构图和查找效率就越低。</li><li>其次，一个节点至少有几个<code>友点</code>，“友点”越多，图的质量越高，查找效率越低。</li><li>作者在论文中还提到了<code>max友点连接数</code>这个参数，设置一个节点至多有多少友点，来提高查找效率，但是设的太小又会影响图的质量，权衡着来。</li><li>上一段中的 $m_l$ 也是你来控制的，设置的大了，层数就少，内存消耗少，但严重影响效率，太大了会严重消耗内存和构图时间。</li></ol><p>在论文中，作者将查找状态下的<strong>动态列表长度和插入状态下的动态列表长度</strong>做了区分，你可以通过调整他们来实现“<strong>精构粗找</strong>”或者“<strong>精找粗构</strong>”。</p><p><strong>参考文章</strong><br><a href="http://yongyuan.name/blog/vector-ann-search.html">图像检索：向量索引</a><br><a href="https://blog.csdn.net/u011233351/article/details/85116719">一文看懂HNSW算法理论的来龙去脉</a></p><hr>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-引言&quot;&gt;&lt;a href=&quot;#1-引言&quot; class=&quot;headerlink&quot; title=&quot;1 引言&quot;&gt;&lt;/a&gt;1 引言&lt;/h2&gt;&lt;p&gt;在向量检索领域，&lt;code&gt;HNSW&lt;/code&gt;算法算是有比较重要的一席之地，很多地方都会用到，那么本文主要是对其具体构建的细节逻辑和背后的思想进行一个阐述和介绍。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1603.09320&quot;&gt;Hierarchical Navigable Small World Graphs (HNSW)&lt;/a&gt; 是Yury A. Malkov提出的一种基于图索引的方法，它是Yury A. Malkov在他本人之前工作&lt;code&gt;NSW&lt;/code&gt;上一种改进。通过采用层状结构，将边按特征半径进行分层，使每个顶点在所有层中平均度数变为常数，从而将 &lt;code&gt;NSW&lt;/code&gt; 的计算复杂度由多重对数(Polylogarithmic)复杂度降到了对数(logarithmic)复杂度。&lt;/p&gt;
&lt;h2 id=&quot;2-朴素思想&quot;&gt;&lt;a href=&quot;#2-朴素思想&quot; class=&quot;headerlink&quot; title=&quot;2 朴素思想&quot;&gt;&lt;/a&gt;2 朴素思想&lt;/h2&gt;&lt;p&gt;这里我们以一个小的场景为例来开始，假设我们现在有13个2维数据向量，我们把这些向量放在了一个平面直角坐标系内，隐去坐标系刻度，它们的位置关系如下图所示。&lt;/p&gt;</summary>
    
    
    
    <category term="学习笔记" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="算法总结" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="机器学习" scheme="https://www.xiemingzhao.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="算法" scheme="https://www.xiemingzhao.com/tags/%E7%AE%97%E6%B3%95/"/>
    
    <category term="向量检索" scheme="https://www.xiemingzhao.com/tags/%E5%90%91%E9%87%8F%E6%A3%80%E7%B4%A2/"/>
    
  </entry>
  
  <entry>
    <title>向量检索算法</title>
    <link href="https://www.xiemingzhao.com/posts/vecRetrieval.html"/>
    <id>https://www.xiemingzhao.com/posts/vecRetrieval.html</id>
    <published>2020-03-07T16:00:00.000Z</published>
    <updated>2025-03-31T16:50:45.528Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-基础知识"><a href="#1-基础知识" class="headerlink" title="1 基础知识"></a>1 基础知识</h2><h3 id="1-1-背景"><a href="#1-1-背景" class="headerlink" title="1.1 背景"></a>1.1 背景</h3><p>在深度学习大兴的时代，<code>embeding</code>无处不在，不论是在搜索推荐领域还是 cv 领域亦或 nlp 领域。俗话说得好，万物皆可embeding，那么对于 embeding 化后的对象我们在做 topk 检索/召回的时候怎么提效呢？毕竟候选集往往都是在千万级或者亿级的时候，计算量是相当大的。这时候，向量检索算法就发挥了作用。</p><p>向量检索一般可以分为两个方向:</p><ul><li>一种是<code>精确化</code>检索，需要遍历每个样本，计算量往往很大，基本上被淘汰了。</li><li>另一种就是<code>近似</code>检索，学术上对应的专有名词叫 Approximate Nearest Neighbor Search (ANNS)，即<code>近似最近邻搜索</code>。</li></ul><blockquote><p>为什么是近似，而不是我们想要的精确？</p></blockquote><p>这就是精度与时间、算力资源的折中，采用了牺牲精度换取时间和空间的方式，从海量的样本中实时获取跟查询最相似的样本。</p><span id="more"></span><h3 id="1-2-向量距离"><a href="#1-2-向量距离" class="headerlink" title="1.2 向量距离"></a>1.2 向量距离</h3><p>深度学习时代用的最多的就是 <code>embeding</code> 向量，而向量的基础形式往往就是一个数组，我们一般所需要的<strong>任务就是找出按照候选集中各个向量与目标向量的相似度倒排后的 topK 个</strong>。既然谈到向量之间的相似度，那么就要引入向量间的距离来进行刻画了。在实际中常用的向量距离主要有以下四种：</p><ol><li><p><code>欧式距离</code>(Euclidean Distance)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d_&#123;12&#125; = \sqrt&#123;\sum_&#123;k=1&#125;^n(x_&#123;1k&#125;-x_&#123;2k&#125;)^2&#125;</span><br></pre></td></tr></table></figure><p>两点间的真实距离，值越小，说明距离越近；</p></li><li><p><code>夹角余弦</code>(Cosine)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cos \theta = \frac&#123;x_1x_2 + y_1y_2&#125;&#123;\sqrt&#123;x_1^2 + y_1^2&#125; \sqrt&#123;x_2^2+y_2^2&#125;&#125;</span><br></pre></td></tr></table></figure><p>就是两个向量围成夹角的 cosine 值，cosine 值越大，越相似；</p></li><li><p><code>汉明距离</code>(Hamming distance)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d(x,y)=\sum x_i \oplus y_i</span><br></pre></td></tr></table></figure><p>一般作用于二值化向量，二值化的意思是向量的每一列只有 0 或者 1 两种取值。汉明距离的值就两个向量每列数值的异或和，值越小说明越相似，一般用于图片识别；</p></li><li><p><code>杰卡德相似系数</code>(Jaccard similarity coefficient)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">J(A,B)=\frac&#123;A \cap B&#125;&#123;A \cup B&#125;</span><br></pre></td></tr></table></figure><p>把向量作为一个集合，所以它可以不仅仅是数字代表，也可以是其他编码，比如词，该值越大说明越相似，一般用于相似语句识别。</p></li></ol><h3 id="1-3-Brute-Force"><a href="#1-3-Brute-Force" class="headerlink" title="1.3 Brute Force"></a>1.3 Brute Force</h3><p>提到向量检索，最简单，最准确但最低效的方法就是 <code>Brute Force</code>，顾名思义，<strong>就是暴力比对每一条向量的距离</strong>。一般可以作为其他检索方法的 baseline，尤其是独一召回率的参考。</p><p>值得一提的是阿里使用 <code>BinaryDocValues</code> 实现了 ES 上的 BF 插件。更进一步，要加速计算，所以使用了 JAVA Vector API 。JAVA Vector API 是在 openJDK project Panama 项目中的，它使用了 <code>SIMD</code> 指令优化。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jdk.panama.vector.*;</span><br><span class="line"><span class="keyword">import</span> jdk.panama.vector.Vector.Shape;</span><br><span class="line"></span><br><span class="line">Species = FloatVector.species(Shape.S_256_BIT);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 求平方和</span></span><br><span class="line">vecSquare = Species.broadcast(<span class="number">0</span>);</span><br><span class="line"><span class="keyword">for</span>(i=<span class="number">0</span>; i+(Species.length()) &lt;+ size; i+= Species.length())&#123;</span><br><span class="line">    <span class="comment">// 可以直接从byte数组计算，减少一步byte[]转floast[]</span></span><br><span class="line">    vec = Species.fromByteArray(array, i&gt;&gt;<span class="number">2</span>);</span><br><span class="line">    vecSquare = vec.mul(vec).add(vecSquare);</span><br><span class="line">    &#125;</span><br><span class="line">    sum = vecSquare.addAll();</span><br></pre></td></tr></table></figure><p>使用 avx2 指令优化，<em>100w 的 256 维向量，单分片比对，RT 在 260ms，是常规 BF 的 1/6</em>。 <code>ElasticSearch</code> 官方在7.3版本也发布了向量检索功能，底层也是基于 Lucene 的 BinaryDocValues，并且它还集成入了 painless 语法中，使用起来更加灵活。</p><h2 id="2-向量检索模型"><a href="#2-向量检索模型" class="headerlink" title="2 向量检索模型"></a>2 向量检索模型</h2><p>一个高效的向量检索模型往往需要满足下面三个条件才能达到工业级可用：</p><ul><li>实时查询，支持海量（百亿、千亿级别）规模库量级的；</li><li>存储高效，要求构建的向量索引模型数据压缩比高，达到大幅缩减内存使占用的目的；</li><li>召回精度好，top@K 有比较好的召回率，跟暴力搜索（brute-force search）的结果相比；</li></ul><p>前面提到向量检索按照思想可以分为精确检索和近似检索，这里按照召回具体方法分为四大类：<strong>基于树的方法、哈希方法、矢量量化方法、图索引量化方法。</strong></p><h3 id="2-1-基于树的方法"><a href="#2-1-基于树的方法" class="headerlink" title="2.1 基于树的方法"></a>2.1 基于树的方法</h3><p><code>KNN</code> 算法表示的是准确的召回 topK 的向量，这里主要有两种算法，一种是 <code>KDTtree</code>，一种是 <code>Brute Force</code>。我们首先分析了 KDTree 的算法，发现 KDTree 并不适合高维向量召回。</p><h4 id="2-1-1-KDTree-算法"><a href="#2-1-1-KDTree-算法" class="headerlink" title="2.1.1 KDTree 算法"></a>2.1.1 KDTree 算法</h4><p>简单来讲，就是把数据按照平面分割，并构造二叉树代表这种分割，在检索的时候，可以通过剪枝减少搜索次数。</p><h5 id="构建树KD"><a href="#构建树KD" class="headerlink" title="构建树KD"></a>构建树KD</h5><p>树选择从哪一维度进行开始划分的标准，采用的是求每一个维度的方差，然后选择<strong>方差最大的那个维度</strong>开始划分。</p><blockquote><p><em>为何要选择方差作为维度划分选取的标准？</em></p></blockquote><p>我们知道，方差的大小可以反映数据的波动性。方差大表示数据波动性越大，选择方差最大作为划分空间标准的好处在于，可以使得所需的划分面数目最小，反映到树的数据结构上，可以使得我们构建的 KD 树的树深度尽可能小。</p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/vecRetrieval1.jpg" alt="vecRetrieval1"></p><p>如上图为例，假设不以方差最大的 x 轴为划分面(x_var = 16.25)，而是以 y 轴(y_var = 0.0)为划分面，如图中虚线所示，可以看到，该划分使得图中的四个点都落入在同一个子空间中，从而使得该划分成为一个无效的划分，体现在以树结构上，就是多了一层无用的树深度。而以 x 轴为初始划分则不同(图像实线所示)，以 x 轴为初始划分可以得到数据能够比较均匀的散布在左右两个子空间中，使得整体的查找时间能够最短。</p><p><strong>注意，在实际的 kd 树划分的时候，并不是图中虚线所示，而是选取中值最近的点。</strong></p><p>以二维平面点 (x,y) 的集合 (2,3),(5,4),(9,6),(4,7),(8,1),(7,2) 为例构建树：</p><ol><li>构建根节点时，x 维度上面的方差较大，如上点集合在 x 维从小到大排序为 (2,3)，(4,7)，(5,4)，(7,2)，(8,1)，(9,6)；</li><li>其中值为 (7,2)。(2,3)，(4,7)，(5,4) 挂在 (7,2) 节点的左子树，(8,1)，(9,6) 挂在 (7,2) 节点的右子树。</li><li>构建 (7,2) 节点的左子树时，点集合 (2,3)，(4,7)，(5,4) 此时的切分维度为 y，中值为 (5,4) 作为分割平面，(2,3) 挂在其左子树，(4,7) 挂在其右子树。</li><li>构建 (7,2) 节点的右子树时，点集合 (8,1)，(9,6) 此时的切分维度也为 y，中值为 (9,6) 作为分割平面，(8,1) 挂在其左子树。</li></ol><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/vecRetrieval2.png" alt="vecRetrieval2"></p><p>上述的构建过程结合下图可以看出，构建一个 KDTree 即是将一个二维平面逐步划分的过程。</p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/vecRetrieval3.png" alt="vecRetrieval3"></p><h5 id="搜索-3-5-的最近邻"><a href="#搜索-3-5-的最近邻" class="headerlink" title="搜索 (3,5) 的最近邻"></a>搜索 (3,5) 的最近邻</h5><ol><li>首先从根节点 (7,2) 出发，将当前最近邻设为 (7,2)，对该 KDTree 作深度优先遍历。</li><li>以 (3,5) 为圆心，其到 (7,2) 的距离为半径画圆（多维空间为超球面），可以看出 (8,1) 右侧的区域与该圆不相交，所以 (8,1) 的右子树全部忽略。</li><li>接着走到 (7,2) 左子树根节点 (5,4)，与原最近邻对比距离后，更新当前最近邻为 (5,4)。</li><li>以 (3,5) 为圆心，其到 (5,4) 的距离为半径画圆，发现 (7,2) 右侧的区域与该圆不相交，忽略改厕所有节点，这样 (7,2) 的整个右子树被标记为已忽略。</li><li>遍历完 (5,4) 的左右叶子节点，发现与当前最优距离相等，不更新最近邻。所以 (3,5) 的最近邻为 (5,4)。</li></ol><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/vecRetrieval4.png" alt="vecRetrieval4"></p><p>KDTree 的查询复杂度为 $O(kn(k−1)/k)$，k 表示维度，n 表示数据量。说明 k 越大，复杂度越接近线性，所以它并<strong>不适合高维向量召回。</strong></p><h4 id="2-1-2-Annoy"><a href="#2-1-2-Annoy" class="headerlink" title="2.1.2 Annoy"></a>2.1.2 Annoy</h4><p><strong>Annoy 是 Erik Bernhardsson 写的一个以树为作为索引结构的近似最近邻搜索库</strong>，并用在 Spotify 的推荐系统中。</p><p><strong><code>Annoy</code> 的核心是不断用选取的两个质心的法平面对空间进行分割，最终将每一个划分的子空间里面的样本数据限制在K以内</strong>。</p><p>对于待插入的样本$x_i$，从根节点依次使用法向量跟 $x_i$ 做内积运算，从而判断使用法平面的哪一边（左子树 or 右子树）。对于查询向量 $q_i$，采用同样的方式（在树结构上体现为从根节点向叶子节点递归遍历），即可定位到跟 $q_i$ 在同一个子空间或者邻近的子空间的样本，这些样本即为 $q_i$ 近邻。</p><p>为了提高查询召回率，Annoy 采用建立<strong>多棵子树</strong>的方式，这种方式其实有点类似 AdaBoost 的意思，即用多个弱分类器构成强单元分类器，NV-tree 也采用了这种方式，哈希方法也同样采用了这种方式（构建多表）的方式。</p><p>值得注意的是，Annoy 如果不保存原始特征，则 Annoy 只能返回查询的 k 个近邻，至于这 k 个里面的排序顺序是怎么样的，它是不知道的，如果需要知道，需要对这 k 个返回的结果，获取原始特征，再计算各自的相似度，排序一下即可得到这 k 个结果的排序。</p><h3 id="2-2-基于Hash的方法"><a href="#2-2-基于Hash的方法" class="headerlink" title="2.2 基于Hash的方法"></a>2.2 基于Hash的方法</h3><p>哈希就是将连续的一系列值映射到较短的离散值，在这里一般是指映射到0、1值。根据学习的策略，可以将哈希方法分为无监督、有监督和半监督三种类型。</p><h4 id="2-2-1-LSH-Local-Sensitive-Hashing"><a href="#2-2-1-LSH-Local-Sensitive-Hashing" class="headerlink" title="2.2.1 LSH(Local Sensitive Hashing)"></a>2.2.1 LSH(Local Sensitive Hashing)</h4><p><code>LSH</code>的中文名称是<code>局部敏感哈希算法</code>，而哈希函数满意局部敏感的定义是在哈希后的空间内，<strong>相近的样本点对比相远的样本点更容易发生碰撞。个人理解就是会有一些聚类的效果</strong>。</p><p><strong>LSH加速检索的原理</strong><br>如下图所示，我们选取多个局部敏感 hash 函数，把候选集向量通过平面分割做 hash。0 表示点在平面的左侧，1 表示点在平面的右侧。由于应用了多个局部敏感 hash 函数，可以发现hash 值相同的点都比较相近，也即查询样本的最近邻将极有可能落在查询样本的 cell 中。所以我们在检索的时候只需要计算 hash 值相似的向量就能够召回较为精确度的 topK。</p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/vecRetrieval5.png" alt="vecRetrieval5"></p><blockquote><p><em>为什么要用多表哈希？</em></p></blockquote><p>对于单表哈希，当哈希函数数目K（上图中是2）取得太大，查询样本与其对应的最近邻落入同一个桶中的可能性会变得很微弱，针对这个问题，我们可以重复这个过程L次，从而增加最近邻的召回率。这个重复L次的过程，可以转化为构建L（上图中是3）个哈希表，这样在给定查询样本时，我们可以找到L个哈希桶（每个表找到一个哈希桶），然后我们在这L个哈希表中进行遍历。这个过程相当于构建了 $K \cdot L$ 个哈希函数(注意是“相当”，不要做“等价”理解)。</p><blockquote><p><em>多表哈希中哈希函数数目K和哈希表数目L如何选取？</em></p></blockquote><p>哈希函数数目K如果设置得过小，会导致每一个哈希桶中容纳了太多的数据点，从而增加了查询响应的时间；<br>而当K设置得过大时，会使得落入每个哈希桶中的数据点变小，而为了增加召回率，我们需要增加L以便构建更多的哈希表，但是哈希表数目的增加会导致更多的内存消耗，并且也使得我们需要计算更多的哈希函数，同样会增加查询相应时间。<br>但是在K过大或过小之间仍然可以找到一个比较合理的折中位置。通过选取合理的K和L，我们可以获得比线性扫描极大的性能提升。</p><h4 id="2-2-2-Multiprobe-LSH"><a href="#2-2-2-Multiprobe-LSH" class="headerlink" title="2.2.2 Multiprobe LSH"></a>2.2.2 Multiprobe LSH</h4><p>多 probe LSH 主要是为了提高<strong>查找准确率</strong>而引入的一种策略。</p><p><strong>原始的LSH</strong>: 是对于构建的L个哈希表，我们在每一个哈希表中找到查询样本落入的哈希桶，然后再在这个哈希桶中做遍历.</p><p><code>Multiprobe</code>: 指的是我们不止在查询样本所在的哈希桶中遍历，还会找到其他的一些哈希桶，然后这些找到的T个哈希桶中进行遍历。</p><p>这些其他哈希桶的<strong>选取准则是：跟查询样本所在的哈希桶邻近的哈希桶</strong>，“邻近”指的是<strong>汉明距离</strong>度量下的邻近。</p><blockquote><p>如果不使用 Multiprobe，我们需要的哈希表数目L在100到1000之间，在处理大数据集的时候，其空间的消耗会非常的高，幸运地是，因为有了上面的 Multiprobe 的策略，LSH在任意一个哈希表中查找到最近邻的概率变得更高，从而使得我们能到减少哈希表的构建数目。</p></blockquote><p><strong>对于LSH，涉及到的主要的参数有三个：</strong></p><ul><li>K，每一个哈希表的哈希函数（空间划分）数目</li><li>L，哈希表（每一个哈希表有K个哈希函数）的数目</li><li>T，近邻哈希桶的数目，即the number of probes</li></ul><p>这三个设置参数可以按照如下顺序进行：</p><ol><li>首先，根据可使用的内存大小选取L，然后在K和T之间做出折中：哈希函数数目K越大，相应地，近邻哈希桶的数目的数目T也应该设置得比较大，反之K越小，L也可以相应的减小。</li><li>获取K和L最优值的方式可以按照如下方式进行：对于每个固定的K，如果在查询样本集上获得了我们想要的精度，则此时T的值即为合理的值。</li><li>在对T进行调参的时候，我们不需要重新构建哈希表，甚至我们还可以采用二分搜索的方式来加快T参数的选取过程。</li></ol><h5 id="LSH开源实现"><a href="#LSH开源实现" class="headerlink" title="LSH开源实现"></a>LSH开源实现</h5><p>关于LSH开源工具库，有很多，这里推荐两个LSH开源工具包：LSHash 和 FALCONN, 分别对应于学习和应用场景。</p><p><strong>LSHash</strong><br>LSHash非常适合用来学习，里面实现的是最经典的LSH方法，并且还是单表哈希。<em>哈希函数的系数采用随机的方式生成</em>，具体代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_generate_uniform_planes</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Generate uniformly distributed hyperplanes and return it as a 2D</span></span><br><span class="line"><span class="string">    numpy array.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.random.randn(<span class="variable language_">self</span>.hash_size, <span class="variable language_">self</span>.input_dim)</span><br></pre></td></tr></table></figure><p>hash_size为哈希函数的数目，即前面介绍的K。如果要在实用中使用LSH，可以使用FALCONN。</p><h4 id="2-2-3-FALCONN"><a href="#2-2-3-FALCONN" class="headerlink" title="2.2.3 FALCONN"></a>2.2.3 FALCONN</h4><p><code>FALCONN</code> 是经过极致优化的 LSH，其对应的论文为 NIPS 2015 Practical and Optimal LSH for Angular Distance，Piotr Indyk 系作者之一，下面将其 Python 例子中初始化索引以及构建哈希表的部分提取出来，对其中的参数做一下简要的分析.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Hyperplane hashing</span></span><br><span class="line">params_hp = falconn.LSHConstructionParameters()</span><br><span class="line">params_hp.dimension = d</span><br><span class="line">params_hp.lsh_family = <span class="string">&#x27;hyperplane&#x27;</span></span><br><span class="line">params_hp.distance_function = <span class="string">&#x27;negative_inner_product&#x27;</span></span><br><span class="line">params_hp.storage_hash_table = <span class="string">&#x27;flat_hash_table&#x27;</span></span><br><span class="line">params_hp.k = <span class="number">19</span></span><br><span class="line">params_hp.l = <span class="number">10</span></span><br><span class="line">params_hp.num_setup_threads = <span class="number">0</span></span><br><span class="line">params_hp.seed = seed ^ <span class="number">833840234</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Hyperplane hash\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">start = timeit.default_timer()</span><br><span class="line">hp_table = falconn.LSHIndex(params_hp)</span><br><span class="line">hp_table.setup(data)</span><br><span class="line">hp_table.set_num_probes(<span class="number">2464</span>)</span><br></pre></td></tr></table></figure><p>有3个很重要的参数，分别是<strong>k、l 和 set_num_probes</strong>，对应的具体意义即前面所述。</p><p>性能方面 FALCONN 的索引构建过程非常快，百万量级数据，维度如果是128维，其构建索引时间大概2-3min的样子，实时搜索可以做到几毫秒响应时间。</p><p>对于小数据集和中型规模的数据集(几个million-几十个million)， FALCONN和NMSLIB是一个非常不错的选择，如果对于大型规模数据集(几百个million以上)，基于矢量量化的<code>Faiss</code>是一个明智的选择。</p><blockquote><p><em>遗留问题：</em>FALCONN 还不是很完善，比如对于数据的动态增删目前还不支持，NMSLIB 目前也不支持。</p></blockquote><p>一般而言，动态的增删在实际应用场合是一个基本的要求，但是我们应注意到，增删并不是毫无限制的，在增删频繁且持续了一段时间后，这时的<em>数据分布已经不是我们原来建索引的数据分布形式</em>了，我们应该重新构建索引。在这一点上，基于<strong>矢量量化的方法对数据的动态增删更友好</strong>。</p><p><strong>在召回率上一般哈希向量量化方法比矢量量化方法要差一些。</strong><br>一个比较直观的理解是：哈希向量量化后在计算距离的时候，计算的是汉明距离，在向量量化比特位长度相同的条件下，<strong>汉明距离表示的距离集合是有限的，而矢量量化计算的距离是一个实数</strong>，意味着它构成的距离集合是无限的。</p><p>所以，实际工业采用的向量索引方法，主要还是<strong>矢量量化方法居多</strong>，主要原因有二：</p><ul><li>矢量量化方法能够较好的兼顾检索召回率以及量化压缩比；</li><li>增删友好的优点使得构建的服务更稳定；</li></ul><h3 id="2-3-矢量量化方法"><a href="#2-3-矢量量化方法" class="headerlink" title="2.3 矢量量化方法"></a>2.3 矢量量化方法</h3><p>矢量量化方法，Vector Quantization，定义为：<strong>将一个向量空间中的点用其中的一个有限子集来进行编码的过程</strong>。其关键是码本的建立和码字搜索算法。</p><h4 id="2-3-1-多阶段矢量量化（MSVQ）"><a href="#2-3-1-多阶段矢量量化（MSVQ）" class="headerlink" title="2.3.1 多阶段矢量量化（MSVQ）"></a>2.3.1 多阶段矢量量化（MSVQ）</h4><p>多阶段矢量量化（Multi-Stage Vector Quantization，MSVQ）也称为<strong>残差矢量量化</strong>（Residual Vector Quantization, RVQ）。它是一种思想，即将编码任务分解为一系列级联过程。级联过程可以用下图直观的展示出来：</p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/vecRetrieval6.jpg" alt="vecRetrieval6"></p><p>如上图所示，图中quantizer为量化器，可以认为就是<code>码本生成器</code>。对于待量化的向量 x：</p><ol><li>经过一级量化器 quantizer1 后，得到的量化残差为 r1 = x - C1b1；</li><li>其中 C1 为一级量化器的码本，b1 为 x 经过一级量化器 quantizer1 后的表示结果；</li><li>将一级量化误差 r1 作为二级量化器的输入，后面过程与此类似。</li></ol><p>通过这种级联量化的量化方式，当构建的量化器为无穷个时，<strong>x可以被这无穷个码本精确表示</strong>。上图右侧子图比较直观的描绘了x被多个码本逐步近似的过程。</p><blockquote><p><strong>码本的构建和查询</strong><br>图中的C1、C2、…、Ci、… 这些码本在构建的时候，可以采用KMeans等方式得到各个量化器的码本。以上面构建的4个级联的码本为例，当得到码本C1、C2、C3、C4后，x量化的结果即可用[b1, b2, b3, b4]表示。对于xq查询向量与x距离的计算，在计算xq与 C1、C2、…、Ci、… 之间的内积距离表后，可以通过查表的方式，获取到非对称距离。</p></blockquote><p>这种多阶段级联的矢量量化方式，相比单阶段一次性量化，极大的降低了码本在训练过程中消耗的计算资源。</p><p>举个例子，4个阶段的MSVQ，每阶段用KMeans只需构建构建256大小的码本，则对空间分割构建的cell数目为$256^4=4294967296$，效率是很高的，但是如果采用单阶段一次性量化构建4294967296大小的码本，这个码本根本没法用KMeans聚出来。<br>此外在计算距离的时候，采用4阶段的<code>MSVQ</code>方式，只需计算4x256次距离的计算构成距离表，然后采用查表方式计算距离，而单阶段一次性量化需要计算4294967296次的距离计算。MSVQ的进一步加速版本是倒排MSVQ，将一级码本视为倒排链，从而构建倒排结构，构建MSVQ倒排结构。我们可以将MSVQ类比成“<strong>深度加深</strong>”的过程</p><h4 id="2-3-2-乘积量化-PQ"><a href="#2-3-2-乘积量化-PQ" class="headerlink" title="2.3.2 乘积量化(PQ)"></a>2.3.2 乘积量化(PQ)</h4><p><code>乘积量化(Product Quantization，PQ)</code>是 Herve Jegou 在2011年提出的一种非常经典实用的矢量量化索引方法，在工业界向量索引中已得到广泛的引用，并作为主要的向量索引方法，在 <a href="https://github.com/facebookresearch/faiss">Fasis</a> 有非常高效的实现。</p><p><strong>乘积量化的核心思想:分段（划分子空间）和聚类，或者说具体应用到ANN近似最近邻搜索上，KMeans 是 PQ 乘积量化子空间数目为1的特例。</strong></p><p>PQ乘积量化生成码本和量化的过程可以用如下图示来说明：</p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/vecRetrieval7.png" alt="vecRetrieval7"></p><p>在<strong>训练阶段</strong>:</p><ol><li>针对 N 个训练样本，假设样本维度为128维，我们将其切分为4个子空间，则每一个子空间的维度为32维;</li><li>然后我们在每一个子空间中，对子向量采用 <code>K-Means</code> 对其进行聚类(图中示意聚成256类)，这样每一个子空间都能得到一个码本。</li><li>这样训练样本的每个子段，都可以用子空间的聚类中心来近似，对应的<strong>编码即为类中心的ID</strong>。</li><li>如图所示，通过这样一种编码方式，训练样本仅使用的很短的一个编码得以表示，从而达到量化的目的。</li><li>对于待编码的样本，将它进行相同的切分，然后在各个子空间里逐一找到距离它们最近的类中心，然后用类中心的id来表示它们，即完成了待编码样本的编码。</li></ol><p>在<strong>查询阶段</strong>:</p><ol><li>PQ 同样在计算查询样本与 dataset 中各个样本的距离，只不过这种距离的计算转化为<strong>间接近似</strong>的方法而获得。</li><li>PQ 乘积量化方法在计算距离的时候，有两种距离计算方式，<strong>一种是对称距离，另外一种是非对称距离</strong>。</li><li>非对称距离的损失小(<em>也就是更接近真实距离</em>)，实际中也经常采用这种距离计算方式。</li></ol><p>下面过程示意的是查询样本来到时，以非对称距离的方式(红框标识出来的部分)计算到 dataset 样本间的计算示意：</p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/vecRetrieval8.png" alt="vecRetrieval8"></p><p><strong>查询向量来到时</strong>:</p><ol><li>按训练样本生成码本的过程，将其同样分成相同的子段，然后在每个子空间中，计算子段到该子空间中所有聚类中心得距离。(如图中所示，可以得到4x256个距离，这里为便于后面的理解说明，可以把这些算好的距离称作<code>距离表</code>)。</li><li>在计算库中某个样本到查询向量的距离时，比如编码为(124, 56, 132, 222)这个样本到查询向量的距离时，我们分别到距离表中取各个子段对应的距离即可。(比如编码为124这个子段，在第1个算出的256个距离里面把编号为124的那个距离取出来就可)。</li><li>所有子段对应的距离取出来后，将这些子段的距离求和相加，即得到该样本到查询样本间的非对称距离。</li><li>所有距离算好后，排序后即得到我们最终想要的结果。</li></ol><p><strong>PQ乘积量化能够加速索引的原理</strong>：即将全样本的距离计算，转化为到子空间类中心的距离计算。</p><ul><li>比如上面所举的例子，原本 brute-force search 的方式计算距离的次数随样本数目N成线性增长，但是经过PQ编码后，对于耗时的距离计算，只要计算4x256次，几乎可以忽略此时间的消耗。</li><li>另外，从上图也可以看出，对特征进行编码后，可以用一个相对比较短的编码来表示样本，自然对于内存的消耗要大大小于 brute-force search 的方式。</li></ul><p>在某些特殊的场合，我们总是希望获得精确的距离，而不是近似的距离，并且我们总是喜欢获取向量间的余弦相似度（余弦相似度距离范围在[-1,1]之间，便于设置固定的阈值），针对这种场景，可以针对PQ乘积量化得到的前top@K做一个 brute-force search 的排序。</p><h4 id="2-3-3-倒排乘积量化（IVFPQ）"><a href="#2-3-3-倒排乘积量化（IVFPQ）" class="headerlink" title="2.3.3 倒排乘积量化（IVFPQ）"></a>2.3.3 倒排乘积量化（IVFPQ）</h4><p><code>倒排PQ乘积量化</code>(IVFPQ)是PQ乘积量化的更进一步加速版。其加速的本质依然是<strong>加速原理：为了加快查找的速度，几乎所有的ANN方法都是通过对全空间分割，将其分割成很多小的子空间，在搜索的时候，通过某种方式，快速锁定在某一（几）子空间，然后在该（几个）子空间里做遍历</strong>。</p><p><strong>仔细观察PQ乘积量化存在一定的优化空间</strong>：</p><ul><li>其在计算距离的时候，虽然已经预先算好了，但是对于每个样本到查询样本的距离，还是得老老实实挨个去求和相加计算距离。</li><li>实际上我们感兴趣的是那些跟查询样本相近的样本（姑且称这样的区域为感兴趣区域），也就是说老老实实挨个相加其实做了很多的无用功。如果能够通过某种手段快速将全局遍历锁定为感兴趣区域，则可以舍去不必要的全局计算以及排序。</li><li>倒排PQ乘积量化的”倒排“，正是这样一种思想的体现，在具体实施手段上，采用的是<strong>通过聚类的方式实现感兴趣区域的快速定位</strong>，在倒排PQ乘积量化中，聚类可以说应用得淋漓尽致。</li></ul><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/vecRetrieval9.png" alt="vecRetrieval9"></p><p>如上图所示：<strong>在PQ乘积量化之前，增加了一个粗量化过程。</strong></p><p>具体地:</p><ol><li>先对N个训练样本采用 KMeans 进行聚类，这里聚类的数目一般设置得不应过大，一般设置为1024差不多，这种可以以比较快的速度完成聚类过程。</li><li>得到了聚类中心后，针对每一个样本$x_$i，找到其距离最近的类中心$c_i$后，两者相减得到样本$x_i$的残差向量$(x_i-c_i)$;</li><li>后面剩下的过程，就是针对$(x_i-c_i)$的PQ乘积量化过程。</li></ol><p>在查询的时候，通过相同的粗量化，可以快速定位到查询向量属于哪个$c_i$（即在哪一个感兴趣区域），然后在该感兴趣区域按上面所述的PQ乘积量化距离计算方式计算距离。</p><h4 id="2-3-4-最优乘积量化（OPQ）"><a href="#2-3-4-最优乘积量化（OPQ）" class="headerlink" title="2.3.4 最优乘积量化（OPQ）"></a>2.3.4 最优乘积量化（OPQ）</h4><p><code>最优乘积量化</code>（Optimal Product Quantization, OPQ）是PQ的一种改进版本。其改进体现在，<strong>致力于在子空间分割时，对各子空间的方差进行均衡</strong>。</p><p>Optimal 的过程一般实现为一个组件。而用于检索的原始特征维度较高，所以实际在使用PQ等方法构建索引的时候，常会对高维的特征使用<strong>PCA等降维</strong>方法对特征先做降维处理。</p><p><strong>这样降维预处理，可以达到两个目的：</strong></p><ul><li>一是降低特征维度；</li><li>二是在对向量进行子段切分的时候要求特征各个维度是不相关的，做完PCA之后，可以一定程度缓解这个问题。</li></ul><p>但是这么做了后，在切分子段的时候，采用顺序切分子段仍然存在一定的问题，这个问题可以借用ITQ中的一个二维平面的例子加以说明：</p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/vecRetrieval10.png" alt="vecRetrieval10"></p><p><strong>这个问题就是：</strong></p><ul><li>如上面a图所示，对于PCA降维后的二维空间，假设在做PQ的时候，将子段数目设置为2段，即切分成x和y两个子向量，然后分别在x和y上做聚类（假设聚类中心设置为2）。</li><li>对a图和c图聚类的结果进行比较，可以明显的发现，a图在y方向上聚类的效果明显差于c图，而PQ又是采用聚类中心来近似原始向量（这里指降维后的向量），也就是c图是我们需要的结果。</li></ul><p>这个问题可以转化为<strong>数据方差来描述：在做PQ编码时，对于切分的各个子空间，我们应尽可能使得各个子空间的方差比较接近，最理想的情况是各个子空间的方差都相等。</strong></p><p><strong>解决办法：为了在切分子段的时候，使得各个子空间的方差尽可能的一致。</strong></p><blockquote><p>Herve Jegou在<a href="https://lear.inrialpes.fr/pubs/2010/JDSP10/jegou_compactimagerepresentation.pdf">Aggregating local descriptors into a compact image representation</a>中提出使用一个正交矩阵来对PCA降维后的数据再做一次变换，使得各个子空间的方差尽可能的一致。</p></blockquote><p>OPQ致力于解决的问题正是对各个子空间方差的均衡。思想主要是<strong>在聚类的时候对聚类中心寻找对应的最优旋转矩阵，使得所有子空间中各个数据点到对应子空间的类中心的L2损失的求和最小。</strong></p><p>OPQ在具体求解的时候，分为非参求解方法和带参求解方法，具体为：</p><ul><li>非参求解方法。跟ITQ的求解过程一样。</li><li>带参求解方法。带参求解方法假设数据服从高斯分布，在此条件下，最终可以将求解过程简化为数据经过PCA分解后，特征值如何分组的问题。在实际中，该解法更具备高实用性。</li></ul><h4 id="2-4-基于图索引量化方法"><a href="#2-4-基于图索引量化方法" class="headerlink" title="2.4 基于图索引量化方法"></a>2.4 基于图索引量化方法</h4><p><code>图索引量化</code>是一种将图引入向量索引的方法，由于图索引的高召回特点，近几年基于图索引量化方法出现了不少这方面的工作，比如早期的 <code>KNNGraph</code> 和最近的将基于图索引量化方法推向成熟应用的 <code>HNSW</code> 方法。</p><p>首先从检索的<strong>召回率</strong>上来评估，基于图的索引方法要优于目前其他一些主流 ANN 搜索方法，比如乘积量化方法（PQ、OPQ）、哈希方法等。虽然乘积量化方法的召回率不如 HNSW，<strong>但由于乘积量化方法具备内存耗用更小、数据动态增删更灵活等特性</strong>，使得在工业检索系统中，在对召回率要求不是特别高的场景下，<em>乘积量化方法在工业界，仍然是使用得较多的一种索引方法。</em></p><p>下面是乘积量化和图索引方法特性做的一个对比，以HNSW和OPQ为典型代表：</p><div class="table-container"><table><thead><tr><th>特性</th><th>OPQ</th><th>HNSW</th></tr></thead><tbody><tr><td>内存占用</td><td>小</td><td>大</td></tr><tr><td>召回率</td><td>较高</td><td>高</td></tr><tr><td>数据动态增删</td><td>灵活</td><td>不易</td></tr></tbody></table></div><p>基于图索引的ANN方法由于数据在插入索引的时候，需要计算（部分）数据间的近邻关系，因而需要实时获取到到数据的原始特征，几乎所有基于图ANN的方法在处理该问题的时候，都是直接将原始特征加载在内存（索引）里，<strong>从而造成对内存使用过大，至于召回率图ANN方法要比基于量化的方法要高</strong>，这个理解起来比较直观。</p><h4 id="2-4-1-NSW"><a href="#2-4-1-NSW" class="headerlink" title="2.4.1 NSW"></a>2.4.1 NSW</h4><p>在介绍经典的HNSW之前我们先介绍它的前身NSW，该算法如下图，它是一个顺序构建图流程：</p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/vecRetrieval11.png" alt="vecRetrieval11"></p><p>我们以图中第5次构造D点为例来介绍流程：</p><ol><li>构建的时候，我们约定每次加入节点只连 3 条边，防止图变大，在实际使用中，要通过自身的数据；</li><li>随机一个节点，比如 A，保存下与 A 的距离，然后沿着 A 的边遍历，E 点最近，连边。然后再重新寻找，不能与之前重复，直到添加完 3 条边；</li></ol><p>查找流程包含在了插入流程中，一样的方式，只是不需要构建边，直接返回结果。</p><h4 id="2-4-2-HNSW"><a href="#2-4-2-HNSW" class="headerlink" title="2.4.2 HNSW"></a>2.4.2 HNSW</h4><p><a href="https://arxiv.org/abs/1603.09320">Hierarchical Navigable Small World Graphs (HNSW)</a> 是Yury A. Malkov提出的一种基于图索引的方法，它是Yury A. Malkov在他本人之前工作<code>NSW</code>上一种改进。通过采用层状结构，将边按特征半径进行分层，使每个顶点在所有层中平均度数变为常数，从而将NSW的计算复杂度由多重对数(Polylogarithmic)复杂度降到了对数(logarithmic)复杂度。</p><p><strong>HNSW的主要贡献如下：</strong></p><ul><li>图输入节点明确的选择；</li><li>使用不同尺度划分链接；</li><li>使用启发式方式来选择最近邻；</li></ul><h5 id="近邻图技术"><a href="#近邻图技术" class="headerlink" title="近邻图技术"></a>近邻图技术</h5><p>对于给定的近邻图，在开始搜索的时候，从若干输入点（随机选取或分割算法）开始迭代遍历整个近邻图。</p><p>在每一次横向迭代的时候，算法会检查链接或当前base节点之间的距离，然后选择下一个base节点作为相邻节点，使得能最好的最小化连接间的距离。</p><p>近邻图主要的缺陷：</p><ol><li>在路由阶段，如果随机从一个或者固定的阶段开始，迭代的步数会随着库的大小增长呈现幂次增加；</li><li>当使用k-NN图的时候，一个全局连接可能的损失会导致很差的搜索结果。</li></ol><h5 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h5><p>网络图以连续插入的方式构建。对于每一个要插入的元素，采用指数衰变概率分布函数来随机选取整数最大层。HNSW 算法是 NSW 算法的分层优化，借鉴了 skiplist 算法的思想，提升查询性能，开始先从稀疏的图上查找，逐渐深入到底层的图。</p><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/vecRetrieval12.jpg" alt="vecRetrieval12"></p><ul><li>图构建元素<strong>插入过程</strong>（Algorithm 1）：从顶层开始贪心遍历graph，以便在某层A中找到最近邻。当在A层找到局部最小值之后，再将A层中找到的最近邻作为输入点（entry point），继续在下一层中寻找最近邻，重复该过程；</li><li><strong>层内最近邻查找</strong>（Algorithm 2）：贪心搜索的改进版本；</li><li><strong>搜索阶段</strong>，维护一个动态列表，用于保持ef个找到的最近邻元素</li></ul><blockquote><p>在搜索的初步阶段，ef参数设置为1。搜索过程包括zoom out和zoom in两个阶段，zoom out是远程路由，zoom in顾名思义就是在定位的区域做精细的搜索过程。</p></blockquote><p>整个过程可以类比在地图上寻找某个位置的过程：</p><ol><li>我们可以把地球当做最顶层，五大洲作为第二层，国家作为第三层，省份作为第四层……;</li><li>现在如果要找海淀五道口，我们可以通过顶层以逐步递减的特性半径对其进行路由（第一层地球-&gt;第二层亚洲—&gt;第三层中国-&gt;第四层北京-&gt;海淀区）;</li><li>到了第0层后，再在局部区域做更精细的搜索。</li></ol><p><strong>参数详细意义</strong></p><ol><li>M：参数M定义了第0层以及其他层近邻数目，不过实际在处理的时候，第0层设置的近邻数目是2xM。如果要更改第0层以及其他层层近邻数目，在HNSW的源码中进行更改即可。另外需要注意的是，第0层包含了所有的数据点，其他层数据数目由参数mult定义，详细的细节可以参考HNSW论文。</li><li>delaunay_type：检索速度和索引速度可以通过该参数来均衡heuristic。HNSW默认delaunay_type为1，将delaunay_type设置为1可以提高更高的召回率(&gt; 80%)，但同时会使得索引时间变长。因此，对于召回率要求不高的场景，推荐将delaunay_type设置为0。</li><li>post：post定义了在构建图的时候，对数据所做预处理的数量（以及类型），默认参数设置为0，表示不对数据做预处理，该参数可以设置为1和2（2表示会做更多的后处理）。</li></ol><p>更详细的参数说明，可以参考<a href="https://github.com/nmslib/nmslib/blob/9ed3071d0a74156a9559f3347ee751922e4b06e7/python_bindings/parameters.md">parameters</a>说明。</p><h4 id="2-4-3-NSG算法"><a href="#2-4-3-NSG算法" class="headerlink" title="2.4.3 NSG算法"></a>2.4.3 NSG算法</h4><p>NSG 全写为 Navigating Spreading-out Graph。NSG 围绕四个方向来改进：</p><ul><li>图的连通性;</li><li>减少出度;</li><li>缩短搜索路径;</li><li>缩减图的大小。</li></ul><p>具体是通过建立<strong>导航点 (Navigation Point)，</strong>特殊的选边策略, 深度遍历收回<strong>离散节点（Deep Traversal）等方法</strong>。</p><ol><li>首先是 <code>Navigation Point</code>，在建图时，首先需要一张预先建立的 <code>K-nearest-neighbor-graph (KNNG)</code>作为构图基准。</li><li>随机选择一个点作为 <code>Navigation Point</code>，后续所有新插入的节点在选边时都会将 <code>Navigation Point</code> 加入候选。</li><li>在建图过程中，逐渐会将子图都和 Navigation point 相连接，这样其他的节点只需保持很少的边即可，从而减少了图的大小。</li><li>每次搜索从 <code>Navigation Point</code> 出发能够指向具体的子图，从而减少无效搜索，获得更好搜索性能。</li></ol><p>NSG 使用的择边策略与 HNSW 类似，<strong>但是不同于 HNSW 只选择最短边为有效边，NSG 使用的择边策略如下图。</strong></p><ul><li>以点 r 为例，当 r 与 p 建立连接时，以 r 和 p 为圆心，r 和 p 的距离为半径，分别做圆，如果两个圆的交集内没有其他与 p 相连接的点，则 r 与 p 相连（见图 3-B）。</li><li>在连接点 s 时，由于以 s 和 p 距离为半径的交集圆内，已有点 r 与 p 相连，所以 s 和 p 不相连（见图 3-C）。下图中最终与点 p 相连的点只有 r, t 和 q（见图 3-A）。</li></ul><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/vecRetrieval13.png" alt="vecRetrieval13"></p><p><code>NSG</code> 这样做的原因是<strong>考虑到由于边数一多，整个图就会变得稠密</strong>，最后在搜索时会浪费大量算力。但是减少边的数量，<strong>带来的坏处也比较明显，最后的图会是一张稀疏图</strong>，会使得有些节点难以被搜索到。</p><p>不仅如此，NSG 的边还是单向边。在如此激进的策略下，图的连通性就会产生问题，这时 NSG 选择使用深度遍历来将离群的节点收回到图中。通过以上步骤，图的建立就完成了。</p><h5 id="HNSW-和-NSG-的比较"><a href="#HNSW-和-NSG-的比较" class="headerlink" title="HNSW 和 NSG 的比较"></a>HNSW 和 NSG 的比较</h5><p><code>HNSW</code> 从结构入手，利用分层图提高图的导航性减少无效计算从而降低搜索时间，达到优化的目的。</p><p>而 <code>NSG</code> 选择将图的整体度数控制在尽可能小的情况下，提高导航性，缩短搜索路径来提高搜索效率。</p><p>下面我们从<strong>内存占用，搜索速度，精度</strong>等方面来比较 HNSW 和 NSG。</p><ol><li><code>HNSW</code> 由于多层图的结构以及连边策略，<em>导致搜索时内存占用量会大于 NSG</em>，在内存受限场景下选择 NSG 会更好。</li><li>但是 <code>NSG</code> 在建图过程中无论内存占用还是耗时都大于 HNSW。此外 HNSW 还拥有目前 NSG 不支持的特性，<em>即增量索引，虽然耗时巨大</em>。</li><li>对比其他的索引类型，无论 NSG 还是 HNSW 在搜索时间和精度两个方面，都有巨大优势。</li></ol><p>目前在 Milvus 内部已经实现了 NSG 算法，并将 KNNG 计算放到了 GPU 上进行从而极大地加快了 NSG 图的构建。</p><h2 id="3-算法总结"><a href="#3-算法总结" class="headerlink" title="3 算法总结"></a>3 算法总结</h2><p><strong>KNN 适合场景：</strong></p><ul><li>数据量小(单分片100w以下)；</li><li>先过滤其他条件，只剩少量数据，再向量召回的场景；</li><li>召回率100%；</li></ul><p><strong>ANN 场景：</strong></p><ul><li>数据量大(千万级以上)；</li><li>先向量过滤再其他过滤；</li><li>召回率不需要100%；</li><li>LSH 算法： 召回率性能要求不高，少量增删；</li><li>IVFPQ 算法：召回率性能要求高，数据量大(千万级)，少量增删，需要提前构建；</li><li>HNSW 算法： 召回率性能要求搞，数据量适中(千万以下)，索引全存内存，内存够用；</li></ul><p><strong>参考文章</strong><br><a href="https://www.sofastack.tech/blog/antfin-zsearch-vector-search/">蚂蚁金服 ZSearch 在向量检索上的探索</a><br><a href="https://www.6aiq.com/article/1587522027341">大规模特征向量检索算法总结 (LSH PQ HNSW)</a><br><a href="http://yongyuan.name/blog/vector-ann-search.html">图像检索：向量索引</a><br><a href="https://milvus.io/cn/blogs/2020-01-16-hnsw-nsg-comparison.md">Milvus 揭秘系列（一）：向量索引算法 HNSW 和 NSG 的比较</a></p><hr>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-基础知识&quot;&gt;&lt;a href=&quot;#1-基础知识&quot; class=&quot;headerlink&quot; title=&quot;1 基础知识&quot;&gt;&lt;/a&gt;1 基础知识&lt;/h2&gt;&lt;h3 id=&quot;1-1-背景&quot;&gt;&lt;a href=&quot;#1-1-背景&quot; class=&quot;headerlink&quot; title=&quot;1.1 背景&quot;&gt;&lt;/a&gt;1.1 背景&lt;/h3&gt;&lt;p&gt;在深度学习大兴的时代，&lt;code&gt;embeding&lt;/code&gt;无处不在，不论是在搜索推荐领域还是 cv 领域亦或 nlp 领域。俗话说得好，万物皆可embeding，那么对于 embeding 化后的对象我们在做 topk 检索/召回的时候怎么提效呢？毕竟候选集往往都是在千万级或者亿级的时候，计算量是相当大的。这时候，向量检索算法就发挥了作用。&lt;/p&gt;
&lt;p&gt;向量检索一般可以分为两个方向:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一种是&lt;code&gt;精确化&lt;/code&gt;检索，需要遍历每个样本，计算量往往很大，基本上被淘汰了。&lt;/li&gt;
&lt;li&gt;另一种就是&lt;code&gt;近似&lt;/code&gt;检索，学术上对应的专有名词叫 Approximate Nearest Neighbor Search (ANNS)，即&lt;code&gt;近似最近邻搜索&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;为什么是近似，而不是我们想要的精确？&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这就是精度与时间、算力资源的折中，采用了牺牲精度换取时间和空间的方式，从海量的样本中实时获取跟查询最相似的样本。&lt;/p&gt;</summary>
    
    
    
    <category term="学习笔记" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="算法总结" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="机器学习" scheme="https://www.xiemingzhao.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="算法" scheme="https://www.xiemingzhao.com/tags/%E7%AE%97%E6%B3%95/"/>
    
    <category term="向量检索" scheme="https://www.xiemingzhao.com/tags/%E5%90%91%E9%87%8F%E6%A3%80%E7%B4%A2/"/>
    
  </entry>
  
  <entry>
    <title>位运算的巧用</title>
    <link href="https://www.xiemingzhao.com/posts/bitArithmetic.html"/>
    <id>https://www.xiemingzhao.com/posts/bitArithmetic.html</id>
    <published>2020-01-15T16:00:00.000Z</published>
    <updated>2025-03-30T17:52:57.112Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-位运算简介"><a href="#1-位运算简介" class="headerlink" title="1 位运算简介"></a>1 位运算简介</h2><ul><li><p><code>&amp;</code> 与运算 两个位都是 1 时，结果才为 1，否则为 0，如：<br><code>10011&amp;11001=10001</code></p></li><li><p><code>|</code> 或运算 两个位都是 0 时，结果才为 0，否则为 1，如 <code>10011 | 11001 = 11011</code></p></li><li><p><code>^</code> 异或运算，两个位相同则为 0，不同则为 1，如 <code>10011 ^ 11001 = 01010</code></p></li><li><p><code>~</code> 取反运算，0 则变为 1，1 则变为 0，如<code>~ 10011 = 01100</code></p></li><li><p><code>&lt;&lt;</code> 左移运算，向左进行移位操作，高位丢弃，低位补 0，如</p></li></ul><span id="more"></span><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">int a = 8;</span><br><span class="line">a &lt;&lt; 3;</span><br><span class="line">移位前：0000 0000 0000 0000 0000 0000 0000 1000</span><br><span class="line">移位后：0000 0000 0000 0000 0000 0000 0100 0000</span><br></pre></td></tr></table></figure><ul><li><code>&gt;&gt;</code> 右移运算，向右进行移位操作，对无符号数，高位补 0，对于有符号数，高位补符号位，如<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">unsigned int a = 8;</span><br><span class="line">a &gt;&gt; 3;</span><br><span class="line">移位前：0000 0000 0000 0000 0000 0000 0000 1000</span><br><span class="line">移位后：0000 0000 0000 0000 0000 0000 0000 0001</span><br><span class="line"></span><br><span class="line">int a = -8;</span><br><span class="line">a &gt;&gt; 3;</span><br><span class="line">移位前：1111 1111 1111 1111 1111 1111 1111 1000</span><br><span class="line">移位前：1111 1111 1111 1111 1111 1111 1111 1111</span><br></pre></td></tr></table></figure></li></ul><h2 id="2-位运算巧解问题"><a href="#2-位运算巧解问题" class="headerlink" title="2 位运算巧解问题"></a>2 位运算巧解问题</h2><h3 id="2-1-位操作实现乘除法数"><a href="#2-1-位操作实现乘除法数" class="headerlink" title="2.1 位操作实现乘除法数"></a>2.1 位操作实现乘除法数</h3><blockquote><p>a 向右移一位，相当于将 a 除以 2；数 a 向左移一位，相当于将 a 乘以 2<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">int a = 2;</span><br><span class="line">a &gt;&gt; 1; ---&gt; 1</span><br><span class="line">a &lt;&lt; 1; ---&gt; 4</span><br></pre></td></tr></table></figure></p></blockquote><h3 id="2-2-位操作交换两数"><a href="#2-2-位操作交换两数" class="headerlink" title="2.2 位操作交换两数"></a>2.2 位操作交换两数</h3><blockquote><p>位操作交换两数可以不需要第三个临时变量，虽然普通操作也可以做到，但是没有其效率高<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">//普通操作</span><br><span class="line">void swap(int &amp;a, int &amp;b) &#123;</span><br><span class="line">  a = a + b;</span><br><span class="line">  b = a - b;</span><br><span class="line">  a = a - b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//位与操作</span><br><span class="line">void swap(int &amp;a, int &amp;b) &#123;</span><br><span class="line">  a ^= b;</span><br><span class="line">  b ^= a;</span><br><span class="line">  a ^= b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>位与操作解释：</p><ul><li>第一步：a ^= b —-&gt; a = (a^b);</li><li>第二步：b ^= a —-&gt; b = b^(a^b) —-&gt; b = (b^b)^a = a</li><li>第三步：a ^= b —-&gt; a = (a^b)^a = (a^a)^b = b</li></ul></blockquote><h3 id="2-3-位操作判断奇偶数"><a href="#2-3-位操作判断奇偶数" class="headerlink" title="2.3 位操作判断奇偶数"></a>2.3 位操作判断奇偶数</h3><blockquote><p>只要根据数的最后一位是 0 还是 1 来决定即可，为 0 就是偶数，为 1 就是奇数。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if(0 == (a &amp; 1)) &#123;</span><br><span class="line"> //偶数</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p></blockquote><h3 id="2-4-位操作交换符号"><a href="#2-4-位操作交换符号" class="headerlink" title="2.4 位操作交换符号"></a>2.4 位操作交换符号</h3><blockquote><p>交换符号将正数变成负数，负数变成正数<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">int reversal(int a) &#123;</span><br><span class="line">  return ~a + 1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>整数取反加1，正好变成其对应的负数(补码表示)；负数取反加一，则变为其原码，即正数</p></blockquote><h3 id="2-5-位操作求绝对值"><a href="#2-5-位操作求绝对值" class="headerlink" title="2.5 位操作求绝对值"></a>2.5 位操作求绝对值</h3><blockquote><p>整数的绝对值是其本身，负数的绝对值正好可以对其进行取反加一求得，即我们首先判断其符号位（整数右移 31 位得到 0，负数右移 31 位得到 -1,即 0xffffffff），然后根据符号进行相应的操作<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">int abs(int a) &#123;</span><br><span class="line">  int i = a &gt;&gt; 31;</span><br><span class="line">  return i == 0 ? a : (~a + 1);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>上面的操作可以进行优化，可以将 i == 0 的条件判断语句去掉。我们都知道符号位 i 只有两种情况，即 i = 0 为正，i = -1 为负。对于任何数与 0 异或都会保持不变，与 -1 即 0xffffffff 进行异或就相当于对此数进行取反,因此可以将上面三目元算符转换为((a^i)-i)，即整数时 a 与 0 异或得到本身，再减去 0，负数时与 0xffffffff 异或将 a 进行取反，然后在加上 1，即减去 i(i =-1)<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">int abs2(int a) &#123;</span><br><span class="line">  int i = a &gt;&gt; 31;</span><br><span class="line">  return ((a^i) - i);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p></blockquote><h3 id="2-6-位操作进行高低位交换"><a href="#2-6-位操作进行高低位交换" class="headerlink" title="2.6 位操作进行高低位交换"></a>2.6 位操作进行高低位交换</h3><p>给定一个 16 位的无符号整数，将其高 8 位与低 8 位进行交换，求出交换后的值，如：</p><blockquote><p>34520的二进制表示：<br>10000110 11011000</p><p>将其高8位与低8位进行交换，得到一个新的二进制数：<br>11011000 10000110<br>其十进制为55430</p></blockquote><p>从上面移位操作我们可以知道，</p><ul><li>只要将无符号数 a&gt;&gt;8 即可得到其高 8 位移到低 8 位，高位补 0；</li><li>将 a&lt;&lt;8 即可将 低 8 位移到高 8 位，低 8 位补 0;</li><li>然后将 a&gt;&gt;8 和 a&lt;&lt;8 进行或操作既可求得交换后的结果。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">unsigned short a = 34520;</span><br><span class="line">a = (a &gt;&gt; 8) | (a &lt;&lt; 8);</span><br></pre></td></tr></table></figure><h3 id="2-7-位操作进行二进制逆序"><a href="#2-7-位操作进行二进制逆序" class="headerlink" title="2.7 位操作进行二进制逆序"></a>2.7 位操作进行二进制逆序</h3><p>将无符号数的二进制表示进行逆序，求取逆序后的结果，如</p><blockquote><p>数34520的二进制表示：10000110 11011000</p><p>逆序后则为：00011011 01100001<br><em>它的十进制为7009</em></p></blockquote><p>在字符串逆序过程中，可以从字符串的首尾开始，依次交换两端的数据。在二进制中使用位的高低位交换会更方便进行处理，这里我们分组进行多步处理。</p><ul><li><p>第一步:以每 2 位为一组，组内进行高低位交换</p><blockquote><p>交换前： 10 00 01 10 11 01 10 00<br>交换后： 01 00 10 01 11 10 01 00</p></blockquote></li><li><p>第二步：在上面的基础上，以每 4 位为 1 组，组内高低位进行交换</p><blockquote><p>交换前： 0100 1001 1110 0100<br>交换后： 0001 0110 1011 0001</p></blockquote></li><li><p>第三步：以每 8 位为一组，组内高低位进行交换</p><blockquote><p>交换前： 00010110 10110001<br>交换后： 01100001 00011011</p></blockquote></li><li><p>第四步：以每16位为一组，组内高低位进行交换</p><blockquote><p>交换前： 0110000100011011<br>交换后： 0001101101100001</p></blockquote></li></ul><p>对于上面的第一步，依次以 2 位作为一组，再进行组内高低位交换，这样处理起来<strong>比较繁琐</strong>，下面介绍另外一种方法进行处理。</p><p>先分别取原数 10000110 11011000 的奇数位和偶数位，将空余位用 0 填充：</p><blockquote><p>原数：  10000110 11011000<br>奇数位： 10000010 10001000<br>偶数位： 00000100 01010000</p></blockquote><p>再将奇数位右移一位，偶数位左移一位，此时将两个数据相或即可以达到奇偶位上数据交换的效果：</p><blockquote><p>原数：  10000110 11011000<br>奇数位右移一位： 0 10000010 1000100<br>偶数位左移一位：0000100 01010000 0<br>两数相或得到： 01001001 11100100</p></blockquote><p>上面的方法用位操作可以表示为：</p><ul><li>取a的奇数位并用 0 进行填充可以表示为：a &amp; 0xAAAA</li><li>取a的偶数为并用 0 进行填充可以表示为：a &amp; 0x5555</li><li>因此，上面的第一步可以表示为：a = ((a &amp; 0xAAAA) &gt;&gt; 1) | ((a &amp; 0x5555) &lt;&lt; 1)</li><li>同理，可以得到其第二、三和四步为：<blockquote><p>a = ((a &amp; 0xCCCC) &gt;&gt; 2) | ((a &amp; 0x3333) &lt;&lt; 2)<br>a = ((a &amp; 0xF0F0) &gt;&gt; 4) | ((a &amp; 0x0F0F) &lt;&lt; 4)<br>a = ((a &amp; 0xFF00) &gt;&gt; 8) | ((a &amp; 0x00FF) &lt;&lt; 8)</p></blockquote></li></ul><p>因此整个操作为：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">unsigned short a = 34520;</span><br><span class="line"></span><br><span class="line">a = ((a &amp; 0xAAAA) &gt;&gt; 1) | ((a &amp; 0x5555) &lt;&lt; 1);</span><br><span class="line">a = ((a &amp; 0xCCCC) &gt;&gt; 2) | ((a &amp; 0x3333) &lt;&lt; 2);</span><br><span class="line">a = ((a &amp; 0xF0F0) &gt;&gt; 4) | ((a &amp; 0x0F0F) &lt;&lt; 4);</span><br><span class="line">a = ((a &amp; 0xFF00) &gt;&gt; 8) | ((a &amp; 0x00FF) &lt;&lt; 8);</span><br></pre></td></tr></table></figure></p><h3 id="2-8-位操作统计二进制中-1-的个数"><a href="#2-8-位操作统计二进制中-1-的个数" class="headerlink" title="2.8 位操作统计二进制中 1 的个数"></a>2.8 位操作统计二进制中 1 的个数</h3><p>统计二进制1的个数可以分别获取每个二进制位数，然后再统计其1的个数，此方法效率比较低。这里介绍另外一种高效的方法，同样以 34520 为例，我们计算其 <code>a &amp;= (a-1)</code>的结果：</p><ul><li>第一次：计算前：1000 0110 1101 1000 计算后：1000 0110 1101 0000</li><li>第二次：计算前：1000 0110 1101 0000 计算后：1000 0110 1100 0000</li><li>第三次：计算前：1000 0110 1100 0000 计算后：1000 0110 1000 0000</li></ul><p>我们发现，每计算一次二进制中就少了一个 1，则我们可以通过下面方法去统计：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">count = 0  </span><br><span class="line">while(a)&#123;  </span><br><span class="line">  a = a &amp; (a - 1);  </span><br><span class="line">  count++;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>参考文章</strong><br><a href="https://www.zhihu.com/question/38206659">位运算有什么奇技淫巧？</a></p><hr>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-位运算简介&quot;&gt;&lt;a href=&quot;#1-位运算简介&quot; class=&quot;headerlink&quot; title=&quot;1 位运算简介&quot;&gt;&lt;/a&gt;1 位运算简介&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;&amp;amp;&lt;/code&gt; 与运算 两个位都是 1 时，结果才为 1，否则为 0，如：&lt;br&gt;&lt;code&gt;10011&amp;amp;11001=10001&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;|&lt;/code&gt; 或运算 两个位都是 0 时，结果才为 0，否则为 1，如 &lt;code&gt;10011 | 11001 = 11011&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;^&lt;/code&gt; 异或运算，两个位相同则为 0，不同则为 1，如 &lt;code&gt;10011 ^ 11001 = 01010&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;~&lt;/code&gt; 取反运算，0 则变为 1，1 则变为 0，如&lt;code&gt;~ 10011 = 01100&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;&amp;lt;&amp;lt;&lt;/code&gt; 左移运算，向左进行移位操作，高位丢弃，低位补 0，如&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="学习笔记" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="算法总结" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="算法" scheme="https://www.xiemingzhao.com/tags/%E7%AE%97%E6%B3%95/"/>
    
    <category term="位运算" scheme="https://www.xiemingzhao.com/tags/%E4%BD%8D%E8%BF%90%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>稀疏矩阵(Sparse Matrix)</title>
    <link href="https://www.xiemingzhao.com/posts/sparseMatrix.html"/>
    <id>https://www.xiemingzhao.com/posts/sparseMatrix.html</id>
    <published>2020-01-07T16:00:00.000Z</published>
    <updated>2025-03-30T17:51:56.076Z</updated>
    
    <content type="html"><![CDATA[<p>-</p><h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1 背景"></a>1 背景</h2><p>在企业的深度学习项目中，<code>Sparse稀疏矩阵</code>这个词想必大家都不陌生。在模型的矩阵计算中，往往会遇到矩阵较为庞大且非零元素较少。由其是现在深度学习中embedding大行其道，稀疏矩阵成为必不可少的基建。而这种情况下，如果依然使用dense的矩阵进行存储和计算将是极其低效且耗费资源的。Sparse稀疏矩阵就称为了救命稻草。在拜读多篇优秀博客后，这里做一些自己的汇总和填补。</p><h2 id="2-稀疏矩阵"><a href="#2-稀疏矩阵" class="headerlink" title="2 稀疏矩阵"></a>2 稀疏矩阵</h2><h3 id="2-1-定义"><a href="#2-1-定义" class="headerlink" title="2.1 定义"></a>2.1 定义</h3><span id="more"></span><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/sparseMatrix1.png" alt="sparseMatrix1"></p><p>如上所示，一般当矩阵非零项较少的时候，就称为非零矩阵，也即其中只有少量的有用信息-非零项。</p><p>那么可以做一个更为书面的<strong>定义：具有少量非零项的矩阵 - Number of Non-Zero (NNZ) &lt; 0.5</strong></p><blockquote><p>在矩阵中，若数值0的元素数目远多于非0元素的数目，并且非0元素分布没有规律。</p></blockquote><p><strong>矩阵的稠密度</strong></p><blockquote><p>非零元素的总数比上矩阵所有元素的总数为矩阵的稠密度。</p></blockquote><h3 id="2-2-压缩存储"><a href="#2-2-压缩存储" class="headerlink" title="2.2 压缩存储"></a>2.2 压缩存储</h3><p>存储矩阵的一般方法是采用二维数组，其优点是可以随机地访问每一个元素，因而能够容易实现矩阵的各种运算，如转置运算、加法运算、乘法运算等。</p><p>对于稀疏矩阵，它通常具有很大的维度，有时甚大到整个矩阵（零元素）占用了绝大部分内存，采用二维数组的存储方法既浪费大量的存储单元来存放零元素，又要在运算中浪费大量的时间来进行零元素的无效运算。因此必须考虑对稀疏矩阵进行压缩存储（只存储非零元素）。</p><p>我们可以通过<code>python</code>的<code>scipy</code>包看到一些压缩方式：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> sparse</span><br><span class="line"><span class="built_in">help</span>(sparse)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Sparse Matrix Storage Formats</span></span><br><span class="line"><span class="string">There are seven available sparse matrix types:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        1. csc_matrix: Compressed Sparse Column format</span></span><br><span class="line"><span class="string">        2. csr_matrix: Compressed Sparse Row format</span></span><br><span class="line"><span class="string">        3. bsr_matrix: Block Sparse Row format</span></span><br><span class="line"><span class="string">        4. lil_matrix: List of Lists format</span></span><br><span class="line"><span class="string">        5. dok_matrix: Dictionary of Keys format</span></span><br><span class="line"><span class="string">        6. coo_matrix: COOrdinate format (aka IJV, triplet format)</span></span><br><span class="line"><span class="string">        7. dia_matrix: DIAgonal format</span></span><br><span class="line"><span class="string">        8. spmatrix: Sparse matrix base clas</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p><p>其中一般较为常用的是<code>csc_matrix</code>，<code>csr_matrix</code>和<code>coo_matrix</code>。</p><h3 id="2-3-一些属性和通用方法"><a href="#2-3-一些属性和通用方法" class="headerlink" title="2.3 一些属性和通用方法"></a>2.3 一些属性和通用方法</h3><p>我们还是以<code>python</code>的<code>scipy</code>包为例，来介绍一些稀疏矩阵的属性和通用方法。</p><p><strong>稀疏属性</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.sparse <span class="keyword">import</span> csr_matrix</span><br><span class="line"></span><br><span class="line"><span class="comment">### 共有属性</span></span><br><span class="line">mat.shape  <span class="comment"># 矩阵形状</span></span><br><span class="line">mat.dtype  <span class="comment"># 数据类型</span></span><br><span class="line">mat.ndim  <span class="comment"># 矩阵维度</span></span><br><span class="line">mat.nnz   <span class="comment"># 非零个数</span></span><br><span class="line">mat.data  <span class="comment"># 非零值, 一维数组</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### COO 特有的</span></span><br><span class="line">coo.row  <span class="comment"># 矩阵行索引</span></span><br><span class="line">coo.col  <span class="comment"># 矩阵列索引</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### CSR\CSC\BSR 特有的</span></span><br><span class="line">bsr.indices    <span class="comment"># 索引数组</span></span><br><span class="line">bsr.indptr     <span class="comment"># 指针数组</span></span><br><span class="line">bsr.has_sorted_indices  <span class="comment"># 索引是否排序</span></span><br><span class="line">bsr.blocksize  <span class="comment"># BSR矩阵块大小</span></span><br></pre></td></tr></table></figure></p><p><strong>通用方法</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.sparse <span class="keyword">as</span> sp</span><br><span class="line"></span><br><span class="line"><span class="comment">### 转换矩阵格式</span></span><br><span class="line">tobsr()、tocsr()、to_csc()、to_dia()、to_dok()、to_lil()</span><br><span class="line">mat.toarray()  <span class="comment"># 转为array</span></span><br><span class="line">mat.todense()  <span class="comment"># 转为dense</span></span><br><span class="line"><span class="comment"># 返回给定格式的稀疏矩阵</span></span><br><span class="line">mat.asformat(<span class="built_in">format</span>)</span><br><span class="line"><span class="comment"># 返回给定元素格式的稀疏矩阵</span></span><br><span class="line">mat.astype(t)  </span><br><span class="line"></span><br><span class="line"><span class="comment">### 检查矩阵格式</span></span><br><span class="line">issparse、isspmatrix_lil、isspmatrix_csc、isspmatrix_csr</span><br><span class="line">sp.issparse(mat)</span><br><span class="line"></span><br><span class="line"><span class="comment">### 获取矩阵数据</span></span><br><span class="line">mat.getcol(j)  <span class="comment"># 返回矩阵列j的一个拷贝，作为一个(mx 1) 稀疏矩阵 (列向量)</span></span><br><span class="line">mat.getrow(i)  <span class="comment"># 返回矩阵行i的一个拷贝，作为一个(1 x n)  稀疏矩阵 (行向量)</span></span><br><span class="line">mat.nonzero()  <span class="comment"># 非0元索引</span></span><br><span class="line">mat.diagonal()   <span class="comment"># 返回矩阵主对角元素</span></span><br><span class="line">mat.<span class="built_in">max</span>([axis])  <span class="comment"># 给定轴的矩阵最大元素</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 矩阵运算</span></span><br><span class="line">mat += mat     <span class="comment"># 加</span></span><br><span class="line">mat = mat * <span class="number">5</span>  <span class="comment"># 乘</span></span><br><span class="line">mat.dot(other)  <span class="comment"># 坐标点积</span></span><br></pre></td></tr></table></figure></p><h2 id="3-常用压缩方法"><a href="#3-常用压缩方法" class="headerlink" title="3 常用压缩方法"></a>3 常用压缩方法</h2><h3 id="3-1-COO"><a href="#3-1-COO" class="headerlink" title="3.1 COO"></a>3.1 COO</h3><p>全称是<code>Coordinate Matrix</code>对角存储矩阵，这里是<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html">官方文档</a>。</p><p><strong>定义详解</strong></p><blockquote><ul><li>采用三元组(row, col, data)(或称为ijv format)的形式来存储矩阵中非零元素的信息;</li><li>三个数组 row 、col 和 data 分别保存非零元素的行下标、列下标与值（一般长度相同;</li><li>故 coo[row[k]][col[k]] = data[k] ，即矩阵的第 row[k] 行、第 col[k] 列的值为 data[k];</li></ul></blockquote><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/sparseMatrix2.png" alt="sparseMatrix2"></p><p><strong>适用场景</strong></p><ul><li>主要用来创建矩阵，因为coo_matrix无法对矩阵的元素进行增删改等操作</li><li>一旦创建之后，除了将之转换成其它格式的矩阵，几乎无法对其做任何操作和矩阵运算</li></ul><p><strong>优点</strong></p><ul><li>转换成其它存储格式很快捷简便（tobsr()、tocsr()、to_csc()、to_dia()、to_dok()、to_lil()）</li><li>能与CSR / CSC格式的快速转换</li><li>允许重复的索引（例如在1行1列处存了值2.0，又在1行1列处存了值3.0，则转换成其它矩阵时就是2.0+3.0=5.0）</li></ul><p><strong>缺点</strong></p><ul><li>不支持切片和算术运算操作</li><li>如果稀疏矩阵仅包含非0元素的对角线，则对角存储格式(DIA)可以减少非0元素定位的信息量</li><li>这种存储格式对有限元素或者有限差分离散化的矩阵尤其有效</li></ul><p><strong>属性</strong></p><ul><li>data：稀疏矩阵存储的值，是一个一维数组</li><li>row：与data同等长度的一维数组，表征data中每个元素的行号</li><li>col：与data同等长度的一维数组，表征data中每个元素的列号</li></ul><p><strong>code case</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据</span></span><br><span class="line">row = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line">col = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成coo格式的矩阵</span></span><br><span class="line"><span class="comment"># &lt;class &#x27;scipy.sparse.coo.coo_matrix&#x27;&gt;</span></span><br><span class="line">coo_mat = sparse.coo_matrix((data, (row, col)), shape=(<span class="number">4</span>, <span class="number">4</span>),  dtype=np.<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># coordinate-value format</span></span><br><span class="line"><span class="built_in">print</span>(coo)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">(0, 0)        1</span></span><br><span class="line"><span class="string">(1, 1)        2</span></span><br><span class="line"><span class="string">(2, 2)        3</span></span><br><span class="line"><span class="string">(3, 3)        4</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看数据</span></span><br><span class="line">coo.data</span><br><span class="line">coo.row</span><br><span class="line">coo.col</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转化array</span></span><br><span class="line"><span class="comment"># &lt;class &#x27;numpy.ndarray&#x27;&gt;</span></span><br><span class="line">coo_mat.toarray()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">array([[1, 0, 0, 0],</span></span><br><span class="line"><span class="string">       [0, 2, 0, 0],</span></span><br><span class="line"><span class="string">       [0, 0, 3, 4],</span></span><br><span class="line"><span class="string">       [0, 0, 0, 0]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p><h3 id="3-2-CSR"><a href="#3-2-CSR" class="headerlink" title="3.2 CSR"></a>3.2 CSR</h3><p>全称是<code>Compressed Sparse Row Matrix</code>压缩稀疏行格式，这里是<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html">官方文档</a>。</p><p><strong>定义详解</strong></p><ul><li>csr_matrix是按行对矩阵进行压缩的</li><li>通过 indices, indptr，data 来确定矩阵。<br>data 表示矩阵中的非零数据</li><li>对于第 i 行而言，该行中非零元素的列索引为 indices[indptr[i]:indptr[i+1]]</li><li>可以将 indptr 理解成利用其自身索引 i 来指向第 i 行元素的列索引</li><li>根据[indptr[i]:indptr[i+1]]，我就得到了该行中的非零元素个数，如<ol><li>若 index[i] = 3 且 index[i+1] = 3 ，则第 i 行的没有非零元素</li><li>若 index[j] = 6 且 index[j+1] = 7 ，则第 j 行的非零元素的列索引为 indices[6:7]</li></ol></li><li>得到了行索引、列索引，相应的数据存放在： data[indptr[i]:indptr[i+1]]</li></ul><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/sparseMatrix3.png" alt="sparseMatrix3"></p><p><strong>构造方法</strong></p><ul><li>对于矩阵第 0 行，我们需要先得到其非零元素列索引<ol><li>由 indptr[0] = 0 和 indptr[1] = 2 可知，第 0 行有两个非零元素。</li><li>它们的列索引为 indices[0:2] = [0, 2] ，且存放的数据为 data[0] = 8 ， data[1] = 2</li><li>因此矩阵第 0 行的非零元素 csr[0][0] = 8 和 csr[0][2] = 2</li></ol></li><li>对于矩阵第 4 行，同样我们需要先计算其非零元素列索引<ol><li>由 indptr[4] = 3 和 indptr[5] = 6 可知，第 4 行有3个非零元素。</li><li>它们的列索引为 indices[3:6] = [2, 3，4] ，且存放的数据为 data[3] = 7 ，data[4] = 1 ，data[5] = 2</li><li>因此矩阵第 4 行的非零元素 csr[4][2] = 7 ， csr[4][3] = 1 和 csr[4][4] = 2</li></ol></li></ul><p><strong>适用场景</strong><br>常用于读入数据后进行稀疏矩阵计算，运算高效。</p><p><strong>优点</strong></p><ul><li>高效的稀疏矩阵算术运算</li><li>高效的行切片</li><li>快速地矩阵矢量积运算</li></ul><p><strong>缺点</strong></p><ul><li>较慢地列切片操作（可以考虑CSC）</li><li>转换到稀疏结构代价较高（可以考虑LIL，DOK）</li></ul><p><strong>属性</strong></p><ul><li>data ：稀疏矩阵存储的值，一维数组</li><li>indices ：存储矩阵有有非零值的列索引</li><li>indptr ：类似指向列索引的指针数组</li><li>has_sorted_indices：索引 indices 是否排序</li></ul><p><strong>code case</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成数据</span></span><br><span class="line">indptr = np.array([<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">7</span>])</span><br><span class="line">indices = np.array([<span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">3</span>])</span><br><span class="line">data = np.array([<span class="number">8</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">9</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建矩阵</span></span><br><span class="line">csr = sparse.csr_matrix((data, indices, indptr))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转为array</span></span><br><span class="line">csr.toarray()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">array([[8, 0, 2, 0, 0],</span></span><br><span class="line"><span class="string">       [0, 0, 5, 0, 0],</span></span><br><span class="line"><span class="string">       [0, 0, 0, 0, 0],</span></span><br><span class="line"><span class="string">       [0, 0, 0, 0, 0],</span></span><br><span class="line"><span class="string">       [0, 0, 7, 1, 2],</span></span><br><span class="line"><span class="string">       [0, 0, 0, 0, 0],</span></span><br><span class="line"><span class="string">       [0, 0, 0, 9, 0]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p><h3 id="3-3-CSC"><a href="#3-3-CSC" class="headerlink" title="3.3 CSC"></a>3.3 CSC</h3><p>全称是<code>Compressed Sparse Column Matrix</code>压缩稀疏列矩阵，这里是<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html">官方文档</a>。</p><p><strong>定义详解</strong></p><ul><li>csc_matrix是按列对矩阵进行压缩的</li><li>通过 indices, indptr，data 来确定矩阵，可以对比CSR</li><li>data 表示矩阵中的非零数据</li><li>对于第 i 列而言，该行中非零元素的行索引为indices[indptr[i]:indptr[i+1]]</li><li>可以将 indptr 理解成利用其自身索引 i 来指向第 i 列元素的列索引</li><li>根据[indptr[i]:indptr[i+1]]，我就得到了该行中的非零元素个数，如<ol><li>若 index[i] = 1 且 index[i+1] = 1 ，则第 i 列的没有非零元素</li><li>若 index[j] = 4 且 index[j+1] = 6 ，则第 j列的非零元素的行索引为 indices[4:6]</li></ol></li><li>得到了列索引、行索引，相应的数据存放在： data[indptr[i]:indptr[i+1]]</li></ul><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/sparseMatrix4.png" alt="sparseMatrix4"></p><p><strong>构造方法</strong></p><ul><li>对于矩阵第 0 列，我们需要先得到其非零元素行索引<ol><li>由 indptr[0] = 0 和 indptr[1] = 1 可知，第 0列行有1个非零元素。</li><li>它们的行索引为 indices[0:1] = [0] ，且存放的数据为 data[0] = 8</li><li>因此矩阵第 0 行的非零元素 csc[0][0] = 8</li></ol></li><li>对于矩阵第 3 列，同样我们需要先计算其非零元素行索引<ol><li>由 indptr[3] = 4 和 indptr[4] = 6 可知，第 4 行有2个非零元素。</li><li>它们的行索引为 indices[4:6] = [4, 6] ，且存放的数据为 data[4] = 1 ，data[5] = 9</li><li>因此矩阵第 i 行的非零元素 csr[4][3] = 1 ， csr[6][3] = 9</li></ol></li></ul><p>应用场景和优缺点基本上与<code>CSR</code>互相对应。</p><p><strong>特殊属性</strong></p><ul><li>data ：稀疏矩阵存储的值，一维数组</li><li>indices ：存储矩阵有有非零值的行索引</li><li>indptr ：类似指向列索引的指针数组</li><li>has_sorted_indices ：索引是否排序</li></ul><p><strong>code case</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成数据</span></span><br><span class="line">row = np.array([<span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">col = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">data = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建矩阵</span></span><br><span class="line">csc = sparse.csc_matrix((data, (row, col)), shape=(<span class="number">3</span>, <span class="number">3</span>)).toarray()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转为array</span></span><br><span class="line">csc.toarray()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">array([[1, 0, 4],</span></span><br><span class="line"><span class="string">       [0, 0, 5],</span></span><br><span class="line"><span class="string">       [2, 3, 6]], dtype=int64)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 按col列来压缩</span></span><br><span class="line"><span class="comment"># 对于第i列，非0数据行是indices[indptr[i]:indptr[i+1]] 数据是data[indptr[i]:indptr[i+1]]</span></span><br><span class="line"><span class="comment"># 在本例中</span></span><br><span class="line"><span class="comment"># 第0列，有非0的数据行是indices[indptr[0]:indptr[1]] = indices[0:2] = [0,2]</span></span><br><span class="line"><span class="comment"># 数据是data[indptr[0]:indptr[1]] = data[0:2] = [1,2],所以在第0列第0行是1，第2行是2</span></span><br><span class="line"><span class="comment"># 第1行，有非0的数据行是indices[indptr[1]:indptr[2]] = indices[2:3] = [2]</span></span><br><span class="line"><span class="comment"># 数据是data[indptr[1]:indptr[2] = data[2:3] = [3],所以在第1列第2行是3</span></span><br><span class="line"><span class="comment"># 第2行，有非0的数据行是indices[indptr[2]:indptr[3]] = indices[3:6] = [0,1,2]</span></span><br><span class="line"><span class="comment"># 数据是data[indptr[2]:indptr[3]] = data[3:6] = [4,5,6],所以在第2列第0行是4，第1行是5,第2行是6</span></span><br></pre></td></tr></table></figure></p><h3 id="3-4-BSR"><a href="#3-4-BSR" class="headerlink" title="3.4 BSR"></a>3.4 BSR</h3><p>全称是<code>Block Sparse Row Matrix</code>分块压缩稀疏行格式，这里是<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.bsr_matrix.html">官方文档</a>。</p><p><strong>定义详解</strong></p><ul><li>基于行的块压缩，与csr类似，都是通过data，indices，indptr来确定矩阵</li><li>与csr相比，只是data中的元数据由0维的数变为了一个矩阵（块），其余完全相同</li><li>块大小 blocksize<ol><li>块大小 (R, C) 必须均匀划分矩阵 (M, N) 的形状。</li><li>R和C必须满足关系：M % R = 0 和 N % C = 0</li><li>适用场景及优点参考csr</li></ol></li></ul><p><strong>特殊属性</strong></p><ul><li>data ：稀疏矩阵存储的值，一维数组</li><li>indices ：存储矩阵有有非零值的列索引</li><li>indptr ：类似指向列索引的指针数组</li><li>blocksize ：矩阵的块大小</li><li>has_sorted_indices：索引 indices 是否排序</li></ul><p><strong>code case</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成数据</span></span><br><span class="line">indptr = np.array([<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">6</span>])</span><br><span class="line">indices = np.array([<span class="number">0</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">data = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]).repeat(<span class="number">4</span>).reshape(<span class="number">6</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建矩阵</span></span><br><span class="line">bsr = bsr_matrix((data, indices, indptr), shape=(<span class="number">6</span>,<span class="number">6</span>)).todense()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转为array</span></span><br><span class="line">bsr.todense()</span><br><span class="line">matrix([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>]])</span><br></pre></td></tr></table></figure></p><h3 id="3-5-LIL"><a href="#3-5-LIL" class="headerlink" title="3.5 LIL"></a>3.5 LIL</h3><p>全称是<code>Linked List Matrix</code>链表矩阵格式，这里是<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html">官方文档</a>。</p><p><strong>定义详解</strong></p><ul><li>使用两个列表存储非0元素data</li><li>rows保存非零元素所在的列</li><li>可以使用列表赋值来添加元素，如 lil[(0, 0)] = 8</li></ul><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/sparseMatrix5.png" alt="sparseMatrix5"></p><p><strong>构造方法</strong></p><ul><li>lil[(0, -1)] = 4 ：第0行的最后一列元素为4</li><li>lil[(4, 2)] = 5 ：第4行第2列的元素为5</li></ul><p><strong>适用场景</strong></p><ul><li>适用的场景是逐渐添加矩阵的元素（且能快速获取行相关的数据）</li><li>需要注意的是，该方法插入一个元素最坏情况下可能导致线性时间的代价，所以要确保对每个元素的索引进行预排序</li></ul><p><strong>优点</strong></p><ul><li>适合递增的构建成矩阵</li><li>转换成其它存储方式很高效</li><li>支持灵活的切片</li></ul><p><strong>缺点</strong></p><ul><li>当矩阵很大时，考虑用coo</li><li>算术操作，列切片，矩阵向量内积操作慢</li></ul><p><strong>属性</strong></p><ul><li>data：存储矩阵中的非零数据</li><li>rows：存储每个非零元素所在的列（行信息为列表中索引所表示）</li></ul><p><strong>code case</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建矩阵</span></span><br><span class="line">lil = sparse.lil_matrix((<span class="number">6</span>, <span class="number">5</span>), dtype=<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置数值</span></span><br><span class="line"><span class="comment"># set individual point</span></span><br><span class="line">lil[(<span class="number">0</span>, -<span class="number">1</span>)] = -<span class="number">1</span></span><br><span class="line"><span class="comment"># set two points</span></span><br><span class="line">lil[<span class="number">3</span>, (<span class="number">0</span>, <span class="number">4</span>)] = [-<span class="number">2</span>] * <span class="number">2</span></span><br><span class="line"><span class="comment"># set main diagonal</span></span><br><span class="line">lil.setdiag(<span class="number">8</span>, k=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set entire column</span></span><br><span class="line">lil[:, <span class="number">2</span>] = np.arange(lil.shape[<span class="number">0</span>]).reshape(-<span class="number">1</span>, <span class="number">1</span>) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 转为array</span></span><br><span class="line">lil.toarray()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">array([[ 8,  0,  1,  0, -1],</span></span><br><span class="line"><span class="string">       [ 0,  8,  2,  0,  0],</span></span><br><span class="line"><span class="string">       [ 0,  0,  3,  0,  0],</span></span><br><span class="line"><span class="string">       [-2,  0,  4,  8, -2],</span></span><br><span class="line"><span class="string">       [ 0,  0,  5,  0,  8],</span></span><br><span class="line"><span class="string">       [ 0,  0,  6,  0,  0]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看数据</span></span><br><span class="line">lil.data</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">array([list([0, 2, 4]), list([1, 2]), list([2]), list([0, 2, 3, 4]),</span></span><br><span class="line"><span class="string">       list([2, 4]), list([2])], dtype=object)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">lil.rows</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">array([[list([8, 1, -1])],</span></span><br><span class="line"><span class="string">       [list([8, 2])],</span></span><br><span class="line"><span class="string">       [list([3])],</span></span><br><span class="line"><span class="string">       [list([-2, 4, 8, -2])],</span></span><br><span class="line"><span class="string">       [list([5, 8])],</span></span><br><span class="line"><span class="string">       [list([6])]], dtype=object)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></p><h3 id="3-6-DIA"><a href="#3-6-DIA" class="headerlink" title="3.6 DIA"></a>3.6 DIA</h3><p>全称是<code>Diagonal Matrix</code>对角存储格式格式，这里是<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.dia_matrix.html">官方文档</a>。</p><p><strong>定义详解</strong></p><ul><li>最适合对角矩阵的存储方式</li><li>dia_matrix通过两个数组确定： data 和 offsets</li><li>data ：对角线元素的值</li><li>offsets ：第 i 个 offsets 是当前第 i 个对角线和主对角线的距离</li><li>data[k:] 存储了 offsets[k] 对应的对角线的全部元素</li></ul><p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/sparseMatrix6.png" alt="sparseMatrix6"></p><p><strong>构造方法</strong></p><ul><li>当 offsets[0] = 0 时，表示该对角线即是主对角线，相应的值为 [1, 2, 3, 4, 5]</li><li>当 offsets[2] = 2 时，表示该对角线为主对角线向上偏移2个单位，相应的值为 [11, 12, 13, 14, 15]</li><li>但该对角线上元素仅有三个 ，于是采用先出现的元素无效的原则</li><li>即前两个元素对构造矩阵无效，故该对角线上的元素为 [13, 14, 15]</li></ul><p><strong>属性</strong></p><ul><li>data：存储DIA对角值的数组</li><li>offsets：存储DIA对角偏移量的数组</li></ul><p><strong>code case</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成数据</span></span><br><span class="line">data = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line">offsets = np.array([<span class="number">0</span>, -<span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建矩阵</span></span><br><span class="line">dia = sparse.dia_matrix((data, offsets), shape=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看数据</span></span><br><span class="line">dia.data</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">array([[[1 2 3 4]</span></span><br><span class="line"><span class="string">        [5 6 0 0]</span></span><br><span class="line"><span class="string">        [0 7 8 9]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 转为array</span></span><br><span class="line">dia.toarray()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">array([[1 7 0 0]</span></span><br><span class="line"><span class="string">       [0 2 8 0]</span></span><br><span class="line"><span class="string">       [5 0 3 9]</span></span><br><span class="line"><span class="string">       [0 6 0 4]])</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br><strong>参考文献</strong><br><a href="https://cloud.tencent.com/developer/article/1544016">经典算法之稀疏矩阵    </a><br><a href="https://zhuanlan.zhihu.com/p/188700729">Sparse稀疏矩阵主要存储格式总结</a><br><a href="https://www.jianshu.com/p/dca6ed5f213f">20190624_稀疏矩阵存储及计算介绍</a><br><a href="https://www.jianshu.com/p/b335ad456990">sparse matrix 的分布式存储和计算</a></p><hr>]]></content>
    
    
    <summary type="html">&lt;p&gt;-&lt;/p&gt;
&lt;h2 id=&quot;1-背景&quot;&gt;&lt;a href=&quot;#1-背景&quot; class=&quot;headerlink&quot; title=&quot;1 背景&quot;&gt;&lt;/a&gt;1 背景&lt;/h2&gt;&lt;p&gt;在企业的深度学习项目中，&lt;code&gt;Sparse稀疏矩阵&lt;/code&gt;这个词想必大家都不陌生。在模型的矩阵计算中，往往会遇到矩阵较为庞大且非零元素较少。由其是现在深度学习中embedding大行其道，稀疏矩阵成为必不可少的基建。而这种情况下，如果依然使用dense的矩阵进行存储和计算将是极其低效且耗费资源的。Sparse稀疏矩阵就称为了救命稻草。在拜读多篇优秀博客后，这里做一些自己的汇总和填补。&lt;/p&gt;
&lt;h2 id=&quot;2-稀疏矩阵&quot;&gt;&lt;a href=&quot;#2-稀疏矩阵&quot; class=&quot;headerlink&quot; title=&quot;2 稀疏矩阵&quot;&gt;&lt;/a&gt;2 稀疏矩阵&lt;/h2&gt;&lt;h3 id=&quot;2-1-定义&quot;&gt;&lt;a href=&quot;#2-1-定义&quot; class=&quot;headerlink&quot; title=&quot;2.1 定义&quot;&gt;&lt;/a&gt;2.1 定义&lt;/h3&gt;</summary>
    
    
    
    <category term="学习笔记" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="算法总结" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="算法" scheme="https://www.xiemingzhao.com/tags/%E7%AE%97%E6%B3%95/"/>
    
    <category term="稀疏矩阵" scheme="https://www.xiemingzhao.com/tags/%E7%A8%80%E7%96%8F%E7%9F%A9%E9%98%B5/"/>
    
  </entry>
  
  <entry>
    <title>Deep and Cross Network for Ad Click Predictions (论文解析)</title>
    <link href="https://www.xiemingzhao.com/posts/dcnpaper.html"/>
    <id>https://www.xiemingzhao.com/posts/dcnpaper.html</id>
    <published>2019-07-28T16:00:00.000Z</published>
    <updated>2025-03-30T17:50:15.698Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/1708.05123.pdf">原始论文：Deep &amp; Cross Network for Ad Click Predictions</a></p><h2 id="深度和交叉网络的广告点击预测"><a href="#深度和交叉网络的广告点击预测" class="headerlink" title="深度和交叉网络的广告点击预测"></a>深度和交叉网络的广告点击预测</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>特征工程已经成为许多预测模型成功的关键。然而，这个过程是不平常的并且经常会要手动特征工程或者穷举搜索。DNNs能够自动地学习特征交叉项；然而，它们都是隐式地生成所有交叉项，并且学习所有类型的交叉特征不一定有效。在本文中，我们提出深度和交叉网络(DCN)，它保持了深度模型的优势，并且又超越了这，它是一种在学习某种边界程度特征交叉项中更为有效的新奇网络。此外，DCN显示地在每一层应用特征交叉，不要求做人工程特征工程，同时也只是给DNN模型增加了一些可以忽略不计的复杂度。我们的实验结果已经证明它在CTR预测数据集和密集的分类数据集上，相对于其他高级模型在模型准确性和记忆方法上都具有优越性。</p><h3 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h3><p>点击率（CTR）预测是一个大规模的问题，它对数十亿美元的在线广告业来说至关重要。在广告业中，广告商会想发布商付费以在发布商的网站上展示他们的广告。一个普遍的付费模式是平均点击成本（CPC）模型，即广告商仅在点击发生的时候才会付费。因此，出版商的收入很大程度上依赖于能够准确预测CTR。</p><span id="more"></span><p>识别出常用的预测特征且同时探索出那些看不见的或者稀少的交叉特征是做出好预测的关键。然而，网站级别的推荐系统的数据主要都是离散的和类别型的，这就导致了一个大的和稀疏的特征空间，而这对于特征探索来说是一个挑战。这就限制了大多数的大规模系统都是线性模型例如逻辑回归。</p><p>线性模型是简单的，可解释的并且容易扩展的；然而，它们受限于自己的表达能力。另一方面，交叉特征已经被证明能够有效地提高模型的表达力。不幸的是，它一般要求人工特征工程或者穷举来找到这些特征；再者，泛化出这些看不见的特征交叉项是很困难的。</p><p>在本文中，我们致力于通过引入一个新的神经网络结构来避免特征工程任务——一个<em>交叉网络</em>——它是以自动的方式显示地应用在特征交叉中。交叉网络由多层网络组成，其中特征的最高交叉维度完全由网络层的深度决定。每一层网络都以及已经存在的特征生成一个更高度的交叉项，同时又保留了前一层网络的交叉项。我们将交叉忘了和一个深度神经网络（DNN）进行联合训练。DNN能够捕获特征中的非常复杂的交叉项；然而，相比于我们的交叉忘了它需要同一数量级的参数，也无法形成显示的交叉特征，并且可能无法有效地学习某些特征交叉项。然而，交叉和深度部分的联合训练能够有效地捕获预测性的特征交叉项，并且在Criteo CTR数据集上提供了一个最先进的效果表现。</p><h4 id="1-1-相关工作"><a href="#1-1-相关工作" class="headerlink" title="1.1 相关工作"></a>1.1 相关工作</h4><p>由于数据集的规模和维度急剧性的增加，于是提出了很多的方法用来避免特定任务中的大规模特征工程，大部分都是基于嵌入技术和神经网络的。</p><p>因式分解机（FMs）将稀疏特征映射到低维的稠密向量上，并且从这些特征的内积中学习特征交叉项。场感知因式分解机（FFMs）让每个特征都可以学习到多个向量，其中每个响亮都是与一个场相关的。遗憾的是，FM和FFM浅显的结构限制了它们的模型表达力。有许多的工作都是为了将FM扩展到一个更高的级别，但是一个缺点就是产生了大量的参数从而大大增加了原本他们不期望产生的计算成本。深度神经网络（DNN）就可以学习到一些重要的高维的特征交叉项，这得益于它们的嵌入向量和非线性的激活函数。最近残差网络的成功使得训练一个非常深的网络有了可能。深度交叉扩展了残差网络，同时通过对所有输入类型的堆叠达到了自动特征学习的效果。</p><p>深度学习的非凡成功引出了它的表达力的理论分析。有研究表明，在给定足够多的的隐含单元或者隐含层的时候，DNN能够再某种平滑线的假设条件下取近似一个有任意准确性的函数。再者，实际上已经发现了DNN在有合适参数的时候就已经能够表现地很好了。一个关键的原因就是实际使用的大多数函数都不是任意选择的。</p><p>一个依然存在的问题就是DNN是否真的在那些实际中使用的表征函数中是最有效的一个。在Kaggle竞赛中，许多胜利者的解决方法中人工精心设计的特征都是低阶的、确切形式且有效地。另一方面，从DNN中学习到的特征都是隐含的且高度非线性的。这就表明了设计一个模型要能够学习到相比于普通的DNN更加有效且确切的有界阶特征交叉项。</p><p>wide-and-deep就是这种想法创建的模型。它将交叉特征作为线性模型的输入，然后将线性模型和DNN模型进行联合训练。然而，wide-and-deep是否成功很大程度上依赖于交叉特征的事前选择，一个指数级的问题就是是否存在还没发现的更有效的方法。</p><h4 id="1-2-主要贡献"><a href="#1-2-主要贡献" class="headerlink" title="1.2 主要贡献"></a>1.2 主要贡献</h4><p>在本文中，我们提出了Deep &amp; Cross Network（DCN）模型，它能够在同时有稀疏输入和密集输入的时候进行网站规模的自动化特征学习。DCN能够有效地抓取有界阶的有用特征交叉项，学习高度非线性的交叉项，并且不要求人工特征工程或者穷举，同时又只有较低的计算成本。</p><p>本文主要的贡献包括：</p><ul><li>我们提出了一个将特征交叉应用在每一层的新交叉网络，它能够有效地学习到具有预测价值的有限阶交叉特征，并且不要求进行人工特征工程或者穷举。</li><li>交叉忘了是简单且有效的。通过设计，每一层多项式的最高阶都在增加并且由层数的深度决定。整个网络是由从低阶到高阶的交叉项以及所有不同的系数组成的。</li><li>交叉网络是能够有效记忆的，并且能够很简单地实现。</li><li>一个带有交叉网络的DNN，在参数个数少一个量级的情况下，它的对数损失依然比普通的DNN要低。</li></ul><h3 id="2-深度-amp-交叉网络（DCN）"><a href="#2-深度-amp-交叉网络（DCN）" class="headerlink" title="2 深度&amp;交叉网络（DCN）"></a>2 深度&amp;交叉网络（DCN）</h3><p>在这一部分，我们将会介绍深度&amp;交叉网络（DCN）模型的结构。DCN是开始于embedding和stacking层的，紧接着是一个交叉网络和一个深度网络并行。按顺序接着是一个最终的联合层用来合并两个网络的输出。完整的DCN模型如图1中所示。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/deep%26cross-1.JPG" alt="D&amp;C-1.jpg"></p><h4 id="2-1-嵌入和堆叠层"><a href="#2-1-嵌入和堆叠层" class="headerlink" title="2.1 嵌入和堆叠层"></a>2.1 嵌入和堆叠层</h4><p>我们考虑包含稀疏和密集特征的输入数据。在网站级规模的推荐系统如CTR预估中，输入数据大部分都是类别型特征，例如“country=usa”。这样的特征经常会被进行one-hot编码，例如“[0,1,0]”；然而，这就经常导致产生过高维的特征空间来适用大型词典。</p><p>为了降低维度，我们使用了一个embedding过程来将这些二值特征转换成密集的实值向量（通常称为嵌入向量）：</p><script type="math/tex; mode=display">x_{embed,i}=W_{embed,i}x_i</script><p>其中$x<em>{embed,i}$是嵌入向量，$x_i$是第i个类别的二值输入，$W</em>{embed,i} \ \in \mathbb R^{n_e \times n_v}$是对应的嵌入矩阵，它可以和网络中其他的参数一起进行优化，$n_e,n_v$分别是嵌入层大小和词典的大小。</p><p>最后，我们将嵌入向量和标准化后的密集特征进行堆叠，形成一个最终的向量：</p><script type="math/tex; mode=display">x_0 = [x_{embed,1}^T,...,x_{embed,k}^T,x_{dense}^T]</script><p>然后再将这个向量喂入到网络中去。</p><h4 id="2-2-交叉网络"><a href="#2-2-交叉网络" class="headerlink" title="2.2 交叉网络"></a>2.2 交叉网络</h4><p>我们创新交叉网络的关键思想就是以一个有效地方式来显示地应用特征交叉。交叉网络由交叉层组成，每一层都有如下的公式：</p><script type="math/tex; mode=display">x_{l+1} = x_0 x_l^T w_l + b_l x_l = f(x_l , w_l, b_L) + x_l</script><p>其中$x<em>l,x</em>{l+1} \ \in \ \mathbb R^d$都是列向量，分别表示第l层和第l+1层交叉网络的输出$w<em>l, b_l \ \in \ \mathbb R^d$是第l层网络的权重和偏置项参数。每一交叉层在特征交叉f之后都反加上它的输入部分，并且映射函数$f:\mathbb R^d \rightarrow \mathbb R^d$拟合$x</em>{l+1}-x_l$的残差。一个交叉层的可视化展示如图2所示。</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/deep%26cross-2.JPG" alt="D&amp;C-2.jpg"></p><p><strong>高阶特征交叉项</strong>。交叉网络特殊的结构造就了交叉特征的阶数随着网络层数增加而增加。第l层交叉网络的多项式最高阶数（相对于输入层来说）是l+1。事实上，交叉网络包含了所有的交叉项$x_1^{\alpha_1} x_2^{\alpha_2}…x_d^{\alpha_d}$，其中d取值从1到l+1。详细的分析在章节3。</p><p><strong>复杂度分析</strong>$L_c$表示交叉层的个数，d表示输入层的维度。然后，交叉网络中的参数个数就是：</p><script type="math/tex; mode=display">d \times L_c \times 2</script><p>交叉网络的时间和空间复杂度是关于输入层维度的线性增长。因此，交叉网络相比与其深度部分仅引入了一个微乎其微的复杂度部分，这使得DCN的整体复杂度与传统的DNN基本一致。这个有效性是得益于$x_0x_l^T$的秩为1的属性，这使得我们可以无需计算和存储整个矩阵的时候生成所有的交叉项。</p><p>交叉网络很少的参数限制了模型的能力。为了获得更高阶的非线性交叉项，我们并行引入了深度网络。</p><h4 id="2-3-深度网络"><a href="#2-3-深度网络" class="headerlink" title="2.3 深度网络"></a>2.3 深度网络</h4><p>深度网络部分是一个全连接的前向神经网络，其每一层的公式可以表示成如下：</p><script type="math/tex; mode=display">h_{l+1} = f(W_lh_l+b_l)</script><p>其中$h<em>l \in \mathbb R^{n_l},h</em>{l+1}\in \mathbb R^{n<em>{l+1}}$分别是第l和第l+1隐含层；$W_l \in \mathbb R^{n</em>{l+1} \times n<em>l}, b_l \in \mathbb R^{n</em>{l+1}}$是第l深度层的参数；$f(\cdot)$是ReLU激活函数。</p><p><strong>复杂度分析</strong>。为了简化，我们假设所有的深度网络层都是等维度的。$L_d$表示深度网络的层数，m表示深度网络层的大小。那么深度网络的参数个数就是：</p><script type="math/tex; mode=display">d \times m + m + (m^2 + m) \times (L_d -1)</script><h4 id="2-4-联合层"><a href="#2-4-联合层" class="headerlink" title="2.4 联合层"></a>2.4 联合层</h4><p>联合层是合并了了两个网络的输出部分，然后将合并后的向量喂入到标准的逻辑层中。</p><p>下面就是二分类问题的公式：</p><script type="math/tex; mode=display">p = \sigma([x_{l_1}^L,h_{L_2}^T w_{logits})</script><p>其中$x<em>{L_1} \in \mathbb R^d, h</em>{L<em>2} \in \mathbb R^m$分别是交叉网络和深度网络的输出，$w</em>{logits} \in \mathbb R^{(d+m)}$是合并层的参数向量，并且$\sigma(x) = 1/(1+exp(-x))$。</p><p>损失函数是带有正则项的对数损失函数，</p><script type="math/tex; mode=display">loss = -\frac{1}{N} \sum_{i=1}^N y_i log(p_i) + (1-y_i) log(1-p_i) + \lambda \sum_l ||w_l||^2</script><p>其中$p_i$是根据前一个公式计算的概率值，$y_i$是真实的标签，N是输入层的总数，$\lambda$是L2正则项参数。</p><p>我们将两个网络联合一起进行训练，这使得每一个单独的网络在训练过程中可以感知到其他部分。</p><h3 id="3-交叉网络分析"><a href="#3-交叉网络分析" class="headerlink" title="3 交叉网络分析"></a>3 交叉网络分析</h3><p>在这一部分，我们分析DCN的交叉网络为了更好地理解它的有效性。我们我们提拱了三个角度：多项式近似，泛化成FM，和高效映射。为了简化，我们假设$b_i = 0$。</p><p><em>注意</em>。将$w<em>j$中的第i个元素表示成$w_j^{(i)}$。对于多索引$\alpha = [\alpha_1,…,\alpha_d] \in \mathbb N^d$和$x = [x_1,…,x_d] \in \mathbb R^d$，我们定义$|\alpha| = \sum</em>{i=1}^d \alpha_i$。</p><p><em>术语</em>。交叉项（单个的）的等级$x_1^{\alpha_1}x_2^{\alpha_2}…x_d^{\alpha_d}$定义为$|\alpha|$。多项式的阶数由交叉项的最高阶来确定。</p><h4 id="3-1-多项式近似"><a href="#3-1-多项式近似" class="headerlink" title="3.1 多项式近似"></a>3.1 多项式近似</h4><p>根据魏尔斯特拉斯逼近定理，闭区间上的连续函数可以用多项式函数一致逼近。因此，我们将从多项式逼近的角度来分析交叉网络。特别地，交叉网络近似的同次多项式类，以一种有效地、更具表达力的并且泛化的方式拟合现实数据集。</p><p>我们仔细地研究了关于交叉网络的同次多项式类的近似。我们定义$P_n(x)$表示n次多项式：</p><script type="math/tex; mode=display">P_n(x) = \{\sum_{\alpha} w_{\alpha} x_1^{\alpha_1} x_2^{\alpha_2}...x_d^{\alpha_d}|0 \leq |\alpha| \leq n, \alpha \in \mathbb N^d\}</script><p>这个类中的每个多项式都有$O(d^n)$个系数。我们证明了，仅仅需要$O(d)$个参数，交叉网络就可以包含同次多项式汇总的所有交叉项，并且每一项的系数都互不相同。</p><p><em>定理3.1</em> 考虑一个l层交叉网络，其第i+1层定义为$x_{i+1} = x_0x_i^Tw_i + x_i$。网络的输入设为$x_0 = [x_1,x_2,…,x_d]^T$，输出为$g_l(x_0) = x_l^Tw_l$，其中参数为$w_i,b_i \in \mathbb R^d$。然后，这个多项式$g_l(x_0)$将会衍生出下面的多项式类：</p><script type="math/tex; mode=display">\{\sum_{\alpha} c_{\alpha}(w_0,...,w_L) x_1^{\alpha_1} x_2^{\alpha_2}...x_d^{\alpha_d}|0 \leq |\alpha| \leq l+1, \alpha \in \mathbb N^d \}</script><p>其中$c<em>{\alpha} = M</em>{\alpha} \sum<em>{i \in B</em>{\alpha}} \sum<em>{j \in P</em>{\alpha}} \prod<em>{k = 1}^{|\alpha|} w</em>{i<em>k}^{(j_k)}$，$M</em>{\alpha}$是常数，且与$w<em>i$无关，$i = [i_1,…,i</em>{|\alpha|}] 和 j = [j<em>1,…,i</em>{|\alpha|}]$是对应的索引，$B<em>{\alpha} = {y \in {0,1,…,l}^{|\alpha|}||y_i &lt; y_j \cap y</em>{|\alpha|} = l}$，并且$P_{\alpha}$是索引所有排列组成的集合$(1,…,1 \cdots d,…,d)$。</p><p>定理3.1的证明在附录中。我们给定一个示例，考虑$x<em>1x_2x_3$的系数$c</em>{\alpha}$，其中$\alpha = (1,1,1,0,…,0)$。对于某些常数，当$l = 2, c<em>{\alpha} = \sum</em>{i,j,k \in P<em>{\alpha}} w_0^{(i)} w_1^{(j)} w_2^{(k)}$；当$l = 3, c</em>{\alpha} = \sum<em>{i,j,k \in P</em>{\alpha}} w_0^{(i)} w_1^{(j)} w_3^{(k)} +  w_0^{(i)} w_2^{(j)} w_3^{(k)} +  w_1^{(i)} w_2^{(j)} w_3^{(k)}$。</p><p>3.2 FM的推广<br>交叉网络这种参数分享的思想类似于FM模型，进一步将其扩展到一个深度结构。</p><p>在FM模型中，特征$x<em>i$是伴随着一个参数向量$v_i$，并且交叉项$x_ix_j$的权重是由$(v_i,v_i)$计算得到的。在DCN汇总，$x_i$是和标量集${w_k^{(i)} }_k^l$相关的，并且$x_i x_j$的权重是由集合${w_k^{(i)} }</em>{k=0}^l$ 和 ${w<em>k^{(j)} }</em>{k=0}^l$中的各参数相乘得到的。模型每个特征学习的一些参数是独立于其他特征的，交叉项的权重是对应参数的某种联合。参数贡献不仅能够是的模型更加有效，而且也能使得模型能够产生看不见的特征交叉项并且使其对于噪声更加稳健。例如，使用带有稀疏特征的数据集的时候。如果两个二值类特征$x_i$和$x_j$很少或者从来不会在训练集中出现，即$x_i \neq 0 \wedge x_j \neq 0$，所以学习到的$x_i,x_j$的权重就不会在预测中输出有异议的信息。</p><p>FM模型是一个比较浅显的结构，其职能表征出2次的交叉项。相反，DCN能够构建所有的交叉项$x_1^{\alpha_1}x_2^{\alpha_2}…x_d^{\alpha_d}$其中$|\alpha|$由一些决定于网络层深度的常数来界定，如定理3.1所声明。因此，交叉网络是将参数共享这种思想从单层扩展到了多层和高次交叉项。注意到，不同于高阶的FM模型，一个交叉网络中的参数的个数仅仅随着输入层维度而线性增的长。</p><h4 id="3-3-高效地投影"><a href="#3-3-高效地投影" class="headerlink" title="3.3 高效地投影"></a>3.3 高效地投影</h4><p>每一个交叉网络层都会将$x_0和x_l$映射出它们之间所有的成对交叉项，并且以一种有效的方式产生输入层的维度。</p><p>考虑$\tilde x \in \mathbb R^d$作为一个交叉层的输入。交叉层会隐式地构建出$d^2$个成对交叉项$x_i \tilde x_j$，并且会以一个高效记忆的方式将它们映射到d维空间。然而，直接的方式将会带来三倍的成本。</p><p>我们的交叉层提供了有效地解决方案来将成本降低到关于d维的线性函数。对于$x_p = x_0 \tilde x^T w$。这实际上等于：</p><script type="math/tex; mode=display">x_p^T = [x_1\tilde x_1 ... x_1 \tilde x_d \ ... \ x_d \tilde x_1 ... x_d \tilde x_d] \left[\begin{array}{cccc}w&0&...&0 \\ 0&w&...&0 \\ ...&...&...&... \\ 0&0&...&w \end{array} \right]</script><p>其中行向量包含所有的$d^2$个成对的交叉向量$x_i\tilde x_j$，投影矩阵有一个固定的对角结构，其中$w\in \mathbb R^d$是一个列向量。</p><h3 id="4-实验结果"><a href="#4-实验结果" class="headerlink" title="4 实验结果"></a>4 实验结果</h3><p>在这一部分，我们字啊一些流行的预测数据及上评估DCN模型的表现。</p><h4 id="4-1-Criteo-Display-Ads-数据"><a href="#4-1-Criteo-Display-Ads-数据" class="headerlink" title="4.1 Criteo Display Ads 数据"></a>4.1 Criteo Display Ads 数据</h4><p>Criteo Display广告数据及是为了预测广告点击率的。它包含13个整数型特征和26个类别型特征，其中每个类别都有高基数集。对于这个数据集，<strong>在对数损失上有0.001的提升就可以被认为是实践显著的。</strong>当考虑一个大的用户基础的时候，预测准确率的一个小提升就潜在地带来公司收益的大增长。数据包含11GB的横跨7天的用户日志（大约4100万条记录）。我们使用前6天的数据进行预测，并且将第7天的数据随机地等量地分成测试集和验证集。</p><h4 id="4-2-实现细节"><a href="#4-2-实现细节" class="headerlink" title="4.2 实现细节"></a>4.2 实现细节</h4><p>DCN是在TensorFlow上实现的，我们简短地讨论一些DCN训练中的一些实现细节。</p><p><em>数据的处理和嵌入*</em>。实值特征是使用对数变化来进行标准化处理的。对于类别特征，我们将特征嵌入成具有维度$6 \times (category cardinality)^{1/4}$的密集向量。将所有的嵌入结果全部连接到一起形成一个1026维的向量。</p><p><em>优化</em>。我们使用Adam这种小批量随机优化的优化器。批量大小社会为512。批量标准化应用在了深度网络中，并且将梯度裁剪常数（gradient clip norm）设为100.</p><p><em>正则化</em>。我们使用early stopping机制，因为我们使用L2正在和dropout都不起作用。</p><p><em>超参数</em>。我们汇报了对隐含层个数，隐含层大小，初始学习率以及交叉层个数进行grid search的结果。隐含层的个数是从2到5，隐含层的大小是从32到1024.对于DCN，交叉层的个数是从1到6,。初始学习率从0.0001到0.001，每次增加0.0001。所有的实验都使用了early stopping，训练步数设为150000，过拟合发生的时候就会提前停止。</p><h4 id="4-3-模型比较"><a href="#4-3-模型比较" class="headerlink" title="4.3 模型比较"></a>4.3 模型比较</h4><p>我们将DCN和5种模型进行了比较：没有交叉网络的DCN模型（DNN），逻辑回归（LR），因式分解机（FMs），Wide&amp;Deep模型（W&amp;D）和深度交叉模型（DC）。</p><p><em>DNN</em>。嵌入层、输出层以及过程中的超参数都是用与DCN一致的。唯一和DCN不用的就是没有交叉层。</p><p><em>LR</em>。我们使用Siby1——一个大型的机器学习系统来区分逻辑回归。整数型特征会被离散到一个对数尺度。交叉特征将会由一个精致且复杂的特诊供选择工具来进行筛选。所有的单特征是都会被使用。</p><p><em>FM</em>。我们使用了带有特定细节的FM模型。</p><p><em>W&amp;D</em>。不同于DCN，它的宽部分作为输入原始稀疏特征，并且依赖于穷举和知识域来选取有预测价值的交叉特征。我们跳过了这一块的比较因为没有比较好的方法来选择交叉特征。</p><p><em>DC</em>。相比于DCN，DC没有显示地构造交叉特征。它主要依靠堆叠和残差项来隐式地创造交叉特征。我们使用和DCN相同的嵌入层，紧接着是另一个ReLu层来生成输入数据到残差单元系列中。残差单元的个数一半设为1到5之间，输入维度和交叉维度一般是从100到1026。</p><h4 id="4-4-模型表现"><a href="#4-4-模型表现" class="headerlink" title="4.4 模型表现"></a>4.4 模型表现</h4><p>在这一部分，我们首先会列出不同模型在对数损失下的最好的结果，然后我们会将DCN和DNN进行仔细对比，之后，我们再进一步分析引入交叉网络的效果。</p><p><strong>不同模型的表现</strong>。不同模型的对数损失的最好测试结果都列在了表1中。最优的超参数设置是DCN模型有2个深层且大小为1024和6个交叉层，DNN则有大小为1024的5层网络，DC模型有5个输入维度为424交叉维度为537的残差单元，LR模型有42个交叉特征。最终发现最深的交叉结构获得了最好的表现结果，这表明交叉网络中高次的特征交叉项是有用的。如我们所见，DCN要远好于所有其他的模型。特别地，它优于最先进的DNN模型，而且仅仅是相对于DNN用了40%的内存消费。</p><p><em>表1 不同模型的最优对数损失</em></p><div class="table-container"><table><thead><tr><th style="text-align:center">Model</th><th style="text-align:center">DCN</th><th style="text-align:center">DC</th><th style="text-align:center">DNN</th><th style="text-align:center">FM</th><th style="text-align:center">LR</th></tr></thead><tbody><tr><td style="text-align:center">Logloss</td><td style="text-align:center"><strong>0.4419</strong></td><td style="text-align:center">0.4425</td><td style="text-align:center">0.4428</td><td style="text-align:center">0.4464</td><td style="text-align:center">0.4474</td></tr></tbody></table></div><p>对于每个模型的最优超参数设置，我们也汇报了10次不同对数损失测试结果的均值和标准差：<br>$DCN:0.4422 \pm 9 \times 10^{-5}$<br>$DNN:0.4430 \pm 3.7 \times 10^{-4}$<br>$DC:0.4430 \pm 4.3 \times 10^{-4}$。<br>如我们如看到的，DCN一致的大幅优于其他模型。</p><p><strong>DCN和DNN之间的比较</strong>。考虑到交叉网络仅仅额外引入了O(d)个参数，我们就将DCN和它——一个传统的DNN进行比较，并且将实验结果展现出来尽管存在较大的内存预算和损失公差。</p><p>接下来，我们将会汇报一定数量参数的损失数据，它们都是在所有的学习率和模型结构上得到的最好的验证集的损失。嵌入层的参数个数被省略了，因为在我们所有模型的计算中这一部分保持不变。</p><p>表2展示了要获得一个达到预期对数损失阈值的模型所需要的最少的参数个数。从表2中我们可以看出DCN的内存有效性要比单一的DNN模型高出近一个量级，这得益于交叉网络能够有效地学习到有限次的特征交叉项。</p><p><em>表2 要获得一个达到预期对数损失阈值的模型所需要的最少的参数个数</em></p><div class="table-container"><table><thead><tr><th style="text-align:center">Logloss</th><th style="text-align:center">0.4430</th><th style="text-align:center">0.4460</th><th style="text-align:center">0.4470</th><th style="text-align:center">0.4480</th></tr></thead><tbody><tr><td style="text-align:center">DNN</td><td style="text-align:center">3.2E6</td><td style="text-align:center">1.5E5</td><td style="text-align:center">1.5E5</td><td style="text-align:center">7.8E4</td></tr><tr><td style="text-align:center">DCN</td><td style="text-align:center">7.9E5</td><td style="text-align:center">7.3E4</td><td style="text-align:center">3.7E4</td><td style="text-align:center">3.7E4</td></tr></tbody></table></div><p>表2对比了固定内存预算的神经网络的表现。我们可以看到，DCN一致的比DNN要好。在一个小参数体制里，交叉网络参数的个数和深度网络相比相差无几，但是可以看到明显提升这就表明交叉网络在学习有用特征交叉项中更为有效。在大参数体制中，DNN缩小了一些差距了。然而，DCN仍然要比DNN好一大截，这表明它可以有效地学习到一些很有用的甚至一个大DNN都学不到的特征交叉项。</p><p><em>表3 不同的内存预算下获得的最好的对数损失</em></p><div class="table-container"><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">5E4</th><th style="text-align:center">1E5</th><th style="text-align:center">4E5</th><th style="text-align:center">1.1E6</th><th style="text-align:center">2.5E6</th></tr></thead><tbody><tr><td style="text-align:center">DNN</td><td style="text-align:center">0.4480</td><td style="text-align:center">0.4471</td><td style="text-align:center">0.4439</td><td style="text-align:center">0.4433</td><td style="text-align:center">0.4431</td></tr><tr><td style="text-align:center">DCN</td><td style="text-align:center"><strong>0.4465</strong></td><td style="text-align:center"><strong>0.4453</strong></td><td style="text-align:center"><strong>0.4432</strong></td><td style="text-align:center"><strong>0.4426</strong></td><td style="text-align:center"><strong>0.4423</strong></td></tr></tbody></table></div><p>我们从更精确的细节来分析对于给定的DNN模型，引入交叉网络的DCN模型的影响。我们首先对比了拥有同样层数和层大小的DNN和DCN模型的最好表现，然后我们展示了验证集的对数损失是如何随着交叉网络层数的增加而变化的。表4展示了DCN和DNN模型在对数损失上面的区别。在同一实验设定下，从最优的对数损失上看DCN模型一致的优于有相同结构的单一DNN模型。这种对于所有超参数的改进是一致的，降低了参数在初始化和随机优化中的随机性影响。</p><p><em>表4 DCN和DNN在验证集上的对数损失之间的区别</em></p><div class="table-container"><table><thead><tr><th style="text-align:center">Layers/Nodes</th><th style="text-align:center">32</th><th style="text-align:center">64</th><th style="text-align:center">128</th><th style="text-align:center">256</th><th style="text-align:center">512</th><th style="text-align:center">1024</th></tr></thead><tbody><tr><td style="text-align:center">2</td><td style="text-align:center">-0.28</td><td style="text-align:center">-0.10</td><td style="text-align:center">-0.16</td><td style="text-align:center">-0.06</td><td style="text-align:center">-0.05</td><td style="text-align:center">-0.08</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">-0.19</td><td style="text-align:center">-0.10</td><td style="text-align:center">-0.13</td><td style="text-align:center">-0.18</td><td style="text-align:center">-0.07</td><td style="text-align:center">-0.05</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">-0.12</td><td style="text-align:center">-0.10</td><td style="text-align:center">-0.06</td><td style="text-align:center">-0.09</td><td style="text-align:center">-0.09</td><td style="text-align:center">-0.21</td></tr><tr><td style="text-align:center">5</td><td style="text-align:center">-0.21</td><td style="text-align:center">-0.11</td><td style="text-align:center">-0.13</td><td style="text-align:center">-0.00</td><td style="text-align:center">-0.06</td><td style="text-align:center">-0.02</td></tr></tbody></table></div><p>图3展示了在随机选择的设置中我们增加交叉层数的改进效果。对于图3中的深度网络，当增加了一个交叉层的时候有一个明显的提升。随着更多的交叉层引入的时候，对于某些模型设置会使得对数损失继续下降，这表明引入交叉项对于预测是有效的；鉴于对于其他模型设置对数损失开始波动甚至出现微幅增加，这就表明高阶的特征交叉项的银如意是没有太大作用的</p><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/deep%26cross-3.JPG" alt="D&amp;C-3.jpg"></p><h4 id="4-5-非CTR数据集"><a href="#4-5-非CTR数据集" class="headerlink" title="4.5 非CTR数据集"></a>4.5 非CTR数据集</h4><p>我们证明了DCN模型在非CTR预测问题中也表现得很好。我们使用来自UCI提供的森林植被类型（forest covertype）（581012样本和54个特征）和 希格斯粒子（Higgs）（11M样本和28个特征）数据集。数据集随机得被分为训练集（90%）和测试集（10%）。对于超参数进行了梯度搜索。深度网络层数从1到10，大小从50到300.交叉网络层数从4到10。残差单元的个数从1到5，他们的出入维度和交叉维度从50到300。对于DCN，输入向量会被直接喂入交叉网络。</p><p>对于森林植被类型数据，DCN在最少的内存消费下获得了最好的测试集准确率0.9740。DNN和DC都是0.9737。DCN最优的超参数设置是8个交叉层且大小为54，6个深度网络层且大小为292，DNN则是有7层大小为292的深度网络层，DC则是有输入维度为271交叉维度为287的4个残差单元。</p><p>对于希格斯粒子数据集，DCN模型获得的最好对数损失测试结果是0.4494，而DNN是0.4506。DCN最优的超参数设定是4层大小为28的交叉网络和4层大小为209深度网络层，DNN则是10层大小为196的深度网络层。DCN在仅用了DNN一半的内存情况下依然表现得比其要好。</p><h3 id="5-结论和未来方向"><a href="#5-结论和未来方向" class="headerlink" title="5 结论和未来方向"></a>5 结论和未来方向</h3><p>区分有效的特征交叉项已经称为了许多预测模型成功的关键。遗憾的是，过程往往需要进行手工特征和穷举。DNN是比较受欢迎的自动特征学习模型；然而，学到的特征是隐式的并且高度非线性的，同时网络并不一定需要很大而且无法学习到某些特征。本文剔除的Deep &amp; Cross Network模型能够处理大的稀疏和密集特征集，并且可以联合传统的深度表示来显示地学习有限次的交叉特征。交叉特征的阶数在每一个交叉层都会增加一。我们的实验结果已经证明了它在系数数据集和密集数据集上都要优于其他最先进的算法，优势体现在模型的准确率和内存使用上。</p><p>我们会进一步地在其他模型中探索使用交叉层，使得深度交叉网络能够有效地训练，研究交叉网络在多项式近似中的有效性，并且在优化过程中可以更好地理解深度网络的交叉项。</p><hr>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1708.05123.pdf&quot;&gt;原始论文：Deep &amp;amp; Cross Network for Ad Click Predictions&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;深度和交叉网络的广告点击预测&quot;&gt;&lt;a href=&quot;#深度和交叉网络的广告点击预测&quot; class=&quot;headerlink&quot; title=&quot;深度和交叉网络的广告点击预测&quot;&gt;&lt;/a&gt;深度和交叉网络的广告点击预测&lt;/h2&gt;&lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h3&gt;&lt;p&gt;特征工程已经成为许多预测模型成功的关键。然而，这个过程是不平常的并且经常会要手动特征工程或者穷举搜索。DNNs能够自动地学习特征交叉项；然而，它们都是隐式地生成所有交叉项，并且学习所有类型的交叉特征不一定有效。在本文中，我们提出深度和交叉网络(DCN)，它保持了深度模型的优势，并且又超越了这，它是一种在学习某种边界程度特征交叉项中更为有效的新奇网络。此外，DCN显示地在每一层应用特征交叉，不要求做人工程特征工程，同时也只是给DNN模型增加了一些可以忽略不计的复杂度。我们的实验结果已经证明它在CTR预测数据集和密集的分类数据集上，相对于其他高级模型在模型准确性和记忆方法上都具有优越性。&lt;/p&gt;
&lt;h3 id=&quot;1-介绍&quot;&gt;&lt;a href=&quot;#1-介绍&quot; class=&quot;headerlink&quot; title=&quot;1 介绍&quot;&gt;&lt;/a&gt;1 介绍&lt;/h3&gt;&lt;p&gt;点击率（CTR）预测是一个大规模的问题，它对数十亿美元的在线广告业来说至关重要。在广告业中，广告商会想发布商付费以在发布商的网站上展示他们的广告。一个普遍的付费模式是平均点击成本（CPC）模型，即广告商仅在点击发生的时候才会付费。因此，出版商的收入很大程度上依赖于能够准确预测CTR。&lt;/p&gt;</summary>
    
    
    
    <category term="学习笔记" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="论文解析" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E8%A7%A3%E6%9E%90/"/>
    
    
    <category term="机器学习" scheme="https://www.xiemingzhao.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="推荐" scheme="https://www.xiemingzhao.com/tags/%E6%8E%A8%E8%8D%90/"/>
    
    <category term="排序" scheme="https://www.xiemingzhao.com/tags/%E6%8E%92%E5%BA%8F/"/>
    
    <category term="CTR预估" scheme="https://www.xiemingzhao.com/tags/CTR%E9%A2%84%E4%BC%B0/"/>
    
    <category term="神经网络" scheme="https://www.xiemingzhao.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    <category term="Deep &amp; Cross" scheme="https://www.xiemingzhao.com/tags/Deep-Cross/"/>
    
  </entry>
  
  <entry>
    <title>ABTest显著性计算</title>
    <link href="https://www.xiemingzhao.com/posts/ABTestsignificancecomputing.html"/>
    <id>https://www.xiemingzhao.com/posts/ABTestsignificancecomputing.html</id>
    <published>2019-07-15T16:00:00.000Z</published>
    <updated>2025-03-30T17:49:08.101Z</updated>
    
    <content type="html"><![CDATA[<h2 id="显著性计算（uv-based）"><a href="#显著性计算（uv-based）" class="headerlink" title="显著性计算（uv based）"></a>显著性计算（uv based）</h2><p><strong>这里以实验目标为提升CR（Conversion Rate）为例说明</strong></p><h3 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h3><p><strong>显著性：</strong> 显著：新版和老版的CR有明显差异，不显著: 新版和老版没有明显差异。<br><strong>上升幅度：</strong>(新版CR-老版CR)/老版CR<br><strong>功效:</strong> 一般功效（即power值）达到0.8, 我们认为样本量即实验UV充足，可下结论。</p><span id="more"></span><p>假设观察实验进行3天后，power=0.5&lt;0.8，并且结果不显著，这时需要累计更多样本。 如果当 power 已达到 0.8 时，仍未显著，一般我们认为新版和老版的CR的确无明显差异。</p><p><code>AA校验</code>：验证主测频道分流是否随机。</p><p>若两个 Control 版本之间的指标没有显著差异，则表明分流随机；反之，则需排查 Control 版本中是否存在异常数据；</p><blockquote><p>AA异常也可能由于两个 control 版本，其中之一包含一些异常用户（订单数极高），而另外一个版本没有异常用户。</p></blockquote><h3 id="如何下结论"><a href="#如何下结论" class="headerlink" title="如何下结论"></a>如何下结论</h3><blockquote><p><code>power</code> 和样本量功能类似，达到样本量基本等同于 power 达到80%。power 与样本量计算相比，power 可以更多的利用实验本身的信息，而样本量主要使用频道的数据，计算时与实验设置分流等无关，仅实验剩余天数与实验相关。</p></blockquote><p>所以这里我们结合power和显著性对实验的结果进行判断。这里以转化率CR为例。</p><ol><li>如果 power 达到80%时，CR仍不显著， 说明此时实验新版与老版无显著差异，停止实验。</li><li>如果 power 未到达80%，CR不显著，说明此时样本量不充足，需继续实验，累计更多的用户。</li></ol><p><strong>以上均基于AA检验正常为前提。</strong></p><p>如果AA异常，需查询原因，如果是AA中某一版本中有少数用户订单数极高，导致AA异常，剔除这种异常用户后重新计算AA Test的结果， 如果不再显著，AA正常。</p><blockquote><p>严谨一点，再检查AB 的检验中(一般B&lt;新版&gt; vs C+D<C、D都为老版>)是否存在同样问题，即某一版本出现一些异常用户(订单数极高的用户), 如果存在，剔除后重新计算显著性。</p></blockquote><h2 id="算法说明"><a href="#算法说明" class="headerlink" title="算法说明"></a>算法说明</h2><h3 id="显著性计算"><a href="#显著性计算" class="headerlink" title="显著性计算"></a>显著性计算</h3><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/Abtest/Abtest-2.png" alt="ABtestsample"></p><p>我们将指标提升的百分比定义为<code>lift</code>， <strong>$lift=\frac{Treatment}{Control-1}$</strong></p><p>如上图的示例结果图所示，CR 的 lift 估计值为：-0.79%， 区间（-1.55%,-0%）.  CR lift 的真实值以95%的可能性落在区间（-1.55%,-0%）内。由于区间未包括0，所以CR显著, 并且从数值上看是显著下降。说明新版的CR明显低于老版的CR.</p><p>具体计算方案，以国内酒店频道的CR例,假设:</p><ul><li>老版本每个用户的订单数X为：$x<em>1,x_2,…,x</em>{n_1}$，其中$n_1$为老版本的用户数，且有：$E[X] = u_1, Var(X) = \sigma_1^2$</li><li>新版本每个用户的订单数Y为：$y<em>1,y_2,…,y</em>{n_2}$，其中$n_2$为新版本的用户数，且有：$E[Y] = u_2, Var(Y) = \sigma_2^2$</li><li>尽管 X 和 Y 的分布不满足正态的假设，由大数定律得到，老版人均订单数$(CR_1)$和新版的人均订单数$(CR_2)$分别满足 $CR_1 \sim  N(u_1, \frac{\sigma_1^2}{n_1})$ 和 $CR_2 \sim N(u_2, \frac{\sigma_2^2}{n_2})$ 的正态分布。人均订单数即 CR。</li></ul><p>那么 lift 值的计算方案就如下：</p><p><strong>Step1： 估计$u_1, u_2, \sigma_1^2, \sigma_2^2$</strong><br>根据上述四个公式即可得到这四个统计量的估计值。</p><p><strong>Step2：抽样产生 lift 的n（一般取10000）个随机数，$lift^i, i = 1, …, n$</strong><br>由于$CR_1 ~ N(u_1,\frac{\sigma_1^2}{n_1}), CR_2 ~ N(u_2,\frac{\sigma_2^2}{n_2})$，那么结合 Step1 中的参数估计，就可以，</p><ul><li>产生满足$N(\hat u_1, \hat \sigma_1^2 / n_1)$分布的n个随机数，$CR_1^i, i = 1,2,…,n$</li><li>产生满足$N(\hat u_2, \hat \sigma_2^2 / n_2)$分布的n个随机数，$CR_2^i, i = 1,2,…,n$</li></ul><p>然后我们就可以计算：$lift^i = (CR_2^i - CR_1^i) / CR_1^i, i = 1,2,…,n$</p><p><strong>Step3：计算 lift 的均值和区间（置信度90%）</strong><br>lift 均值： $\sum_{i=1}^n lift^i / n$；<br>区间上界： $lift^i$ 的95%分位数；<br>区间下界： $lift^i$ 的5%分位数。</p><h3 id="功效（power值）计算"><a href="#功效（power值）计算" class="headerlink" title="功效（power值）计算"></a><strong>功效（power值）计算</strong></h3><script type="math/tex; mode=display">Power = \Phi (-Z_{1 - \alpha / 2} + \frac {\Delta}{\sqrt {\sigma_1^2 / n_1 + \sigma_2^2 / n_2} })</script><p>其中：</p><ul><li>$\alpha$ 是 Type I Error， 一般为0.05 或者0.1；</li><li>$\sigma_1^2$是老版订单数（或其他指标）的方差，$n_1$是老版的uv数；</li><li>$\sigma_2^2$是新版订单数（或其他指标）的方差，$n_2$是新版的uv数；</li><li>$\Delta = lift * u$中的 lift 是实际实验新版相对老板提升的百分比，一般取值为0.02或者0.04，这里设此目标值是为了固定，使用实际的会出现波动太乱的情况；</li><li>u 是老版的 CR （或者其他检验指标）。</li></ul><p>示例：一下以某一次酒店排序实验为例，其 type I error = 0.05, lift = 0.02, 计算 CR 对应的 power。<br>$n_1$ = 老版用户数 = 22917； $n_2$ = 新版用户数 = 34389<br>$\hat u$ = 老版 CR 估计值 = 0.37474<br>$\hat \sigma_1^2$ = 老版订单数方差估计值 = 0.7188733<br>$\hat \sigma_2^2$ = 新版订单数方差估计值 = 0.721059</p><script type="math/tex; mode=display">\begin{array}{c}Power = \Phi (-Z_{1 - \alpha / 2} + \frac {\Delta}{\sqrt {\sigma_1^2 / n_1 + \sigma_2^2 / n_2} }) \\=  \Phi (-1.959964 + \frac {0.02 \times 0.37474}{\sqrt {0.7188733/22917 + 0.721059/34389} }) \\= \Phi (-0.923327) = 17.79 \%\end{array}</script><h2 id="显著性计算（date-based）"><a href="#显著性计算（date-based）" class="headerlink" title="显著性计算（date based）"></a>显著性计算（date based）</h2><h3 id="方法简述"><a href="#方法简述" class="headerlink" title="方法简述"></a>方法简述</h3><blockquote><p>当总体呈现正态分布且总体标准差未知，而且容量小于30，那么这时一切可能的样本平均数与总体平均数的离差统计量呈T分布。</p></blockquote><p>该方法采用统计中的<code>two sample t test</code>， 检验两组数据的均值是否相等。</p><blockquote><p>例如100个男生身高数据和100个女生身高数据，通过该方法可以检验男生的平均身高是否显著不等于女生的平均身高。</p></blockquote><p>在报表中我们输入的两组数据，一组是一个版本每日的指标数据，另外一组是选择的另外一个版本对应的每日的指标数据。指标可以是任意数值型指标，比如UV数，点击率，订单数等等。</p><p><strong>该方法我们只做两两间的比较。</strong></p><h3 id="实验举例"><a href="#实验举例" class="headerlink" title="实验举例"></a>实验举例</h3><p>我们以一个首页改版为例，酒店预定入口发生变化是实验变量，我们想知道位置的改变是否会影响酒店的点击数量，分流比：新版：老版=50%：50%</p><p>我们需要拿到每天新老版本的点击UV数，通过统计检验，判断是否新版的点击UV数明显低于老版。</p><h3 id="检验方法"><a href="#检验方法" class="headerlink" title="检验方法"></a>检验方法</h3><p>假设x：新版每日UV数，y：老板每日UV数。计算如下统计量：</p><script type="math/tex; mode=display">t = \frac {\bar x - \bar y}{s \sqrt{1 / n_1 + 1 / n_2} }</script><p>这里，</p><ul><li>$\bar x$ 是新版均值，$\bar x = \sum_{i=1}^{n_1} \frac{x_i}{n_1}$, $n_1$ 是天数；</li><li>$\bar y$ 是老板均值，$\bar y = \sum_{i=1}^{n_1} \frac{y_i}{n_2}$, $n_2$ 是天数；</li><li>$s = \sqrt{[(n_1 - 1) s_1^2 + (n_2 - 1) s_2^2] / (n_1 + n_2 -2)}$；</li><li>$s<em>1^2 = \sum</em>{i=1}^{n<em>1} (x_i - \bar x)^2 / (n_1 - 1), s_2^2 = \sum</em>{i=1}^{n_2} (y_i - \bar y) / (n_2 - 1)$</li></ul><p>若 $| t | &gt; t<em>{n_1 + n_2 - 2, 1 - \alpha / 2}$，则显著，否则不显著。$t</em>{n_1 + n_2 - 2, 1 - \alpha / 2}$数值可通过查表或者计算器获取。</p><h3 id="示例剖析"><a href="#示例剖析" class="headerlink" title="示例剖析"></a>示例剖析</h3><p>基于上述方案，我们对前面的例子进行计算有：<br>$\bar x = 7555.111, \bar y = 14935$<br>共有9天数据，所以$n_1 = n_2 = 9$</p><p>$s_1^2 = 1556811, s_2^2 = 335096.8, s = 972.6015$<br>t = (7555.111-14935) / 3890.406/$\sqrt{2/9}$ = -16.09612</p><p>查表或者计算机可得：$t<em>{n_1 + n_2 - 2, 1 - \alpha / 2} = t</em>{16,1 - \alpha / 2} = 1.745884$（自由度=9+9-2=16）。<br>由于$| t | &gt; t_{16,1 - \alpha / 2}$，所以新版还外加酒店宫格点击用户数相对老版是显著下降的。</p><p><strong>最小样本量</strong>：<br>在方法简述中提到：当总体呈现正态分布且总体标准差未知，而且容量小于30，那么这时一切可能的样本平均数与总体平均数的离差统计量呈T分布。</p><blockquote><p>在ABtest中实验天数小于30天即可用T检验来进行判定。那么是不是实验天数越小越好呢？</p></blockquote><p>答案显然是否定的，实验天数越多得到的结论可靠性越好。</p><p>但是业务人员希望实验天数越少越好，两者之间形成了悖论。在此，一般建议实验最少进行两周（14天）：一周数据（7天）太少，且旅游数据大部分都是以一周为一个周期上下浮动，选择两周可以有效地平滑掉周期对结果的影响。</p><h3 id="实验最小-uv-量"><a href="#实验最小-uv-量" class="headerlink" title="实验最小 uv 量"></a>实验最小 uv 量</h3><blockquote><p>假设实验组分流比例 (B) = 对照组分流比例 (C+D), 指标(CR等)满足正态分布(Central Limit Theorem)且方差相等。</p></blockquote><p>选择参与实验的主指标数量为 m (选项有CR, Quantity, GP-C)。对于每个选中的主指标, 计算该指标需要的最小样本量$S_i$:</p><script type="math/tex; mode=display">n = \frac {((k+1) + (k+1)/k) \sigma^2 (z_{a - \alpha / 2m} + z_{1 - \beta})^2}{\Delta^2}= treatment_uv + control_uv</script><ul><li>$\Delta = lift * u_x$，大流量 lift 可取值0.02，小流量可取0.03<br>（$u_x$可取该指标在试验频道前2周的均值；$lift = (u_y - u_x)/u_x$，其中$(u_y - u_x)$是实验组和对照组的均值差）</li><li>Type I Error 一般取$\alpha = 10 \%$；Type II Error 一般取$\beta = 0.2(Power = 10 \%)$</li><li>$z_x$是正态分布累计概率为 x 时对应的分位数</li><li>$\sigma^2$是该指标子啊试验频道前2周的方差。k = 实验组UV/对照组UV</li></ul><p><strong>最后选取实验的所需最小样本量的最大值 $max{S_i: i = 1, …, m}$</strong></p><h4 id="知识小科普："><a href="#知识小科普：" class="headerlink" title="知识小科普："></a>知识小科普：</h4><blockquote><p>T检验和成对T检验的区别：通常T检验或成对T检验是用来判断两组数据的平均值是否在统计上有差别。</p></blockquote><p>换一个理解，对两组数据而言，每组数据本身内部有一个波动范围(组内变异)，而两组数据之间平均值的波动相称为组间变异，如果组间变异相对于组内变异小的话，就可以认为两组数据之间的平均值是没有差异的，这是T检验的做法。<br>而对于成对T检验，在一组中的数据与另一组的数据有对应关系，也就是两组数据是以成队的形式出现的。这个时候运用这两个成队数据之间的差值，可以得到一个数据列，如果这个数据列的平均值在统计上是非零的，即可认为两组数据均值是有差异的。<br>在这个地方，没有单独的去考虑两组数据之内的差异，而是通过将两组数据中对应的数据相减，得到一组数据，通过类似偏倚的算法，来看它在统计是是否非零。<br>换一句话说，是当组内差异比较大(或者说是噪音较大)，但是可以通过其它一个因子作区隔时，可以用成对T检验。</p><hr>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;显著性计算（uv-based）&quot;&gt;&lt;a href=&quot;#显著性计算（uv-based）&quot; class=&quot;headerlink&quot; title=&quot;显著性计算（uv based）&quot;&gt;&lt;/a&gt;显著性计算（uv based）&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;这里以实验目标为提升CR（Conversion Rate）为例说明&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;名词解释&quot;&gt;&lt;a href=&quot;#名词解释&quot; class=&quot;headerlink&quot; title=&quot;名词解释&quot;&gt;&lt;/a&gt;名词解释&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;显著性：&lt;/strong&gt; 显著：新版和老版的CR有明显差异，不显著: 新版和老版没有明显差异。&lt;br&gt;&lt;strong&gt;上升幅度：&lt;/strong&gt;(新版CR-老版CR)/老版CR&lt;br&gt;&lt;strong&gt;功效:&lt;/strong&gt; 一般功效（即power值）达到0.8, 我们认为样本量即实验UV充足，可下结论。&lt;/p&gt;</summary>
    
    
    
    <category term="ABTest" scheme="https://www.xiemingzhao.com/categories/ABTest/"/>
    
    
    <category term="ABTest" scheme="https://www.xiemingzhao.com/tags/ABTest/"/>
    
  </entry>
  
  <entry>
    <title>RF,GBDT,XGBOOST, LightGBM之间的爱恨情仇</title>
    <link href="https://www.xiemingzhao.com/posts/diffofRFGBDTXGBLGB.html"/>
    <id>https://www.xiemingzhao.com/posts/diffofRFGBDTXGBLGB.html</id>
    <published>2019-07-04T16:00:00.000Z</published>
    <updated>2025-03-31T17:09:54.476Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h2><p>RF,GBDT,XGBoost,lightGBM都属于<strong>集成学习（Ensemble Learning）</strong>，集成学习的目的是通过结合多个基学习器的预测结果来改善基本学习器的泛化能力和鲁棒性。</p><p>根据基本学习器的生成方式，目前的集成学习方法大致分为两大类：即基本学习器之间存在强依赖关系、必须串行生成的<strong>序列化方法</strong>；以及基本学习器间不存在强依赖关系、可同时生成的<strong>并行化方法</strong>。前者的代表就是Boosting，后者的代表是Bagging和“随机森林”（Random Forest）。</p><p>本文主要从下面四个广泛讨论和使用的方法进行了对比分析总结：<br><strong>RF（随机森林）,GBDT（梯度提升决策树）,XGBoost,lightGBM</strong></p><span id="more"></span><h2 id="2-RF（随机森林）"><a href="#2-RF（随机森林）" class="headerlink" title="2. RF（随机森林）"></a>2. RF（随机森林）</h2><h3 id="2-1-算法原理"><a href="#2-1-算法原理" class="headerlink" title="2.1 算法原理"></a>2.1 算法原理</h3><p>介绍RF之前需要先了解一下<strong>Bagging</strong>，其可以简单的理解为：<strong>放回抽样，多数表决（分类）或简单平均（回归）,同时 Bagging 的基学习器之间属于并列生成，不存在强依赖关系</strong>。</p><p>回到RF身上，他其实是 Bagging 的一个扩展变体，主要可以概括为如下四步：</p><ol><li>随机选择样本（放回抽样）；</li><li>随机选择特征（相比普通通bagging多了特征采样）；</li><li>构建决策树；</li><li>随机森林投票（平均）。</li></ol><h3 id="2-2-特性"><a href="#2-2-特性" class="headerlink" title="2.2 特性"></a>2.2 特性</h3><p>在构建决策树的时候，RF的每棵决策树都最大可能的进行生长而不进行剪枝；在对预测输出进行结合时，<strong>RF通常对分类问题使用简单投票法，回归任务使用简单平均法</strong>。</p><p>RF的重要特性是不用对其进行交叉验证或者使用一个独立的测试集获得无偏估计，它可以在内部进行评估，也就是说在生成的过程中可以对误差进行无偏估计，由于每个基学习器只使用了训练集中约63.2%的样本，剩下约36.8%的样本可用做验证集来对其泛化性能进行<strong>“包外估计”</strong>。</p><h3 id="2-3-RF和Bagging对比："><a href="#2-3-RF和Bagging对比：" class="headerlink" title="2.3 RF和Bagging对比："></a>2.3 RF和Bagging对比：</h3><p>随机选择样本和Bagging相同，随机选择特征是指在树的构建中，会从样本集的特征集合中<strong>随机选择部分特征</strong>，然后再从这个子集中选择最优的属性用于划分，这种随机性导致随机森林的偏差会有稍微的增加（相比于单棵不随机树），但是由于随机森林的‘平均’特性，会使得它的<strong>方差减小</strong>，而且方差的减小<strong>补偿了偏差的增大</strong>，因此总体而言是更好的模型。</p><p>RF的起始性能较差，特别当只有一个基学习器时，但随着学习器数目增多，随机森林通常会收敛到更低的泛化误差。随机森林的训练效率也会高于Bagging，因为在单个决策树的构建中，Bagging使用的是‘确定性’决策树，在选择特征划分结点时，要对所有的特征进行考虑，而随机森林使用的是‘随机性’特征数，只需考虑特征的子集。</p><h3 id="2-4-优缺点"><a href="#2-4-优缺点" class="headerlink" title="2.4 优缺点"></a>2.4 优缺点</h3><p>优点：</p><ol><li>训练可以高度并行化，对于大数据时代的大样本训练速度有优势，算是主要优势；</li><li>能够处理很高维的数据，并且不用特征选择，而且在训练完后，给出特征的重要性；</li><li>相对于Boosting系列的Adaboost和GBDT， RF实现比较简单。　　</li></ol><p>缺点：在噪声较大的分类或者回归问题上容易过拟合。</p><h2 id="3-GBDT（梯度提升决策树）"><a href="#3-GBDT（梯度提升决策树）" class="headerlink" title="3. GBDT（梯度提升决策树）"></a>3. GBDT（梯度提升决策树）</h2><h3 id="3-1-算法原理"><a href="#3-1-算法原理" class="headerlink" title="3.1 算法原理"></a>3.1 算法原理</h3><p>首先是基于 Boosting 的，提升树是加法模型，学习算法为前向分布算法时的算法。但 GBDT 与传统的 Boosting 区别较大，是对提升树的改进，不过它限定基本学习器为决策树。它的每一次计算都是为了减少上一次的残差，而为了消除残差，我们可以在<strong>残差减小的梯度方向上建立模型</strong>,所以说，在 GradientBoost 中，每个新的模型的建立是为了使得之前的模型的残差往梯度下降的方法，与传统的 Boosting 中关注正确错误的样本加权有着很大的区别。</p><p>具体的算法求解过程可以参考<a href="https://www.jianshu.com/p/405f233ed04b">这篇博客</a>或者<a href="https://www.jstor.org/stable/2699986">原始论文</a>。</p><h3 id="3-2-特性"><a href="#3-2-特性" class="headerlink" title="3.2 特性"></a>3.2 特性</h3><ul><li>对于二分类问题，损失函数为指数函数，就是把AdaBoost算法中的基本学习器限定为二叉决策树就可以了；</li><li>对于回归问题，损失函数为平方误差，此时，拟合的是当前模型的残差。</li></ul><p><strong>提升树算法只适合误差函数为指数函数和平方误差，对于一般的损失函数，梯度提升树算法利用损失函数的负梯度在当前模型的值，作为残差的近似值。</strong></p><p>在 GradientBoosting 算法中，关键就是利用损失函数的负梯度方向在当前模型的值作为残差的近似值，进而拟合一棵CART回归树。<strong>GBDT的会累加所有树的结果，而这种累加是无法通过分类完成的，因此 GBDT 的树都是 CART 回归树，而不是分类树</strong>（尽管GBDT调整后也可以用于分类但不代表GBDT的树为分类树）。</p><h3 id="3-3-优缺点："><a href="#3-3-优缺点：" class="headerlink" title="3.3 优缺点："></a>3.3 优缺点：</h3><p>优点：</p><ol><li>它能灵活的处理各种类型的数据；</li><li>在相对较少的调参时间下，预测的准确度较高，这个是相对SVM来说的。</li></ol><p>缺点：基学习器之前存在串行关系，难以并行训练数据。</p><h2 id="4-XGBoost"><a href="#4-XGBoost" class="headerlink" title="4 XGBoost"></a>4 XGBoost</h2><h3 id="4-1-算法原理"><a href="#4-1-算法原理" class="headerlink" title="4.1 算法原理"></a>4.1 算法原理</h3><p>XGBoost 的性能在 GBDT 上又有一步提升，而其性能也能通过各种比赛管窥一二。坊间对 XGBoost 最大的认知在于其能够自动地运用CPU的多线程进行并行计算，同时在算法精度上也进行了精度的提高。由于 GBDT 在合理的参数设置下，往往要生成一定数量的树才能达到令人满意的准确率，在数据集较复杂时，模型可能需要几千次迭代运算。但是 XGBoost 利用并行的CPU更好的解决了这个问题。</p><p>针对XGBoost细节可以参考本人对<a href="https://www.xiemingzhao.com/posts/15b10533.html">原始论文的译文</a>以及本人的另一篇专门针对<a href="https://www.xiemingzhao.com/posts/XGBoostDetailAnalysis.html">XGBoost原理的详解</a>的博客。</p><h3 id="4-2-特性"><a href="#4-2-特性" class="headerlink" title="4.2 特性"></a>4.2 特性</h3><p>这里的特性也一般是 XGBoost 本身加的很多 tricks，也是算法的优势所在。</p><h4 id="4-2-1-基分类器"><a href="#4-2-1-基分类器" class="headerlink" title="4.2.1 基分类器"></a>4.2.1 基分类器</h4><p>传统 GBDT 以 CAR T树作为基分类器，<strong>xgboost还支持线性分类器</strong>，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。—可以通过booster[default=gbtree]设置参数:gbtree: tree-based models/gblinear: linear models。</p><h4 id="4-2-2-目标函数"><a href="#4-2-2-目标函数" class="headerlink" title="4.2.2 目标函数"></a>4.2.2 目标函数</h4><p>传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了<strong>二阶泰勒展开</strong>，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。</p><p>对损失函数做了改进（泰勒展开），我们知道目标函数定义如下：</p><script type="math/tex; mode=display">\mathcal L^{(t)} = \sum_{t=1}^n l(y_i,\hat y_i^{(t-1)} + f_t(x_i)) + \Omega(f_t) + constant</script><p>其中： $\Omega(f) = \gamma T + \frac{1}{2} \lambda ||w||^2$</p><p>上述目标函数的第二项是正则项，包含了L1和L2正则。且最后一项constant是常数项，优化中可以先忽略，后面我们不再提。然后我们对目标函数进行二阶的泰勒展开来近似，所以有：</p><script type="math/tex; mode=display">\mathcal L^{(t)} \simeq \sum_{t=1}^n [l(y_i,\hat y^{(t-1)}) + g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i)] + \Omega(f_t)</script><p>其中，$g<em>i = \partial</em>{\hat y^{(t-1)}} l(y<em>i,\hat y^{(t-1)})$和$h_i = \partial</em>{\hat y^{(t-1)}}^2 l(y_i,\hat y^{(t-1)})$，</p><p>我们可以很清晰地看到，<strong>最终的目标函数只依赖于每个数据点的在误差函数上的一阶导数和二阶导数</strong>。</p><h4 id="4-2-3-正则项"><a href="#4-2-3-正则项" class="headerlink" title="4.2.3 正则项"></a>4.2.3 正则项</h4><p>xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了<strong>树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和</strong>。从 Bias-variance tradeoff 角度来讲，正则项降低了模型 variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。正则化包括了两个部分，都是为了防止过拟合，剪枝是都有的，叶子结点输出L2平滑是新增的。</p><h4 id="4-2-4-shrinkage"><a href="#4-2-4-shrinkage" class="headerlink" title="4.2.4 shrinkage"></a>4.2.4 shrinkage</h4><p>还是为了防止过拟合，shrinkage缩减类似于学习速率，在每一步 tree boosting 之后增加了一个参数$\eta$（权重），即有$F^{t} =F^{t-1} + \eta f^t$。 通过这种方式来减小每棵树的影响力，给后面的树提供空间去优化模型。权重一般在0-1之间，经验上&lt;=0.1的时候比较好。（补充：传统GBDT的实现也有学习速率）</p><h4 id="4-2-5-column-subsampling"><a href="#4-2-5-column-subsampling" class="headerlink" title="4.2.5 column subsampling"></a>4.2.5 column subsampling</h4><p>列(特征)抽样，说是从随机森林那边学习来的，防止过拟合的效果比传统的行抽样还好（xgb同样也包含行抽样功能），并且有利于后面提到的并行化处理算法。</p><h4 id="4-2-6-split-finding-algorithms-划分点查找算法"><a href="#4-2-6-split-finding-algorithms-划分点查找算法" class="headerlink" title="4.2.6 split finding algorithms(划分点查找算法)"></a>4.2.6 split finding algorithms(划分点查找算法)</h4><ul><li><strong>exact greedy algorithm</strong>—贪心算法获取最优切分点;</li><li><strong>approximate algorithm</strong>—近似算法，提出了候选分割点概念，先通过直方图算法获得候选分割点的分布情况，然后根据候选分割点将连续的特征信息映射到不同的buckets中，并统计汇总信息。详细见论文3.3节</li><li><strong>Weighted Quantile Sketch</strong>—分布式加权直方图算法，论文3.4节</li></ul><p><code>可并行的近似直方图算法</code>。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低.</p><blockquote><p>所以xgboost还提出了一种可并行的近似直方图算法，对特征排序后仅选择常数个候选分裂位置作为候选分裂点，用于高效地生成候选的分割点。</p></blockquote><p><strong>这里的加权就是指当进行某一个特征划分点查找的时候，利用样本的二阶导即$h_i$值作为样本的权重，而并不是按照样本原始个数取找分位数。</strong></p><h4 id="4-2-7-对缺失值的处理"><a href="#4-2-7-对缺失值的处理" class="headerlink" title="4.2.7 对缺失值的处理"></a>4.2.7 对缺失值的处理</h4><p>对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。<strong>稀疏感知算法</strong>，论文3.4节，Algorithm 3: Sparsity-aware Split Finding。具体处理方式可以概述如下：</p><ol><li>在特征k上寻找最佳 split point 时，不会对该列特征 missing 的样本进行遍历，而只对该列特征值为 non-missing 的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找 split point 的时间开销。</li><li>在逻辑实现上，为了保证完备性，会将该特征值missing的样本分别分配到左叶子结点和右叶子结点，两种情形都计算一遍后，选择分裂后增益最大的那个方向（左分支或是右分支），作为预测时特征值缺失样本的默认分支方向。</li></ol><p><em>一棵树中每个结点在分裂时，寻找的是某个特征的最佳分裂点（特征值），完全可以不考虑存在特征值缺失的样本，也就是说，如果某些样本缺失的特征值缺失，对寻找最佳分割点的影响不是很大。</em></p><h4 id="4-2-8-Built-in-Cross-Validation（内置交叉验证"><a href="#4-2-8-Built-in-Cross-Validation（内置交叉验证" class="headerlink" title="4.2.8 Built-in Cross-Validation（内置交叉验证)"></a>4.2.8 Built-in Cross-Validation（内置交叉验证)</h4><p>XGBoost允许在每一轮boosting迭代中使用交叉验证。因此，可以方便地获得最优boosting迭代次数。而GBM使用网格搜索，只能检测有限个值。</p><h4 id="4-2-9-continue-on-Existing-Model（接着已有模型学习）"><a href="#4-2-9-continue-on-Existing-Model（接着已有模型学习）" class="headerlink" title="4.2.9 continue on Existing Model（接着已有模型学习）"></a>4.2.9 continue on Existing Model（接着已有模型学习）</h4><p>XGBoost可以在上一轮的结果上继续训练。这个特性在某些特定的应用上是一个巨大的优势。</p><h4 id="4-2-10-High-Flexibility（高灵活性）"><a href="#4-2-10-High-Flexibility（高灵活性）" class="headerlink" title="4.2.10 High Flexibility（高灵活性）"></a>4.2.10 High Flexibility（高灵活性）</h4><p>XGBoost 允许用户定义自定义优化目标和评价标准，它对模型增加了一个全新的维度，所以我们的处理不会受到任何限制。</p><h4 id="4-2-11-并行化处理—系统设计模块-块结构设计等"><a href="#4-2-11-并行化处理—系统设计模块-块结构设计等" class="headerlink" title="4.2.11 并行化处理—系统设计模块,块结构设计等"></a>4.2.11 并行化处理—系统设计模块,块结构设计等</h4><ul><li><p>XGBoost的并行，<strong>并不是说每棵树可以并行训练</strong>，XGB本质上仍然采用boosting思想，每棵树训练前需要等前面的树训练完成才能开始训练。</p></li><li><p>XGBoost的并行，指的是<strong>特征维度的并行</strong>：在训练之前，每个特征按特征值对样本进行预排序，并存储为Block结构，在后面查找特征分割点时可以重复使用，而且特征已经被存储为一个个block结构，那么在寻找每个特征的最佳分割点时，可以利用多线程对每个block并行计算。</p></li></ul><h4 id="4-2-12-剪枝"><a href="#4-2-12-剪枝" class="headerlink" title="4.2.12 剪枝"></a>4.2.12 剪枝</h4><ol><li>在目标函数中增加了正则项：使用叶子结点的数目和叶子结点权重的L2模的平方，控制树的复杂度。</li><li>在结点分裂时，定义了一个阈值，如果分裂后目标函数的增益Gain小于该阈值(最小划分损失min_split_loss)，则不分裂。</li><li>当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值（最小样本权重和min_child_weight），也会放弃此次分裂。</li><li>XGBoost 先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的结点，进行剪枝。</li><li>当树达到最大深度时，停止建树，因为树的深度太深容易出现过拟合，这里需要设置一个超参数max_depth。</li></ol><p>对于最后一点，XGBoost会一直分裂到指定的最大深度(max_depth)，然后回过头来剪枝，即后剪枝。如果某个节点之后不再有正值，它会去除这个分裂。这种做法的优点，当一个负损失（如-2）后面有个正损失（如+10）的时候，就显现出来了。预剪枝会在-2处停下来，因为它遇到了一个负值。但是XGBoost会继续分裂，然后发现这两个分裂综合起来会得到+8，因此会保留这两个分裂。</p><h4 id="4-2-13-高速缓存压缩感知算法"><a href="#4-2-13-高速缓存压缩感知算法" class="headerlink" title="4.2.13 高速缓存压缩感知算法"></a>4.2.13 高速缓存压缩感知算法</h4><p>xgboost还设计了高速缓存压缩感知算法，这是系统设计模块的效率提升。当梯度统计不适合于处理器高速缓存和高速缓存丢失时，会大大减慢切分点查找算法的速度。</p><ul><li>针对 <code>exact greedy algorithm</code> 采用缓存感知预取算法</li><li>针对 <code>approximate algorithms</code> 选择合适的块大小</li></ul><h4 id="4-2-14-处理不平衡数据"><a href="#4-2-14-处理不平衡数据" class="headerlink" title="4.2.14 处理不平衡数据"></a>4.2.14 处理不平衡数据</h4><p>对于不平衡的数据集，例如用户的购买行为，肯定是极其不平衡的，这对 XGBoost 的训练有很大的影响，XGBoost 有两种自带的方法来解决：</p><ul><li><p>第一种，如果你在意AUC，采用AUC来评估模型的性能，那你可以通过设置<strong>scale_pos_weight</strong>来平衡正样本和负样本的权重。例如，当正负样本比例为1:10时，scale_pos_weight可以取10；</p></li><li><p>第二种，如果你在意概率(预测得分的合理性)，你不能重新平衡数据集(会破坏数据的真实分布)，应该设置<strong>max_delta_step</strong>为一个有限数字来帮助收敛（基模型为LR时有效）。</p></li></ul><p>原理是增大了少数样本的权重。除此之外，还可以通过<strong>上采样、下采样、SMOTE算法或者自定义代价函数</strong>的方式解决正负样本不平衡的问题。</p><h3 id="4-3-XGBoost不足之处"><a href="#4-3-XGBoost不足之处" class="headerlink" title="4.3 XGBoost不足之处"></a>4.3 XGBoost不足之处</h3><ol><li>每轮迭代时，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。</li><li>预排序方法（pre-sorted）：首先，空间消耗大。这样的算法需要保存数据的特征值，还保存了特征排序的结果（例如排序后的索引，为了后续快速的计算分割点），这里需要消耗训练数据两倍的内存。其次时间上也有较大的开销，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。</li><li>对cache优化不友好。在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的cache miss。</li></ol><h3 id="4-4-常用问题解答"><a href="#4-4-常用问题解答" class="headerlink" title="4.4 常用问题解答"></a>4.4 常用问题解答</h3><h4 id="4-4-1-RF和GBDT的区别"><a href="#4-4-1-RF和GBDT的区别" class="headerlink" title="4.4.1 RF和GBDT的区别"></a>4.4.1 RF和GBDT的区别</h4><p>他们都是由多棵树组成，最终的结果都是由多棵树一起决定。而不同点主要有：</p><ul><li><code>集成学习</code>：RF属于bagging思想，而GBDT是boosting思想；</li><li><code>偏差-方差权衡</code>：RF不断的降低模型的方差，而GBDT不断的降低模型的偏差；</li><li><code>训练样本</code>：RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本；</li><li><code>并行性</code>：RF的树可以并行生成，而GBDT只能顺序生成(需要等上一棵树完全生成)；</li><li><code>最终结果</code>：RF最终是多棵树进行多数表决（回归问题是取平均），而GBDT是加权融合；</li><li><code>数据敏感性</code>：RF对异常值不敏感，而GBDT对异常值比较敏感；</li><li><code>泛化能力</code>：RF不易过拟合，而GBDT容易过拟合。</li></ul><h4 id="4-4-2-XGBoost与GBDT有什么不同"><a href="#4-4-2-XGBoost与GBDT有什么不同" class="headerlink" title="4.4.2 XGBoost与GBDT有什么不同"></a>4.4.2 XGBoost与GBDT有什么不同</h4><ul><li><code>基分类器</code>：XGBoost的基分类器不仅支持CART决策树，还支持线性分类器，此时XGBoost相当于带L1和L2正则化项的Logistic回归（分类问题）或者线性回归（回归问题）。</li><li><code>导数信息</code>：XGBoost对损失函数做了二阶泰勒展开，GBDT只用了一阶导数信息，并且XGBoost还支持自定义损失函数，只要损失函数一阶、二阶可导。</li><li><code>正则项</code>：XGBoost的目标函数加了正则项， 相当于预剪枝，使得学习出来的模型更加不容易过拟合。</li><li><code>列抽样和缩减</code>：XGBoost支持列采样，与随机森林类似，同时对每棵树输出使用shrinkage，都是用于防止过拟合。</li><li><code>缺失值处理</code>：对树中的每个非叶子结点，XGBoost可以自动学习出它的默认分裂方向。如果某个样本该特征值缺失，会将其划入默认分支。</li><li>`并行化``：注意不是tree维度的并行，而是特征维度的并行。XGBoost预先将每个特征按特征值排好序，存储为块结构，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度。</li><li>可并行的近似直方图算法`：树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。</li></ul><h4 id="4-4-3-XGBoost为什么使用泰勒二阶展开"><a href="#4-4-3-XGBoost为什么使用泰勒二阶展开" class="headerlink" title="4.4.3 XGBoost为什么使用泰勒二阶展开"></a>4.4.3 XGBoost为什么使用泰勒二阶展开</h4><ul><li><strong>精准性</strong>：相对于GBDT的一阶泰勒展开，XGBoost采用二阶泰勒展开，阶信息本身就能让梯度收敛更快更准确。这一点在优化算法里的牛顿法里已经证实了。可以简单认为一阶导指引梯度方向，二阶导指引梯度方向如何变化。</li><li><strong>可扩展性</strong>：Xgboost官网上有说，当目标函数是MSE时，展开是一阶项（残差）+二阶项的形式（官网说这是一个nice form），而其他目标函数，如logloss的展开式就没有这样的形式。为了能有个统一的形式，所以采用泰勒展开来得到二阶项，这样就能把MSE推导的那套直接复用到其他自定义损失函数上。简短来说，就是为了统一损失函数求导的形式以支持自定义损失函数。</li></ul><h4 id="4-4-4-XGBoost为什么快"><a href="#4-4-4-XGBoost为什么快" class="headerlink" title="4.4.4 XGBoost为什么快"></a>4.4.4 XGBoost为什么快</h4><ul><li><strong>分块并行</strong>：训练前每个特征按特征值进行排序并存储为Block结构，后面查找特征分割点时重复使用，并且支持并行查找每个特征的分割点</li><li><strong>候选分位点</strong>：每个特征采用常数个分位点作为候选分割点</li><li><strong>CPU cache 命中优化</strong>： 使用缓存预取的方法，对每个线程分配一个连续的buffer，读取每个block中样本的梯度信息并存入连续的Buffer中。</li><li><strong>Block 处理优化</strong>：Block预先放入内存；Block按列进行解压缩；将Block划分到不同硬盘来提高吞吐</li></ul><h4 id="4-4-5-XGBoost防止过拟合的方法"><a href="#4-4-5-XGBoost防止过拟合的方法" class="headerlink" title="4.4.5 XGBoost防止过拟合的方法"></a>4.4.5 XGBoost防止过拟合的方法</h4><blockquote><ul><li><strong>目标函数添加正则项</strong>：叶子节点个数+叶子节点权重的L2正则化</li><li><strong>列抽样</strong>：训练的时候只用一部分特征（不考虑剩余的block块即可）</li><li><strong>子采样</strong>：每轮计算可以不使用全部样本，使算法更加保守</li><li><strong>shrinkage</strong>: 可以叫学习率或步长，为了给后面的训练留出更多的学习空间</li><li><strong>剪枝</strong>：多种方式限制树的复杂度，参考4.2.12。</li></ul></blockquote><h4 id="4-4-6-XGBoost中叶子结点的权重如何计算出来"><a href="#4-4-6-XGBoost中叶子结点的权重如何计算出来" class="headerlink" title="4.4.6 XGBoost中叶子结点的权重如何计算出来"></a>4.4.6 XGBoost中叶子结点的权重如何计算出来</h4><p>$\mathcal F = {f(x) = w_{q(x)}}(q: \mathbb R^m \rightarrow T, w \in \mathbb R^T)$是回归树（也叫做CART）的空间。</p><p>$q$表示将样本映射到叶节点的树的结构。$T$是每棵树叶子的数量。每个$F_k$对应了独立的树结构$q$和叶权值$w$。与决策树不同，每棵回归树的每个叶子上包含连续的连续值打分，<strong>我们用$w_i$表示第$i$个叶子的打分。</strong></p><p><strong>即这里每个样本最终在$q$结构的树上最终只会落在某一个叶子节点上，那么对应的$w$这个权重即为输出值。所以我们需要确定最优的$w$，而这个就需要根据最优的损失目标来确定。有$T$个叶子节点所以有$T$个$w$</strong></p><p>定义$I_j = {i|q(x_i)=j}$为叶子结点j里面的样本，我们可以通过扩展$\Omega$来重写公式（3）：</p><script type="math/tex; mode=display">\begin{array}{c}\tilde{\mathcal L}^{(t)} &=& \sum_{t = 1}^n [g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i)] + \gamma T + \frac{1}{2} \lambda \sum_{j = 1}^T w_j^2 \\&=& \sum_{j = 1}^T[(\sum_{i \in I_j} g_i) w_j + \frac{1}{2}(\sum_{i \in I_j} h_i + \lambda) w_j^2] + \gamma T\end{array}</script><p>对于一个固定的结构$q(x)$，我们可以根据二次函数求最值的方法计算叶子结点$j$的最优权重$w_j^{*}$，即在$w_j^{*}$取如下值的时候目标函数能够取最小值：</p><script type="math/tex; mode=display">w_j^* = -\frac{\sum_{i \in I_j} g_i} {\sum_{i \in I_j} h_i + \lambda}</script><p>并通过下式计算相应的目标函数最优值：</p><script type="math/tex; mode=display">\tilde{\mathcal{L}}^{(t)}(q)=-\frac{1}{2} \sum_{j=1}^{T} \frac{\left(\sum_{i \in I_{j}} g_{i}\right)^{2}}{\sum_{i \in I_{j}} h_{i}+\lambda}+\gamma T</script><p>我们需要检测这次分裂是否会给损失函数带来增益，增益的定义如下：</p><script type="math/tex; mode=display">\mathcal{L}_{\text {split}}=\frac{1}{2}\left[\frac{\left(\sum_{i \in I_{L}} g_{i}\right)^{2}}{\sum_{i \in I_{L}} h_{i}+\lambda}+\frac{\left(\sum_{i \in I_{R}} g_{i}\right)^{2}}{\sum_{i \in I_{R}} h_{i}+\lambda}-\frac{\left(\sum_{i \in I} g_{i}\right)^{2}}{\sum_{i \in I} h_{i}+\lambda}\right]-\gamma</script><h4 id="4-4-7-XGBoost如何评价特征的重要性"><a href="#4-4-7-XGBoost如何评价特征的重要性" class="headerlink" title="4.4.7 XGBoost如何评价特征的重要性"></a>4.4.7 XGBoost如何评价特征的重要性</h4><p>官方文档主要介绍了三种方法来评判XGBoost模型中特征的重要程度：</p><ul><li><code>weight</code> ：该特征在所有树中被用作分割样本的特征的总次数。</li><li><code>gain</code> ：该特征在其出现过的所有树中产生的平均增益。</li><li><code>cover</code> ：该特征在其出现过的所有树中的平均覆盖范围。</li></ul><p><em>注意：覆盖范围这里指的是一个特征用作分割点后，其影响的样本数量，即有多少样本经过该特征分割到两个子节点。</em></p><blockquote><p>除了这一部分，本人在实际项目中为了做模型的可解释性，发现了另一种方法。得益于XGBoost本身依然是树结构的模型，所以我们能够得到样本在打分过程中在每一棵树上所到达的每一个节点的权重值，通过这样的路径和权重值我们能够通过叶子结点沿着路径反向计算权重的差值即可得到每一个特征在各个节点处的贡献值。</p></blockquote><h4 id="4-4-8-XGBoost如何选择最佳分裂点"><a href="#4-4-8-XGBoost如何选择最佳分裂点" class="headerlink" title="4.4.8 XGBoost如何选择最佳分裂点"></a>4.4.8 XGBoost如何选择最佳分裂点</h4><p>在分裂一个结点时，我们会有很多个候选分割点，寻找最佳分割点的大致步骤如下：</p><ol><li>遍历每个结点的每个特征；</li><li>对每个特征，按特征值大小将特征值排序；</li><li>线性扫描，找出每个特征的最佳分裂特征值；</li><li>在所有特征中找出最好的分裂点（分裂后增益最大的特征及特征值）。</li></ol><p><em>上面是一种贪心的方法，每次进行分裂尝试都要遍历一遍全部候选分割点，也叫做全局扫描法。</em></p><p>但当数据量过大导致内存无法一次载入或者在分布式情况下，贪心算法的效率就会变得很低，全局扫描法不再适用。基于此，XGBoost提出了一系列加快寻找最佳分裂点的方案，也即4.2.6中提到的<strong>近似分位数（直方图）算法</strong>：</p><ul><li><strong>特征预排序+缓存</strong>：XGBoost在训练之前，预先对每个特征按照特征值大小进行排序，然后保存为block结构，后面的迭代中会重复地使用这个结构，使计算量大大减小。</li><li><strong>分位点近似法</strong>：对每个特征按照特征值排序后，采用类似分位点选取的方式，仅仅选出常数个特征值作为该特征的候选分割点，在寻找该特征的最佳分割点时，从候选分割点中选出最优的一个。</li><li><strong>并行查找</strong>：由于各个特性已预先存储为block结构，XGBoost支持利用多个线程并行地计算每个特征的最佳分割点，这不仅大大提升了结点的分裂速度，也极利于大规模训练集的适应性扩展。</li></ul><h2 id="5-LightGBM"><a href="#5-LightGBM" class="headerlink" title="5 LightGBM"></a>5 LightGBM</h2><h3 id="5-1-算法原理"><a href="#5-1-算法原理" class="headerlink" title="5.1 算法原理"></a>5.1 算法原理</h3><p>它是微软出的新的boosting框架，基本原理与XGBoost一样，只是在框架上做了一优化（重点在模型的训练速度的优化）。对于其细节算法可以参考本人对<a href="https://www.xiemingzhao.com/posts/c7ab2b84.html">原始论文的译文博客</a>。</p><h3 id="5-2-LightGBM和XGBoost的区别"><a href="#5-2-LightGBM和XGBoost的区别" class="headerlink" title="5.2 LightGBM和XGBoost的区别"></a>5.2 LightGBM和XGBoost的区别</h3><h4 id="5-2-1-树生长策略"><a href="#5-2-1-树生长策略" class="headerlink" title="5.2.1 树生长策略"></a>5.2.1 树生长策略</h4><p>XGB采用level-wise的分裂策略，LGB采用leaf-wise的分裂策略。XGB对每一层所有节点做无差别分裂，但是可能有些节点增益非常小，对结果影响不大，带来不必要的开销。Leaf-wise是在所有叶子节点中选取分裂收益最大的节点进行的，但是很容易出现过拟合问题，所以需要对最大深度做限制 。</p><p><strong>Level-wise</strong>：此为XGBoost主要是用的方法，过一次数据可以<strong>同时分裂同一层的叶子</strong>，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上Level-wise是一种<strong>低效算法</strong>，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。</p><p><strong>Leaf-wise</strong>：LightGBM使用的方法，是一种更为高效的策略，每次从当前所有叶子中，找到<strong>分裂增益最大</strong>的一个叶子，然后分裂，如此循环。因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。但是可能会长出比较深的决策树，<strong>产生过拟合</strong>。因此LightGBM在Leaf-wise之上增加了一个<strong>最大深度限制</strong>，在保证高效率的同时防止过拟合。</p><p><em>注意：目前XGBoost现在两种方式都是支持的</em></p><h4 id="5-2-2-分割点查找算法"><a href="#5-2-2-分割点查找算法" class="headerlink" title="5.2.2 分割点查找算法"></a>5.2.2 分割点查找算法</h4><p>lightgbm使用了基于histogram的切分点算法，这一点不同与xgboost中的预排序的 exact greedy 算法，histogram算法在内存和计算代价上都有不小优势。<strong>XGBoost里现在也提供了这一选项</strong>，不过默认的方法是对特征预排序。</p><p>直方图算法的基本思想：先把连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图。遍历数据时，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。<strong>直方图算法是一种牺牲了一定的切分准确性而换取训练速度以及节省内存空间消耗的算法。</strong></p><p>下面我们就看一下直方图算法的优势：</p><ul><li><strong>内存上优势</strong>。很明显，直方图算法的内存消耗为(# data <em> # features </em> 1Bytes)(因为对特征分桶后只需保存特征离散化之后的值)，而预排序的 exact greedy 算法内存消耗为：(2 <em> # data </em> # features* 4Bytes)，因为后者既要保存原始feature的值，也要保存这个值的顺序索引，这些值需要32位的浮点数来保存。直方图算法则不需要从而减少了并行训练的通信代价。</li><li><strong>计算上的优势</strong>。预排序算法在选择好分裂特征计算分裂收益时需要遍历所有样本的特征值，时间为O(# feature <em> # data),而直方图算法只需要遍历桶就行了，时间为O(#feature </em> # bins)。�𝑒×#𝑏�</li><li><strong>直方图做差加速</strong>。一个子节点的直方图可以通过父节点的直方图减去兄弟节点的直方图得到，从而加速计算。</li></ul><p>但实际上xgboost的近似直方图算法也类似于lightgbm这里的直方图算法，为什么xgboost的近似算法比lightgbm还是慢很多呢？</p><blockquote><p>xgboost在每一层都动态构建直方图，因为xgboost的直方图算法不是针对某个特定的feature，而是所有feature共享一个直方图(每个样本的权重是二阶导),所以每一层都要重新构建直方图，而lightgbm中对每个特征都有一个直方图，所以构建一次直方图就够了。</p></blockquote><h4 id="5-2-3-支持离散变量"><a href="#5-2-3-支持离散变量" class="headerlink" title="5.2.3 支持离散变量"></a>5.2.3 支持离散变量</h4><p>XGBoost无法直接输入类别型变量，因此需要事先对类别型变量进行编码（例如独热编码），而LightGBM可以<strong>直接输入 categorical 的 feature</strong>。在对离散特征分裂时，每个取值都当作一个桶，分裂时的增益算的是”是否属于某个category“的gain，类似于one-hot编码。</p><h4 id="5-2-4-缓存命中率"><a href="#5-2-4-缓存命中率" class="headerlink" title="5.2.4 缓存命中率"></a>5.2.4 缓存命中率</h4><p>使用<strong>collective communication</strong>算法替代了<strong>point-to-point communication</strong>算法提升了效率</p><ul><li>XGB使用Block结构的一个缺点是取梯度的时候，是通过索引来获取的，而这些梯度的获取顺序是按照特征的大小顺序的，这将导致非连续的内存访问，可能使得CPU cache缓存命中率低，从而影响算法效率。</li><li>而LGB是基于直方图分裂特征的，梯度信息都存储在一个个bin中，所以访问梯度是连续的，缓存命中率高。</li></ul><h4 id="5-2-5-并行策略"><a href="#5-2-5-并行策略" class="headerlink" title="5.2.5 并行策略"></a>5.2.5 并行策略</h4><p><strong>(1) 特征并行</strong></p><ul><li><code>LGB 特征并行</code>：</li></ul><ol><li>是每个worker留有一份完整的数据集（不经过采样的），这样就不必在切分后传输切分结果数据，因为每个机器已经持有完整的数据集；</li><li>各个机器上的worker根据所分配的特征子集寻找到局部的最优切分点(特征、阈值)；</li><li>worker之间需要相互通信，通过比对损失来确定的最佳切分点；</li><li>然后将这个最佳切分点的位置进行全局广播，每个worker进行切分即可。</li></ol><ul><li><code>XGB 的特征并行</code>：</li></ul><ol><li>对数据列采样，即不同的机器上保留不同的特征子集；</li><li>各个机器上的worker根据所分配的特征子集寻找到局部的最优切分点(特征、阈值)；</li><li>互相通信来从局部最佳切分点里得到最佳切分点；</li><li>拥有最佳切分点的worker执行切分操作，然后将切分结果传送给其他的worker；</li><li>其他的worker根据接收到的数据来切分数据。</li></ol><p>二者的区别就导致了LGB中 worker 间通信成本明显降低，<strong>只需通信一个特征分裂点即可</strong>。而XGB中要广播样本索引，计算量太大，并没有提升切分的效率，时间复杂度为O(#data)(因为每个worker持有所有行，需要处理全部的记录)，当数据量较大时特征并行并不能提升速度切分结果的通信代价，大约为O(#data/8)(若一个数据样本为1bit)</p><p><strong>Notes:LGB是典型的空间换时间，差别就是减少了传输切分结果的步骤，节省了这里的通信消耗</strong></p><p><strong>(2) 数据并行</strong> ：<br>当数据量很大，特征相对较少时，需要考虑数据并行策略。</p><blockquote><p><strong>XGB中的数据并行是传统做法</strong></p></blockquote><ol><li>行采样，对数据进行横向切分；</li><li>worker使用分配到的局部数据构建局部的直方图；</li><li>合并局部直方图得到全局的直方图；</li><li>对全局直方图寻找最优切分点，然后进行切分。</li></ol><blockquote><p><strong>LightGBM的做法(依然是降低通信代价)</strong></p></blockquote><ol><li>不同于合并所有的局部直方图获得全局的直方图，LightGBM通过Reduce Scatter方法来合并不同worker的无交叉的不同特征的直方图，这样找到该直方图的局部最优切分点，最后同步到全局最优切分点；</li><li>基于直方图做差的方法，先计算样本量少的节点的样本索引，在通信的过程中可以只传输某一叶节点的直方图，而对于其邻居可通过直接相减得到子节点的样本索引，这个直方图算法使得worker间的通信成本降低一倍，因为只用通信以此样本量少的节点。通信的时间复杂度为O(0.5<em>#feature</em>#bin)</li></ol><p><strong>Notes: 传统做法的通信代价过高，若使用point-to-point的通信算法，每个机器的通信代价时间复杂度为O(# machine <em> # feature </em> # bin)，若使用collective通信算法则通信代价为O(2 <em> # feature </em> \ # bin)</strong></p><p><strong>(3) 投票并行（LGB）</strong>：</p><p>当数据量和维度都很大时，选用投票并行，该方法是数据并行的一个改进。数据并行中的合并直方图的代价相对较大，尤其是当特征维度很大时。</p><p><code>大致思想</code>：<strong>每个worker首先会找到本地的一些优秀的特征，然后进行全局投票，根据投票结果，选择top的特征进行直方图的合并，再寻求全局的最优分割点。</strong></p><h4 id="5-2-6-Early-Stopping"><a href="#5-2-6-Early-Stopping" class="headerlink" title="5.2.6 Early Stopping"></a>5.2.6 Early Stopping</h4><p>XGBoost,LightGBM都支持早停止，不过在细节上略有不同。XGBoost 和 LightGBM  里的  early_stopping 则都是用来控制基学习器的数目的。</p><ul><li>两者都可以使用多组评价指标，但是不同之处在于 XGBoost 会根据指标列表中的最后一项指标控制模型的早停止，而 LightGBM 则会受到所有的评估指标的影响；</li><li>在使用 early stopping 控制迭代次数后，模型直接返回的是最后一轮迭代的学习器不一定是最佳学习器，而在做出预测时可以设置参数选择某一轮的学习器作出预测。</li><li>XGBoost 里保存了三种状态的学习器，分别是 bst.best_score, bst.best_iteration, bst.best_ntree_limit,官方的建议是在做预测时设置为 bst.best_ntree_limit，实际使用时感觉 bst.best_iteration 和 bst.best_ntree_limit 的表现上区别不大</li><li>LightGBM 则仅提供了 bst.best_iteration 这一种方式。</li></ul><p><strong>参考博文：</strong><br><a href="https://blog.csdn.net/u014248127/article/details/79015803">yealxxy: RF,GBDT,XGBoost,lightGBM的对比</a><br><a href="https://blog.csdn.net/data_scientist/article/details/79022025">data_scientist:F、GBDT、XGBoost、lightGBM原理与区别</a><br><a href="https://baijiahao.baidu.com/s?id=1645723756242129387&amp;wfr=spider&amp;for=pc">启迪云:XGBoost超详细推导，终于有人讲明白了</a><br><a href="https://www.cnblogs.com/cassielcode/p/12469053.html">AlwaysBeta:XGBoost20题</a><br><a href="https://blog.csdn.net/yimingsilence/article/details/82193890">默一鸣:RF,GBDT,XGBOOST, LightGBM的对比和分析</a></p><hr>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-前言&quot;&gt;&lt;a href=&quot;#1-前言&quot; class=&quot;headerlink&quot; title=&quot;1. 前言&quot;&gt;&lt;/a&gt;1. 前言&lt;/h2&gt;&lt;p&gt;RF,GBDT,XGBoost,lightGBM都属于&lt;strong&gt;集成学习（Ensemble Learning）&lt;/strong&gt;，集成学习的目的是通过结合多个基学习器的预测结果来改善基本学习器的泛化能力和鲁棒性。&lt;/p&gt;
&lt;p&gt;根据基本学习器的生成方式，目前的集成学习方法大致分为两大类：即基本学习器之间存在强依赖关系、必须串行生成的&lt;strong&gt;序列化方法&lt;/strong&gt;；以及基本学习器间不存在强依赖关系、可同时生成的&lt;strong&gt;并行化方法&lt;/strong&gt;。前者的代表就是Boosting，后者的代表是Bagging和“随机森林”（Random Forest）。&lt;/p&gt;
&lt;p&gt;本文主要从下面四个广泛讨论和使用的方法进行了对比分析总结：&lt;br&gt;&lt;strong&gt;RF（随机森林）,GBDT（梯度提升决策树）,XGBoost,lightGBM&lt;/strong&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="学习笔记" scheme="https://www.xiemingzhao.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="机器学习" scheme="https://www.xiemingzhao.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="LightGBM" scheme="https://www.xiemingzhao.com/tags/LightGBM/"/>
    
    <category term="XGBoost" scheme="https://www.xiemingzhao.com/tags/XGBoost/"/>
    
    <category term="GBDT" scheme="https://www.xiemingzhao.com/tags/GBDT/"/>
    
    <category term="RF" scheme="https://www.xiemingzhao.com/tags/RF/"/>
    
  </entry>
  
</feed>
