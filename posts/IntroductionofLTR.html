<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">


  <link rel="manifest" href="/images/manifest.json">


  <meta name="msapplication-config" content="/images/browserconfig.xml" />



  <meta name="keywords" content="机器学习,排序,LTR," />





  <link rel="alternate" href="/atom.xml" title="小火箭的博客" type="application/atom+xml" />






<meta name="description" content="1 Learning to Rank 简介Learning to Rank 是采用机器学习算法，通过训练模型来解决排序问题，在Information Retrieval，Natural Language Processing，Data Mining等领域有着很多应用。 1.1 排序问题如图 Fig.1 所示，在信息检索中，给定一个query，搜索引擎会召回一系列相关的Documents（通过ter">
<meta property="og:type" content="article">
<meta property="og:title" content="LTR(Learning to Rank)概述">
<meta property="og:url" content="https://www.xiemingzhao.com/posts/IntroductionofLTR.html">
<meta property="og:site_name" content="小火箭的博客">
<meta property="og:description" content="1 Learning to Rank 简介Learning to Rank 是采用机器学习算法，通过训练模型来解决排序问题，在Information Retrieval，Natural Language Processing，Data Mining等领域有着很多应用。 1.1 排序问题如图 Fig.1 所示，在信息检索中，给定一个query，搜索引擎会召回一系列相关的Documents（通过ter">
<meta property="og:locale">
<meta property="og:image" content="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/ltr-1.png">
<meta property="og:image" content="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/ltr-2.png">
<meta property="og:image" content="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/ltr-3.png">
<meta property="og:image" content="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/ltr-4.png">
<meta property="og:image" content="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/ltr-5.png">
<meta property="og:image" content="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/ltr-6.png">
<meta property="article:published_time" content="2019-06-28T16:00:00.000Z">
<meta property="article:modified_time" content="2025-03-30T18:12:50.475Z">
<meta property="article:author" content="小火箭">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="排序">
<meta property="article:tag" content="LTR">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/ltr-1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://www.xiemingzhao.com/posts/IntroductionofLTR.html"/>





  <title>LTR(Learning to Rank)概述 | 小火箭的博客</title>
  








<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>


<script type="text/javascript"
color="0,0,0" opacity='0.5' zIndex="-1" count="150" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>


<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
    
    <a href="https://github.com/xiemingzhao"><img style="position:absolute;top:0;right:0;border:0;" src="https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png?resize=149%2C149" class="attachment-full size-full" alt="Fork me on GitHub" data-recalc-dims="1"></a>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">小火箭的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">愿世界和平！！！</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            公益404
          </a>
        </li>
      
        
        <li class="menu-item menu-item-guestbook">
          <a href="/guestbook/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-comments"></i> <br />
            
            留言板
          </a>
        </li>
      
        
        <li class="menu-item menu-item-others">
          <a href="/others/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-folder"></i> <br />
            
            其他
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.xiemingzhao.com/posts/IntroductionofLTR.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.postimg.cc/vBxZQfvz/img-0182.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小火箭的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">LTR(Learning to Rank)概述</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-29T00:00:00+08:00">
                2019-06-29
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index">
                    <span itemprop="name">学习笔记</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/" itemprop="url" rel="index">
                    <span itemprop="name">算法总结</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/posts/IntroductionofLTR.html#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/posts/IntroductionofLTR.html" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i> 阅读数
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>次
            </span>
          

          
            <div class="post-wordcount">
              
                
                  <span class="post-meta-divider">|</span>
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  4k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  15
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="1-Learning-to-Rank-简介"><a href="#1-Learning-to-Rank-简介" class="headerlink" title="1 Learning to Rank 简介"></a>1 Learning to Rank 简介</h2><p><code>Learning to Rank</code> 是采用机器学习算法，通过训练模型来解决排序问题，在Information Retrieval，Natural Language Processing，Data Mining等领域有着很多应用。</p>
<h3 id="1-1-排序问题"><a href="#1-1-排序问题" class="headerlink" title="1.1 排序问题"></a>1.1 排序问题</h3><p>如图 Fig.1 所示，在信息检索中，给定一个query，搜索引擎会召回一系列相关的Documents（通过term匹配，keyword匹配，或者semantic匹配的方法），然后便需要对这些召回的Documents进行排序，最后将Top N的Documents输出,一版可以认为是召回后的精排。而排序问题就是使用一个模型 f(q,d)来对该query下的documents进行排序，这个模型可以是人工设定一些参数的模型，也可以是用机器学习算法自动训练出来的模型。现在第二种方法越来越流行，尤其在Web Search领域，因为在Web Search 中，有很多信息可以用来确定query-doc pair的相关性，而另一方面，由于大量的搜索日志的存在，可以将用户的点击行为日志作为training data，使得通过机器学习自动得到排序模型成为可能。</p>
<p><strong>需要注意的是，排序问题最关注的是各个Documents之间的相对顺序关系，而不是各个Documents的预测分最准确。</strong></p>
<span id="more"></span>
<p>Learning to Rank是监督学习方法，所以会分为training阶段和testing阶段，如图 Fig.2  所示。</p>
<center class="half">
    <img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/ltr-1.png" width="400"/><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/ltr-2.png" width="400"/>
</center>

<h4 id="1-1-1-Training-data的生成"><a href="#1-1-1-Training-data的生成" class="headerlink" title="1.1.1 Training data的生成"></a>1.1.1 Training data的生成</h4><p>对于Learning to Rank，training data是必须的，而feature vector通常都是可以得到的，关键就在于label的获取，而这个label实际上反映了query-doc pair的真实相关程度。通常我们有两种方式可以进行label的获取：</p>
<ul>
<li><p>第一种方式是<strong>人工标注</strong>，这种方法被各大搜索引擎公司广为应用。人工标注即对抽样出来作为training data的query-doc pair人为地进行相关程度的判断和标注。一般标注的相关程度分为5档：perfect，excellent，good，fair，bad。例如，query=“Microsoft”，这时候，Microsoft的官网是perfect；介绍Microsoft的wikipedia则是excellent；一篇将Microsoft作为其主要话题的网页则是good；一篇只是提到了Microsoft这个词的网页则是fair，而一篇跟Microsoft毫不相关的网页则是bad。人工标注的方法可以通过多人同时进行，最后以类似投票表决的方式决定一个query-doc pair的相关程度，这样可以相对减少各个人的观点不同带来的误差。</p>
</li>
<li><p>第二种方式是<strong>通过搜索日志获取</strong>。搜索日志记录了人们在实际生活中的搜索行为和相应的点击行为，点击行为实际上隐含了query-doc pair的相关性，所以可以被用来作为query-doc pair的相关程度的判断。一种最简单的方法就是利用同一个query下，不同doc的点击数的多少来作为它们相关程度的大小。实际中一般使用行为/曝光来制作target，但是最终还是要将连续型的值进行分桶，否则泛化能力不够。</p>
</li>
</ul>
<p>不过需要注意的是，这里存在着一个很大的陷阱，就是用户的点击行为实际上是存在 <strong>position bias</strong> 的，即用户偏向于点击位置靠前的doc，即便这个doc并不相关或者相关性不高。有很多 tricky的和 general 的方法可以用来去除这个“position bias”，例如，</p>
<ol>
<li>当位置靠后的doc的点击数都比位置靠前的doc的点击数要高了，那么靠后的doc的相关性肯定要比靠前的doc的相关性大。</li>
<li>Joachims等人则提出了一系列去除bias的方法，例如 Click &gt; Skip Above, Last Click &gt; Skip Above, Click &gt; Earlier Click, Click &gt; Skip Previous, Click &gt; No Click Next等。</li>
<li>有个很tricky但是效果很不错的方法，之前我们说一个doc的点击数比另一个doc的点击数多，并不一定说明前者比后者更相关。但如果两者的差距大到一定程度了，即使前者比后者位置靠前，但是两者的点击数相差5-10倍，这时候我们还是愿意相信前者更加相关。当然这个差距的大小需要根据每个场景具体的调整。</li>
<li>position bias 存在的原因是，永远无法保证在一次搜索行为中，用户能够看到所有的结果，往往只看到前几位的结果。这时候就到了 Click Model大显身手的时候了，一系列的 Click Model 根据用户的点击信息对用户真正看到的doc进行“筛选”，进而能更准确地看出用户到底看到了哪些doc，没有看到哪些doc，一旦这些信息知道了，那么我们就可以根据相对更准确的 点击数/展示数（即展现CTR）来确定各个doc的相关性大小。</li>
</ol>
<p>上述讲到的两种label获取方法各有利弊。人工标注受限于标注的人的观点，不同的人有不同的看法，而且毕竟标注的人不是真实搜索该query的用户，无法得知其搜索时候的真实意图；另一方面人工标注的方法代价较高且非常耗时。而从搜索日志中获取的方法则受限于用户点击行为的噪声，这在长尾query中更是如此，且有用户点击的query毕竟只是总体query的一个子集，无法获取全部的query下doc的label。</p>
<h4 id="1-1-2-Feature的生成"><a href="#1-1-2-Feature的生成" class="headerlink" title="1.1.2 Feature的生成"></a>1.1.2 Feature的生成</h4><p>这里只是简单介绍下，后续博客会有更纤细的讲解。</p>
<p>一般Learning to Rank的模型的feature分为两大类：<strong>relevance 和 importance（hotness）</strong>，即query-doc pair 的相关性feature，和doc本身的热门程度的feature。两者中具有代表性的分别是 BM25 和 PageRank。</p>
<h4 id="1-1-3-Evaluation"><a href="#1-1-3-Evaluation" class="headerlink" title="1.1.3 Evaluation"></a>1.1.3 Evaluation</h4><p>怎么判断一个排序模型的好坏呢？我们需要有验证的方法和指标。方法简单来说就是，比较模型的输出结果，和真实结果（ground truth）之间的差异大小。<em>用于Information Retrieval的排序衡量指标通常有：NDCG，MAP等。</em></p>
<p><strong>NDCG（Normalized Discounted Cumulative Gain）：</strong><br>NDCG表示了从第1位doc到第k位doc的“归一化累积折扣信息增益值”。其基本思想是：</p>
<ul>
<li>每条结果的相关性分等级来衡量</li>
<li>考虑结果所在的位置，位置越靠前的则重要程度越高</li>
<li>等级高（即好结果）的结果位置越靠前则值应该越高，否则给予惩罚</li>
</ul>
<script type="math/tex; mode=display">NDCG(k)=G_{max,i}^{-1}(k)\sum_{j:\pi_i(j) \leq k}G(j)D(\pi_i(j))</script><p>其中G表示了这个doc得信息增益大小，一般与该doc的相关程度正相关：</p>
<script type="math/tex; mode=display">G(j)=2^{y_{i,j}}-1</script><p>D则表示了该doc所在排序位置的折扣大小，一般与位置负相关：</p>
<script type="math/tex; mode=display">D(\pi_i(j))=\frac{1}{log_2(1+\pi_i(j))}</script><p>而$G_{max}$则表示了归一化系数，是最理想情况下排序的“累积折扣信息增益值”。<br>最后，将每个query下的NDCG值平均后，便可以得到排序模型的总体NDCG大小。</p>
<p><strong>MAP（Mean Average Precision）</strong>：</p>
<p>其定义是求每个相关文档检索出后的准确率的平均值（即Average Precision）的算术平均值（Mean）。这里对准确率求了两次平均，因此称为 <code>Mean Average Precision</code>。</p>
<p>在MAP中，对query-doc pair的相关性判断只有两档：1和0。<br>对于一个query，其AP值为：</p>
<script type="math/tex; mode=display">AP=\frac{\sum_{j=1}^{n_i} P(j) \cdot y_{i,j} }{\sum_{j=1}^{n_i}y_{i,j} }</script><p>$y_{ij}$即每个doc的label（1和0），而每个query-doc pair的P值代表了到dij这个doc所在的位置为止的precision：</p>
<script type="math/tex; mode=display">P(j)=\frac{\sum_{k:\pi_i(k)\leq \pi_i(j)}y_{i,k} }{\pi_i(j)}</script><p>其中，$\pi<em>i(j)$是$d</em>{ij}$在排序中的位置。</p>
<h3 id="1-2-Formulation"><a href="#1-2-Formulation" class="headerlink" title="1.2 Formulation"></a>1.2 Formulation</h3><p>用通用的公式来表示Learning to Rank算法，loss function为$L(F(x),y)$，从而risk function（loss function在X，Y联合分布下的期望值）为：</p>
<script type="math/tex; mode=display">R(F)=\int_{\mathcal{X},\mathcal{Y}}L(F(x),y) \mathcal{d}P(x,y)</script><p>有了training data后，进一步得到empirical risk function：</p>
<script type="math/tex; mode=display">\hat R(F)=\frac{1}{m} \sum_{i=1}^m L'(F(x_i),y_i)</script><p>于是，学习问题变成了如何最小化这个empirical risk function。而这个优化问题很难解决，因为loss function不连续。于是可以使用一个方便求解的surrogate function来替换原始loss function，转而优化这个替换函数：</p>
<script type="math/tex; mode=display">\hat {R'}(F)=\frac{1}{m} \sum_{i=1}^m L'(F(x_i),y_i)</script><p>替换函数的选择有很多种，根据Learning to Rank的类型不同而有不同的选择：</p>
<p>1）pointwise loss：例如squared loss等。</p>
<script type="math/tex; mode=display">L'(F(x),y)=\sum_{i=1}^n (f(x_i),y_i)^2</script><p>2）pairwise loss：例如hinge loss，exponential loss，logistic loss等。</p>
<script type="math/tex; mode=display">L'(F(x),y)=\sum_{i=1}^{n-1} \sum_{j=i+1}^n \phi(sign(y_i-y_j),f(x_i)-f(x_j))</script><p>3）listwise loss：</p>
<script type="math/tex; mode=display">L'(F(x),y)=exp(-NDCG)</script><h3 id="1-3-Learning-to-Rank-Methods"><a href="#1-3-Learning-to-Rank-Methods" class="headerlink" title="1.3 Learning to Rank Methods"></a>1.3 Learning to Rank Methods</h3><p><strong>Learning to Rank 方法可以分为三种类型：pointwise，pairwise，和listwise</strong>。</p>
<p>pointwise 和 pairwise 方法将排序问题转化为 classification，regression，ordinal classification 等问题.</p>
<ul>
<li>优点是可以直接利用已有的classificatin和regression算法;</li>
<li>缺点是group structure其实是被忽略的，即不会考虑每个query下所有doc之间的序关系。</li>
</ul>
<p>导致其学习目标和真实的衡量排序的目标并不一定是一致的（很多排序衡量指标，例如NDCG都是衡量每个query下的整体list的序关系的）。而listwise方法则将一个ranking list作为一个instance来进行训练，其实会考虑每个query下所有doc之间的序关系的。</p>
<p>这三种类型的 Learning to Rank 方法的具体算法一般有：</p>
<ul>
<li>Pointwise: Subset Ranking, McRank, Prank, OC SVM</li>
<li>Pairwise: Ranking SVM, RankBoost, RankNet, GBRank, IR SVM, Lambda Rank, LambdaMart</li>
<li>Listwise: ListNet, ListMLE, AdaRank, SVM MAP, Soft Rank</li>
</ul>
<p>针对各个具体的算法介绍，后续的博客会进一步给出，这里就不再多加详述了。</p>
<h2 id="2-RankNet，LambdaRank，LambdaMart简介"><a href="#2-RankNet，LambdaRank，LambdaMart简介" class="headerlink" title="2 RankNet，LambdaRank，LambdaMart简介"></a>2 RankNet，LambdaRank，LambdaMart简介</h2><h3 id="2-1-RankNet"><a href="#2-1-RankNet" class="headerlink" title="2.1 RankNet"></a>2.1 RankNet</h3><p>RankNet是2005年微软提出的一种pairwise的Learning to Rank算法，它从概率的角度来解决排序问题。RankNet的核心是提出了一种概率损失函数来学习Ranking Function，并应用Ranking Function对文档进行排序。这里的Ranking Function可以是任意对参数可微的模型，也就是说，该概率损失函数并不依赖于特定的机器学习模型，在论文中，RankNet是基于神经网络实现的。除此之外，GDBT等模型也可以应用于该框架。</p>
<h4 id="2-1-1-相关性概率"><a href="#2-1-1-相关性概率" class="headerlink" title="2.1.1 相关性概率"></a>2.1.1 相关性概率</h4><p>我们先定义两个概率：预测相关性概率、真实相关性概率。</p>
<p><strong>（1）预测相关性概率</strong><br>对于任意一个doc对$(U_i,U_j)$，模型输出的score分别为$s_i$和$s_j$，那么根据模型的预测，$U_i$比$U_j$与Query更相关的概率为：</p>
<script type="math/tex; mode=display">P_{ij}=P(U_i>U_j)=\frac{1}{1+e^{-\sigma (s_i-s_j)}}</script><p>由于RankNet使用的模型一般为神经网络，根据经验sigmoid函数能提供一个比较好的概率评估。参数σ决定sigmoid函数的形状，对最终结果影响不大。</p>
<blockquote>
<p>RankNet证明了如果知道一个待排序文档的排列中相邻两个文档之间的排序概率，则通过推导可以算出每两个文档之间的排序概率。因此对于一个待排序文档序列，只需计算相邻文档之间的排序概率，不需要计算所有pair，减少计算量。</p>
</blockquote>
<p><strong>（2）真实相关性概率</strong><br>对于训练数据中的$U_i$和$U_j$，它们都包含有一个与Query相关性的真实label，比如$U_i$与Query的相关性label为good，$U_j$与Query的相关性label为bad，那么显然$U_j$比$U_j$更相关。我们定义$U_j$比$U_j$更相关的真实概率为：</p>
<script type="math/tex; mode=display">\bar P_{ij}=\frac{1}{2}(1+S_{ij})</script><p>如果$U<em>i$比$U_j$更相关，那么$S</em>{ij}$=1；如果$U<em>i$不如$U_j$相关，那么$S</em>{ij}$=−1；如果$U<em>i$、$U_j$与Query的相关程度相同，那么$S</em>{ij}$=0。通常，两个doc的relative relevance judgment可由人工标注或者从搜索日志中获取得到。</p>
<h4 id="2-1-2-损失函数"><a href="#2-1-2-损失函数" class="headerlink" title="2.1.2 损失函数"></a>2.1.2 损失函数</h4><p>对于一个排序，RankNet从各个doc的相对关系来评价排序结果的好坏，排序的效果越好，那么有错误相对关系的pair就越少。所谓错误的相对关系即如果根据模型输出$U_i$排在$U_j$前面，但真实label为$U_i$的相关性小于$U_j$，那么就记一个错误pair，RankNet本质上就是以错误的pair最少为优化目标。而在抽象成cost function时，<strong>RankNet实际上是引入了概率的思想：不是直接判断$U_i$排在$U_j$前面，而是说$U_i$以一定的概率P排在$U_j$前面，即是以预测概率与真实概率的差距最小作为优化目标</strong>。</p>
<p>最后，RankNet使用Cross Entropy作为cost function，来衡量$P<em>{ij}$对$\bar P</em>{ij}$的拟合程度：</p>
<script type="math/tex; mode=display">C=-\bar P_{ij}logP_{ij}-(1-\bar P_{ij})log(1-P_{ij})</script><p>带入相应等式整理得：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
C_{ij} &= -\frac{1}{2}(1+S_{ij})log \frac{1}{1+e^{-\sigma(s_i-s_j)}} -\frac{1}{2}(1-S_{ij}) log \frac{e^{-\sigma (s_i-s_j)}}{1+e^{-\sigma (s_i-s_j)}} \\
&=-\frac{1}{2}(1+S_{ij})log \frac{1}{1+e^{-\sigma(s_i-s_j)}}-\frac{1}{2}(1-S_{ij})[-\sigma(s_i-s_j)+log \frac{1}{1+e^{-\sigma(s_i-s_j)}}] \\
&=\frac{1}{2}(1-S_{ij})\sigma(s_i-s_j)+log(1+e^{-\sigma(s_i-s_j)})
\end{array}</script><p>其中：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
C=\left\{
\begin{array}{lr}
log(1+e^{-\sigma(s_i-s_j)}), & S_{ij}=1  \\
log(1+e^{-\sigma(s_j-s_i)}), & S_{ij}=-1 \\
\end{array}
\right.
\end{array}</script><p>下面展示了当$S_{ij}$分别取1，0，-1的时候cost function以$s_i-s_j$为变量的示意图：</p>
<p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/ltr-3.png" alt="cost function.jpg"></p>
<p>可以看到当$S<em>{ij}$=1时，模型预测的$s_i$比$s_j$越大，其代价越小；$S</em>{ij}$=−1时，$s<em>i$比$s_j$j越小，代价越小；$S</em>{ij}$=0时，代价的最小值在$s_i$与$s_j$相等处取得。</p>
<p>该损失函数有以下几个特点：</p>
<ul>
<li>当两个相关性不同的文档算出来的模型分数相同时，损失函数的值大于0，仍会对这对pair做惩罚，使他们的排序位置区分开。</li>
<li>损失函数是一个类线性函数，可以有效减少异常样本数据对模型的影响，因此具有鲁棒性。</li>
</ul>
<p>所以一个query的总代价为：</p>
<script type="math/tex; mode=display">C=\sum_{(i,j)\in I}C_{ij}</script><p>其中，I表示所有在同一query下，且具有不同relevance judgment的doc pair，每个pair有且仅有一次。</p>
<h4 id="2-1-3-合并概率"><a href="#2-1-3-合并概率" class="headerlink" title="2.1.3 合并概率"></a>2.1.3 合并概率</h4><p>上述的模型$P_{ij}$需要保持一致性，即如果Ui的相关性高于$U_j$，$U_j$的相关性高于$U_k$，则Ui的相关性也一定要高于$U_k$。否则，如果不能保持一致性，那么上面的理论就不好使了。</p>
<p>我们使用$U_i$ vs $U_j$的真实概率 和 $U_j$ vs $U_k$ 的真实概率，计算$U_j$ vs $U_k$的真实概率：</p>
<script type="math/tex; mode=display">\bar P_{ik}=\frac{\bar P_{ij}\bar P_{jk}}{1+2\bar P_{ij}\bar P_{jk}-\bar P_{ij}-\bar P_{jk}}</script><p>若$\bar P<em>{ij}=\bar P</em>{jk}=P$,则有如下图所示：</p>
<p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/ltr-4.png" alt="$P_{ik}$变化图.jpg"></p>
<ol>
<li>$P=0$时，有$\bar P_{i,k}=P=0$表示：$D_i$排$D_j$后面,$D_j$排$D_k$的后面，则$D_i$一定排$D_k$的后面；</li>
<li>$0&lt;P&lt;0.5$时，\bar P_{i,k} &lt; P$；</li>
<li>$P=0.5$时，有\bar P_{i,k} = P = 0.5$表示：$D_i$有一半概率排在$D_j$前面，$D_j$也有一半概率排在$D_k$的前面，则$D_i$同样也是一半的概率排在$D_k$的前面；</li>
<li>$0.5 &lt; P <1$时，$\bar P_{i,k}>P$；</li>
<li>$P=1$时，有$\bar P_{i,k}=P=1$表示：$D_i$排在$D_j$前面，$D_j$排在$D_k$的前面，则$D_i$也一定排在$D_k$的前面；</li>
</ol>
<h4 id="2-1-4-Gradient-Descent"><a href="#2-1-4-Gradient-Descent" class="headerlink" title="2.1.4 Gradient Descent"></a>2.1.4 Gradient Descent</h4><p>我们获得了一个可微的代价函数，下面我们就可以用随机梯度下降法来迭代更新模型参数$w_k$了，即</p>
<script type="math/tex; mode=display">w_k \rightarrow w_k - \eta \frac{\partial C}{\partial w_k}</script><p>$\eta$为步长，代价C沿负梯度方向变化。</p>
<script type="math/tex; mode=display">\Delta =\sum_k \frac{\partial C}{\partial w_k} \delta w_k = \sum_k\frac{\partial C}{\partial w_k}(\eta \frac{\partial C}{\partial w_k})=-\eta \sum_k (\frac{\partial C}{\partial w_k})^2<0</script><p>这表明沿负梯度方向更新参数确实可以降低总代价。而使用了随机梯度下降法时，有：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
\frac{\partial C}{\partial w_k} &= \frac{\partial C}{\partial s_i} \frac{\partial s_i}{\partial w_k} + \frac{\partial C}{\partial s_j} \frac{\partial s_j}{\partial w_k} \\
&= \sigma (\frac{1}{2}(1-S_{ij})-\frac{1}{1 + e^{\sigma (s_i - s_j)}})(\frac{\partial s_i}{\partial w_k}-\frac{\partial s_j}{\partial w_k}) \\
&=\lambda_{ij}(\frac{\partial s_i}{\partial w_k}-\frac{\partial s_j}{\partial w_k}) \\
\end{array}</script><p>其中：</p>
<script type="math/tex; mode=display">\lambda_{ij}=\frac{\partial C(s_i-s_j)}{\partial s_i} = \sigma (\frac{1}{2}(1-S_{ij})-\frac{1}{1+e^{\sigma (s_i-s_j)}})</script><h4 id="2-1-5-加速RankNet训练过程"><a href="#2-1-5-加速RankNet训练过程" class="headerlink" title="2.1.5 加速RankNet训练过程"></a>2.1.5 加速RankNet训练过程</h4><p>上面的是对于每一对pair都会进行一次权重的更新，其实是可以对<strong>同一个query下的所有文档pair</strong>全部带入神经网络进行前向预测，然后计算总差分并进行误差后向反馈，这样将大大减少误差反向传播的次数。</p>
<p>即，我们可以转而利用<strong>批处理的梯度下降法</strong>：</p>
<script type="math/tex; mode=display">\frac{\partial C}{\partial w_k}=\sum_{(i ,j) \in I}(\frac{\partial C_{ij}}{\partial s_i} \frac{\partial s_i}{\partial w_k} + \frac{\partial C_{ij}}{\partial s_j} \frac{\partial s_j}{\partial w_k})</script><p>其中：</p>
<script type="math/tex; mode=display">\frac{\partial C_{ij}}{\partial s_i}=\sigma (\frac{1}{2}(1-S_{ij})-\frac{1}{1+e^{\sigma (s_i-s_j)}})=-\frac{\partial C_{ij}}{\partial s_j}</script><p>令：</p>
<script type="math/tex; mode=display">\lambda_{ij}=\frac{\partial C_{ij}}{\partial s_i} = \sigma(\frac{1}{2}(1-S_{ij})-\frac{1}{1+e^{\sigma (s_i-s_j)}})</script><p>于是有：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
\frac{\partial C}{\partial w_k} &= \sum_{(i,j) \in I}\sigma (\frac{1}{2}(1-S_{ij})-\frac{1}{1+e^{\sigma (s_i-s_j)}})(\frac{\partial s_i}{\partial w_k}-\frac{\partial s_j}{\partial w_k}) \\
&=\sum_{(i,j) \in I} \lambda_{ij}(\frac{\partial s_i}{\partial w_k}-\frac{\partial s_j}{\partial w_k})\\
&=\sum_i \lambda_i \frac{\partial s_i}{\partial w_k} \\
\end{array}</script><p>下面我们来看看这个$\lambda<em>i$是什么。前面讲过集合I中只包含label不同的doc的集合，且每个pair仅包含一次，即$(U_i,U_j)$与$(U_j,U_i)$等价。为方便起见，我们假设I中只包含$(U_i,U_j)$)表示$U_i$相关性大于$U_j$的pair，即I中的pair均满足$S</em>{ij}=1$，那么</p>
<script type="math/tex; mode=display">\lambda_i=\sum_{j:(i,j)\in I}\lambda_{ij}-\sum_{j:(j,i)\in I}\lambda_{ij}</script><p>这个写法是Burges的paper上的写法。下面我们用一个实际的例子来看：有三个doc，其真实相关性满足$U_1&gt;U_2&gt;U_3$，那么集合I中就包含{(1,2), (1,3), (2,3)}共三个pair，那么:</p>
<script type="math/tex; mode=display">\frac{\partial C}{\partial w_k}=(\lambda_{12} \frac{\partial s_1}{\partial w_k}-\lambda_{12}\frac{\partial s_2}{\partial w_k})+(\lambda_{13}\frac{\partial s_1}{\partial w_k}-\lambda_{13}\frac{\partial s_3}{\partial w_k})+(\lambda_{23}\frac{\partial s_2}{\partial w_k}-\lambda_{23}\frac{\partial s_3}{\partial w_k})</script><p>显然$\lambda<em>1=\lambda</em>{12}+\lambda<em>{13},\lambda_2=\lambda</em>{23}-\lambda<em>{12},\lambda_3=-\lambda</em>{13}-\lambda_{23}$,因此$\lambda_i$其实可以写为：</p>
<script type="math/tex; mode=display">\lambda_i=\sum_{j:(i,j)\in I}\lambda_{ij}-\sum_{k:(k,i)\in I}\lambda_{ki}</script><p><strong>$\lambda_i$决定着第i个doc在迭代中的移动方向和幅度，真实的排在$U_i$前面的doc越少，排在$U_i$后面的doc越多，那么文档$U_i$向前移动的幅度就越大(实际$\lambda_i$负的越多越向前移动)。这表明每个f下次调序的方向和强度取决于同一Query下可以与其组成relative relevance judgment的“pair对”的其他不同label的文档。</strong></p>
<blockquote>
<p>同时，这样的改造相当于是mini-batch learning。可以加速RankNet的学习过程。</p>
</blockquote>
<p>原先使用神经网络模型，通过Stochastic gradient descent计算的时候，是对每一个pair对都会进行一次权重的更新。而通过因式分解重新改造后，现在的mini-batch learning的方式，是对同一个query下的所有doc进行一次权重的更新。时间消耗从O(n2)降到了O(n)。这对训练过程的影响是很大的，因为使用的是神经网络模型，每次权重的更新迭代都需要先进行前向预测，再进行误差的后向反馈。</p>
<h3 id="2-2-Information-Retrieval的评价指标"><a href="#2-2-Information-Retrieval的评价指标" class="headerlink" title="2.2 Information Retrieval的评价指标"></a>2.2 Information Retrieval的评价指标</h3><p>Information Retrieval的评价指标包括：MRR，MAP，ERR，NDCG等。NDCG和ERR指标的优势在于，它们对doc的相关性划分多个（&gt;2）等级，而MRR和MAP只会对doc的相关性划分2个等级（相关和不相关）。并且，这些指标都包含了doc位置信息（给予靠前位置的doc以较高的权重），这很适合于web search。然而，这些指标的缺点是不平滑、不连续，无法求梯度，如果将这些指标直接作为模型评分的函数的话，是无法直接用梯度下降法进行求解的。</p>
<p>这里简单介绍下ERR（Expected Reciprocal Rank）。ERR是受到cascade model的启发，即一个用户从上到下依次浏览doc，直至他找到一个满意的结果，ERR可以定义为：</p>
<script type="math/tex; mode=display">\sum_{r=1}^n \frac{1}{r}R_r \prod_{i=1}^{r-1}(1-R_i)</script><p>其中，$R_i$表示第i位的doc的相关性概率：</p>
<script type="math/tex; mode=display">R_i=\frac{2^{l_i}-1}{2^{l_m}}</script><p>其中，$l_m$表示相关性评分最高的一档。</p>
<h3 id="2-3-LambdaRank"><a href="#2-3-LambdaRank" class="headerlink" title="2.3 LambdaRank"></a>2.3 LambdaRank</h3><p>上面我们介绍了以错误pair最少为优化目标的RankNet算法，然而许多时候仅以错误pair数来评价排序的好坏是不够的，像NDCG或者ERR等评价指标就只关注top k个结果的排序，当我们采用RankNet算法时，往往无法以这些指标为优化目标进行迭代，所以RankNet的优化目标和IR评价指标之间还是存在gap的。以下图为例：</p>
<p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/ltr-5.png" alt="lambdarank.ipg"></p>
<p>如上图所示，每个线条表示文档，蓝色表示相关文档，灰色表示不相关文档，RankNet以pairwise error的方式计算cost，左图的cost为13，右图通过把第一个相关文档下调3个位置，第二个文档上条5个位置，将cost降为11，但是像NDCG或者ERR等评价指标只关注top k个结果的排序，在优化过程中下调前面相关文档的位置不是我们想要得到的结果。图 1右图左边黑色的箭头表示RankNet下一轮的调序方向和强度，但我们真正需要的是右边红色箭头代表的方向和强度，即更关注靠前位置的相关文档的排序位置的提升。LambdaRank正是基于这个思想演化而来，其中<strong>Lambda指的就是红色箭头，代表下一次迭代优化的方向和强度，也就是梯度。</strong></p>
<p><strong>LambdaRank是一个经验算法，它不是通过显示定义损失函数再求梯度的方式对排序问题进行求解，而是分析排序问题需要的梯度的物理意义，直接定义梯度，即Lambda梯度。</strong></p>
<p>LambdaRank在RankNet的加速算法形式($\lambda<em>{ij}=\frac{\partial C</em>{ij}}{\partial s<em>i} = \sigma(\frac{1}{2}(1-S</em>{ij})-\frac{1}{1+e^{\sigma (s<em>i-s_j)}}),S</em>{ij}=1$)的基础上引入评价指标Z（如NDCG、ERR等），把交换两个文档的位置引起的评价指标的变化$|\Delta_{NDCG}|$作为其中一个因子，实验表明对模型效果有显著的提升：</p>
<script type="math/tex; mode=display">\lambda_{ij}=\frac{\partial C(s_i-s_j)}{\partial s_i}=\frac{-\sigma}{1+e^{\sigma (s_i-s_j)}}|\Delta_{NDCG}|</script><p>损失函数的梯度代表了文档下一次迭代优化的方向和强度，由于引入了IR评价指标，Lambda梯度更关注位置靠前的优质文档的排序位置的提升。有效的避免了下调位置靠前优质文档的位置这种情况的发生。LambdaRank相比RankNet的优势在于分解因式后训练速度变快，同时考虑了评价指标，直接对问题求解，效果更明显。</p>
<h3 id="2-4-LambdaMart"><a href="#2-4-LambdaMart" class="headerlink" title="2.4 LambdaMart"></a>2.4 LambdaMart</h3><ol>
<li>Mart定义了一个框架，缺少一个梯度。</li>
<li>LambdaRank重新定义了梯度，赋予了梯度新的物理意义。</li>
</ol>
<p>因此，所有可以使用梯度下降法求解的模型都可以使用这个梯度，MART就是其中一种，将梯度Lambda和MART结合就是大名鼎鼎的LambdaMART。</p>
<p>MART的原理是直接在函数空间对函数进行求解，模型结果由许多棵树组成，每棵树的拟合目标是损失函数的梯度，在LambdaMART中就是Lambda。LambdaMART的具体算法过程如下：</p>
<p><img src="http://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/ltr-6.png" alt="lambdamart.jpg"></p>
<p><strong>可以看出LambdaMART的框架其实就是MART，主要的创新在于中间计算的梯度使用的是Lambda，是pairwise的。MART需要设置的参数包括：树的数量M、叶子节点数L和学习率v，这3个参数可以通过验证集调节获取最优参数。</strong></p>
<p>MART支持“热启动”，即可以在已经训练好的模型基础上继续训练，在刚开始的时候通过初始化加载进来即可。</p>
<p><strong>下面简单介绍LambdaMART每一步的工作：</strong><br>1)  每棵树的训练会先遍历所有的训练数据（label不同的文档pair），计算每个pair互换位置导致的指标变化$|\Delta Z<em>{ij}|$以及Lambda，即$\lambda</em>{ij}=-\frac{1}{1+e^{s<em>i-s_j}}|\Delta Z</em>{ij}|$ ，然后计算每个文档的Lambda： $\lambda<em>i=\sum</em>{j:(i,j)\in I}\lambda<em>{ij}-\sum</em>{k:(k,i)\in I}\lambda_{ki}$，再计算每个$\lambda_i$ 的导数$w_i$，用于后面的Newton step求解叶子节点的数值。</p>
<p>2)  创建回归树拟合第一步生成的$\lambda_i$，划分树节点的标准是Mean Square Error，生成一颗叶子节点数为L的回归树。</p>
<p>3)  对第二步生成的回归树，计算每个叶子节点的数值，采用Newton step求解，即对落入该叶子节点的文档集，用公式$\frac{\sum<em>{x_i \in R</em>{lm}}y<em>i}{\sum</em>{x<em>i \in R</em>{lm}}w_i}$计算该叶子节点的输出值。</p>
<p>4)  更新模型，将当前学习到的回归树加入到已有的模型中，用学习率v（也叫shrinkage系数）做regularization。</p>
<p><strong>LambdaMART具有很多优势：</strong><br>1)  适用于排序场景：不是传统的通过分类或者回归的方法求解排序问题，而是直接求解</p>
<p>2)  损失函数可导：通过损失函数的转换，将类似于NDCG这种无法求导的IR评价指标转换成可以求导的函数，并且赋予了梯度的实际物理意义，数学解释非常漂亮</p>
<p>3)  增量学习：由于每次训练可以在已有的模型上继续训练，因此适合于增量学习</p>
<p>4)  组合特征：因为采用树模型，因此可以学到不同特征组合情况</p>
<p>5)  特征选择：因为是基于MART模型，因此也具有MART的优势，可以学到每个特征的重要性，可以做特征选择</p>
<p>6)  适用于正负样本比例失衡的数据：因为模型的训练对象具有不同label的文档pair，而不是预测每个文档的label，因此对正负样本比例失衡不敏感</p>
<p><a href="https://www.cnblogs.com/bentuwuying/p/6681943.html">参考博文：Learning to Rank简介</a><br><a href="https://www.cnblogs.com/bentuwuying/p/6690836.html">参考博文：Learning to Rank算法介绍：RankNet，LambdaRank，LambdaMart</a></p>
<hr>

      
    </div>
    
    
    
    
    <div>
      
      <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
      
    </div>

    <div>
      
        
<div class="my_post_copyright">
  <script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>
  
  <!-- JS库 sweetalert 可修改路径 -->
  <script src="https://cdn.bootcss.com/jquery/2.0.0/jquery.min.js"></script>
  <script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script>
  <p><span>本文标题:</span><a href="/posts/IntroductionofLTR.html">LTR(Learning to Rank)概述</a></p>
  <p><span>文章作者:</span><a href="/" title="访问  的个人博客"></a></p>
  <p><span>原始链接:</span><a href="/posts/IntroductionofLTR.html" title="LTR(Learning to Rank)概述">https://www.xiemingzhao.com/posts/IntroductionofLTR.html</a>
    <span class="copy-path"  title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="https://www.xiemingzhao.com/posts/IntroductionofLTR.html"  aria-label="复制成功！"></i></span>
  </p>
  <p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">署名-非商业性使用-禁止演绎 4.0 国际</a> 转载请保留原文链接及作者。</p>  
</div>
<script> 
    var clipboard = new Clipboard('.fa-clipboard');
      $(".fa-clipboard").click(function(){
      clipboard.on('success', function(){
        swal({   
          title: "",   
          text: '复制成功',
          icon: "success", 
          showConfirmButton: true
          });
        });
    });  
</script>

      
    </div>

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 机器学习</a>
          
            <a href="/tags/%E6%8E%92%E5%BA%8F/" rel="tag"><i class="fa fa-tag"></i> 排序</a>
          
            <a href="/tags/LTR/" rel="tag"><i class="fa fa-tag"></i> LTR</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/posts/ltrindices.html" rel="next" title="LTR信息检索评价指标">
                <i class="fa fa-chevron-left"></i> LTR信息检索评价指标
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/posts/deepfmpaper.html" rel="prev" title="DeepFM A Factorization-Machine based Neural Network for CTR Prediction (论文解析)">
                DeepFM A Factorization-Machine based Neural Network for CTR Prediction (论文解析) <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
        
  <div class="bdsharebuttonbox">
    <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
    <a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a>
    <a href="#" class="bds_sqq" data-cmd="sqq" title="分享到QQ好友"></a>
    <a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a>
    <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a>
    <a href="#" class="bds_tieba" data-cmd="tieba" title="分享到百度贴吧"></a>
    <a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a>
    <a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a>
    <a href="#" class="bds_more" data-cmd="more"></a>
    <a class="bds_count" data-cmd="count"></a>
  </div>
  <script>
    window._bd_share_config = {
      "common": {
        "bdText": "",
        "bdMini": "2",
        "bdMiniList": false,
        "bdPic": ""
      },
      "share": {
        "bdSize": "16",
        "bdStyle": "0"
      },
      "image": {
        "viewList": ["tsina", "douban", "sqq", "qzone", "weixin", "twi", "fbook"],
        "viewText": "分享到：",
        "viewSize": "16"
      }
    }
  </script>

<script>
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script>

      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="https://i.postimg.cc/vBxZQfvz/img-0182.jpg"
                alt="" />
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%7C%7C%20archive">
              
                  <span class="site-state-item-count">61</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">64</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/xiemingzhao" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-globe"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="514829265@qq.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-globe"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Learning-to-Rank-%E7%AE%80%E4%BB%8B"><span class="nav-number">1.</span> <span class="nav-text">1 Learning to Rank 简介</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-%E6%8E%92%E5%BA%8F%E9%97%AE%E9%A2%98"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 排序问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-1-Training-data%E7%9A%84%E7%94%9F%E6%88%90"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.1.1 Training data的生成</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-2-Feature%E7%9A%84%E7%94%9F%E6%88%90"><span class="nav-number">1.1.2.</span> <span class="nav-text">1.1.2 Feature的生成</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-3-Evaluation"><span class="nav-number">1.1.3.</span> <span class="nav-text">1.1.3 Evaluation</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-Formulation"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 Formulation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-Learning-to-Rank-Methods"><span class="nav-number">1.3.</span> <span class="nav-text">1.3 Learning to Rank Methods</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-RankNet%EF%BC%8CLambdaRank%EF%BC%8CLambdaMart%E7%AE%80%E4%BB%8B"><span class="nav-number">2.</span> <span class="nav-text">2 RankNet，LambdaRank，LambdaMart简介</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-RankNet"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 RankNet</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-1-%E7%9B%B8%E5%85%B3%E6%80%A7%E6%A6%82%E7%8E%87"><span class="nav-number">2.1.1.</span> <span class="nav-text">2.1.1 相关性概率</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-2-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">2.1.2.</span> <span class="nav-text">2.1.2 损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-3-%E5%90%88%E5%B9%B6%E6%A6%82%E7%8E%87"><span class="nav-number">2.1.3.</span> <span class="nav-text">2.1.3 合并概率</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-4-Gradient-Descent"><span class="nav-number">2.1.4.</span> <span class="nav-text">2.1.4 Gradient Descent</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-5-%E5%8A%A0%E9%80%9FRankNet%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="nav-number">2.1.5.</span> <span class="nav-text">2.1.5 加速RankNet训练过程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-Information-Retrieval%E7%9A%84%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 Information Retrieval的评价指标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-LambdaRank"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 LambdaRank</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-LambdaMart"><span class="nav-number">2.4.</span> <span class="nav-text">2.4 LambdaMart</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 &mdash; <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">小火箭</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">信仰</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>


<!--

  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>

-->



<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共271.8k字</span>
</div>

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<!-- 网站运行时间的设置 -->
<!--
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>  Sometimes your whole life boils down to one insame move.
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/09/2019 13:14:21");//此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);
</script>
-->
        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访客数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i> 总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'TpMiFGGr4T8FnG248uRRaf4H-gzGzoHsz',
        appKey: 'NYRTcUPshFEJWpUk54Bfu4nX',
        placeholder: '朋友记得留下昵称和邮箱，我会尽快回复您的！',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  


  

  <script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":true,"log":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true}});</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>

<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/love.js"></script>
