<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">


  <link rel="manifest" href="/images/manifest.json">


  <meta name="msapplication-config" content="/images/browserconfig.xml" />



  <meta name="keywords" content="算法,word2vec," />





  <link rel="alternate" href="/atom.xml" title="小火箭的博客" type="application/atom+xml" />






<meta name="description" content="1 引言很多算法工程师认为 Embedding 技术是机器学习中最迷人的一种思想，在过去的近10年中，该技术在各个深度学习领域大放异彩。已经逐步进化到了近几年基于 BERT 和 GPT2 等模型的语境化嵌入。本文重点基于原始论文Efficient Estimation of Word Representations in Vector Space，整理 word2vec 相关技术的基础原理和应用经">
<meta property="og:type" content="article">
<meta property="og:title" content="word2vec 详解">
<meta property="og:url" content="https://www.xiemingzhao.com/posts/word2vec.html">
<meta property="og:site_name" content="小火箭的博客">
<meta property="og:description" content="1 引言很多算法工程师认为 Embedding 技术是机器学习中最迷人的一种思想，在过去的近10年中，该技术在各个深度学习领域大放异彩。已经逐步进化到了近几年基于 BERT 和 GPT2 等模型的语境化嵌入。本文重点基于原始论文Efficient Estimation of Word Representations in Vector Space，整理 word2vec 相关技术的基础原理和应用经">
<meta property="og:locale">
<meta property="og:image" content="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/word2vec0.png">
<meta property="og:image" content="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/word2vec1.png">
<meta property="og:image" content="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/word2vec2.jpg">
<meta property="og:image" content="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/word2vec3.png">
<meta property="og:image" content="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/word2vec4.png">
<meta property="og:image" content="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/word2vec5.png">
<meta property="article:published_time" content="2020-12-12T16:00:00.000Z">
<meta property="article:modified_time" content="2025-04-01T17:01:23.401Z">
<meta property="article:author" content="小火箭">
<meta property="article:tag" content="算法">
<meta property="article:tag" content="word2vec">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/word2vec0.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://www.xiemingzhao.com/posts/word2vec.html"/>





  <title>word2vec 详解 | 小火箭的博客</title>
  








<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>


<script type="text/javascript"
color="0,0,0" opacity='0.5' zIndex="-1" count="150" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>


<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
    
    <a href="https://github.com/xiemingzhao"><img style="position:absolute;top:0;right:0;border:0;" src="https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png?resize=149%2C149" class="attachment-full size-full" alt="Fork me on GitHub" data-recalc-dims="1"></a>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">小火箭的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">愿世界和平！！！</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            公益404
          </a>
        </li>
      
        
        <li class="menu-item menu-item-guestbook">
          <a href="/guestbook/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-comments"></i> <br />
            
            留言板
          </a>
        </li>
      
        
        <li class="menu-item menu-item-others">
          <a href="/others/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-folder"></i> <br />
            
            其他
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.xiemingzhao.com/posts/word2vec.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.postimg.cc/vBxZQfvz/img-0182.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小火箭的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">word2vec 详解</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-12-13T00:00:00+08:00">
                2020-12-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index">
                    <span itemprop="name">学习笔记</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/" itemprop="url" rel="index">
                    <span itemprop="name">算法总结</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/posts/word2vec.html#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/posts/word2vec.html" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i> 阅读数
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>次
            </span>
          

          
            <div class="post-wordcount">
              
                
                  <span class="post-meta-divider">|</span>
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  3.8k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  14
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>很多算法工程师认为 Embedding 技术是机器学习中最迷人的一种思想，在过去的近10年中，该技术在各个深度学习领域大放异彩。已经逐步进化到了近几年基于 BERT 和 GPT2 等模型的语境化嵌入。本文重点基于原始论文<a href="https://arxiv.org/pdf/1301.3781v3.pdf">Efficient Estimation of Word Representations in Vector Space</a>，整理 word2vec 相关技术的基础原理和应用经验，旨在利于自己回溯巩固和他人参考学习。</p>
<p>首先 Embedding 的思想是如何来的呢？我们知道计算机底层只能识别数字，并基于其进行逻辑等计算。而世间大多的实体或概念都不是以数据形式存在的，如何让计算机能够记住甚至理解是一件很难的事情。</p>
<p>如果我们能够将实体或概念以一种有意义的代数向量的形式输入给计算机，那么计算机对于它们的存储、理解和计算将会极大的友好。比如，对于一个人，如果我们重点关注他的性别、年龄、身高、体重、胸围、存款这些信息，那么我们可以将其记为以下形式：</p>
<p>[1,18,180,70,90,100]</p>
<p>其中每个维度的数值对应该维度的信息，也即性别=男（1）、年龄=18、身高=180cm、体重=70kg、胸围=90cm、存款=100W。当然你可以继续扩增更多的维度，维度信息越多，计算机对这个对象认识的更全面。</p>
<h2 id="2-Word-Embedding"><a href="#2-Word-Embedding" class="headerlink" title="2 Word Embedding"></a>2 Word Embedding</h2><p>在 NLP 领域，计算对于词的理解一直是一个很重要的问题。如前文所述，Word Embedding <code>目的</code>就是<strong>把词汇表中的单词或者短语（words or phrases）映射成由实数构成的向量</strong>上，而其<code>方法</code>一般是<strong>从数据中自动学习输入空间到 Distributed representation 空间的映射 f</strong>。</p>
<h3 id="2-1-One-hot"><a href="#2-1-One-hot" class="headerlink" title="2.1 One-hot"></a>2.1 One-hot</h3><p><code>One-hot</code> 编码又称<code>独热编码</code>，具体方法是：用一个N位状态寄存器来对N个状态进行编码，N是指所编码特征的空间大小。例如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gender:[&quot;male&quot;, &quot;female&quot;]</span><br><span class="line">country:[&quot;US&quot;, &quot;China&quot;,&quot;Japan&quot;,&quot;France&quot;,&quot;Italy&quot;]</span><br></pre></td></tr></table></figure>
<p>这两个特征的每一个取值可以被 One-hot 编码为：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gender = [[1,0], [0,1]]</span><br><span class="line">country = [[1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1]]</span><br></pre></td></tr></table></figure></p>
<p>如此，One-hot 编码的优缺点还是很明显的。</p>
<ul>
<li>优点：解决了分类器不好处理离散数据的问题，在一定程度上也起到了扩充特征的作用。</li>
<li>缺点：1. 不考虑词的顺序；2. 假设词之间相互独立；3. 向量是高度稀疏的；4. 容易出现维度爆炸。</li>
</ul>
<h3 id="2-2-Dristributed-representation"><a href="#2-2-Dristributed-representation" class="headerlink" title="2.2 Dristributed representation"></a>2.2 Dristributed representation</h3><p>根据One-hot的缺点，我们更希望用诸如“语义”，“复数”，“时态”等维度去描述一个单词。每一个维度不再是0或1，而是连续的实数，表示不同的程度。</p>
<p>Dristributed representation 可以解决 One hot representation 的问题，它的<strong>思路是通过训练，将每个词都映射到一个较短的稠密词向量上来。</strong></p>
<p>例如，king 这个词可能从一个非常稀疏的空间映射到一个稠密的四维空间，假设[0.99,0.99,0.05,0.7]。那这个映射一般要满足：</p>
<ul>
<li>这个映射是一一映射；</li>
<li>映射后的向量不会丢失之前所包含的信息。</li>
</ul>
<p>这个过程就成为 <code>Word Embedding</code> （词嵌入），而一个好的词嵌入一般能够获得有意义的词向量，例如一个经典的case，即可以从词向量上发现:</p>
<script type="math/tex; mode=display">\vec King - \vec Man + \vec Womman = \vec Queen</script><h3 id="2-3-Cocurrence-matrix"><a href="#2-3-Cocurrence-matrix" class="headerlink" title="2.3 Cocurrence matrix"></a>2.3 Cocurrence matrix</h3><p><strong>一般认为某个词的意思跟它临近的单词是紧密相关的</strong>。这时可以设定一个窗口（大小一般是5~10），如下窗口大小是2，那么在这个窗口内，与 rests 共同出现的单词就有 life、he、in、peace。然后我们就利用这种共现关系来生成词向量。</p>
<blockquote>
<p>… Bereft of life he rests in peace! If you hadn’t nailed him …</p>
</blockquote>
<p>假设窗口大小为1，此时，将得到一个对称矩阵——<code>共现矩阵</code>，如此就可以实现将 word 变成向量的设想，在共现矩阵每一行（或每一列）都是对应单词的一个向量表示。如下所示：</p>
<p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/word2vec0.png" alt="word2vec"></p>
<h2 id="3-word2vec"><a href="#3-word2vec" class="headerlink" title="3. word2vec"></a>3. word2vec</h2><h3 id="3-1-基本模型结构"><a href="#3-1-基本模型结构" class="headerlink" title="3.1 基本模型结构"></a>3.1 基本模型结构</h3><p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/word2vec1.png" alt="word2vec1"></p>
<p>如上图所示，是 word2vec 的基本模型结构，其目的是：</p>
<p><strong>利用单词本身的 one-hot 编码来完成的预测（即不同场景下的context），然后利用训练过程中映射矩阵中对应的向量来作为单词的表示。</strong></p>
<p>简述一下上图的流程：</p>
<ol>
<li>输入是 One-hot 编码，通过与映射矩阵$W_{V \times N}$得到隐层的行向量；</li>
<li>从隐层到输出层，有另一个映射矩阵$W’_{N \times V}$，与前面的行向量相乘得到输出向量；</li>
<li>之后经过 softmax 层，便得到每个词的概率。</li>
</ol>
<p>整个过程用数学来表达就是：</p>
<script type="math/tex; mode=display">u_j = W'W^T x = { w'_{ij} } = {v'}_{w_j}^{T} {v}_{w_i}^T</script><script type="math/tex; mode=display">p{w_j | w_i} = y_i = \frac{exp(u_j)}{\sum_{ {j}'=1}^V exp(u'_j)} = \frac{exp({v'}_{w_j}^T v_{w_i})}{\sum_{ {j}'=1}^V exp({v'}_{w_{j'} }^T  v_{w_i}) )}</script><p>其中 $u<em>i$ 代表了输出向量中第 i 个单词的概率， $v</em>{w<em>i}$ 和 ${v’}</em>{w_{j’ } }^T$ 分别代表了 $W$ 中对应的行向量和 $W’$ 中对应的列向量。</p>
<h3 id="3-2-CBOW-Continuous-Bags-of-word"><a href="#3-2-CBOW-Continuous-Bags-of-word" class="headerlink" title="3.2 CBOW(Continuous Bags-of-word)"></a>3.2 CBOW(Continuous Bags-of-word)</h3><p>基于上述，我们来看一个经典的模型结构，<code>CBOW</code>，即连续词袋模型。与基准模型结构不同的是，<strong>CBOW 模型利用输入 context 多个词的向量均值作为输入</strong>。</p>
<p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/word2vec2.jpg" alt="word2vec2"></p>
<p>如上图所示，用数学描述即：</p>
<script type="math/tex; mode=display">h = \frac{1}{C} W^T (x_1 ++ x_2 + \dots + x_C) = \frac{1}{C}(v_{w_1} + v_{w_2} + \dots + v_{w_C})</script><p>其中，C 为 context 的词语数量，所以CBOW的损失函数为：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
E & = & -log p(w_O|w_{I,1}, \dots , w_{I,C}) \\
& = & - u_{j^*} + log \sum_{ {j}' = 1} ^ V exp(u_{ {j}'}) \\
& = & - v'_{w_O} \cdot h + log \sum_{ {j}' = 1} ^ V exp({v'}_{w_j}^T) \cdot h
\end{array}</script><h3 id="3-3-Skip-Gram-Model"><a href="#3-3-Skip-Gram-Model" class="headerlink" title="3.3 Skip-Gram Model"></a>3.3 Skip-Gram Model</h3><blockquote>
<p><code>核心区别</code>：Skip-gram 是预测一个词的上下文，而 CBOW 是用上下文预测这个词，即 y 有多个词。</p>
</blockquote>
<p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/word2vec3.png" alt="word2vec3"></p>
<p>在传递过程中，其不再输出单个的多项分布（即一个词语的 one-hot 编码），而是利用共享的参数输出映射矩阵输出 C 个多项分布（此处  C 为context词语的数量）：</p>
<script type="math/tex; mode=display">p(w_{c,j} = w_{O,c}|w_I) = y_{c,j} = \frac{exp(u_{c,j})}{\sum_{ {j'} - 1}^V exp(u_{j'})}</script><p>其中：</p>
<ul>
<li>$w_{c,j}$是输出层第 c 部分中的第 j 个数字；</li>
<li>$w_{O,c}$是输出 context 词中第 c 个数字；</li>
<li>$w_I$ 是输入的唯一单词；</li>
<li>$y_{c,j}$ 是输出层第 c 部分中的第 j 个单元；</li>
<li>$u_{c,j}$是输出层第 c 部分上第 j 个单元的净输入。</li>
</ul>
<p>由于输出时映射矩阵的参数共享，所以有：</p>
<script type="math/tex; mode=display">u_{c,j} = u_j = {v'}_{w_j}^T \cdot h, \quad for  c = 1, 2, \dots , C</script><p>在Skip-Gram中的 loss function 为：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
E & = & -log p(w_{O,1}, w_{O,2}, \dots , w_{O,C} | w_I) \\
& = & - log \prod_{c = 1} ^C \frac{exp(u_{j'})}{\sum_{ {j'}-1}^V exp(u_{j'})} \\
& = & -\sum_{c = 1}^C u_{j_c^*} + C \cdot log \sum_{ {j'}-1}^V exp(u_{j'})
\end{array}</script><h2 id="4-输出层-softmax-优化"><a href="#4-输出层-softmax-优化" class="headerlink" title="4 输出层 softmax 优化"></a>4 输出层 softmax 优化</h2><p>我们回顾 <code>word2vec</code> 算法，容易发现其在输出层为了预估词的概率，需要经过 sotmax 层。而，当词典空间很大的时候，<strong>softmax层容易成为整个算法的计算瓶颈</strong>。一般会有两种方法可以解决，<code>Hierarchical SoftMax</code> 和 <code>Negative Sampling</code>。</p>
<h3 id="4-1-Hierarchical-SoftMax"><a href="#4-1-Hierarchical-SoftMax" class="headerlink" title="4.1 Hierarchical SoftMax"></a>4.1 Hierarchical SoftMax</h3><p><code>哈夫曼（Huffman）树</code>是一种二叉树数据结构，基于其衍生的 <code>Hierarchical SoftMax</code> 能够有效地的降低 Softmax 的计算复杂度。我们首先介绍一下如何构建一颗哈夫曼（Huffman）树。</p>
<p>假设待构建的 n 个权值（一般是词频）为 ${w_1, w_2, \dots , w_n}$，可以通过以下步骤来构建 Huffman 树：</p>
<ol>
<li>将 ${w_1, w_2, \dots , w_n}$作为森林中 n 棵树的根节点；</li>
<li>选取森林中根权值最小的2棵树，分别作为左右子树合成新树，且新根节点的权值为左右子树根节点权值之和；</li>
<li>用新合成的数替换森林中原来的2个子树，重复上述过程直至仅剩一棵树。</li>
</ol>
<p>假设，有下面的一句：</p>
<blockquote>
<p>I love data science. I love big data. I am who I am.</p>
</blockquote>
<p>对应的词频表为：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>word</th>
<th>code</th>
<th>freq</th>
<th>bits</th>
</tr>
</thead>
<tbody>
<tr>
<td>I</td>
<td>10</td>
<td>4</td>
<td>8</td>
</tr>
<tr>
<td>love</td>
<td>110</td>
<td>2</td>
<td>6</td>
</tr>
<tr>
<td>data</td>
<td>010</td>
<td>2</td>
<td>6</td>
</tr>
<tr>
<td>science</td>
<td>11110</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>big</td>
<td>11111</td>
<td>1</td>
<td>5</td>
</tr>
<tr>
<td>am</td>
<td>011</td>
<td>2</td>
<td>6</td>
</tr>
<tr>
<td>who</td>
<td>1110</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>.</td>
<td>00</td>
<td>3</td>
<td>6</td>
</tr>
</tbody>
</table>
</div>
<p>根据构建流程，实际上经过7次合并后完成 Huffman 树的构建：</p>
<ol>
<li>T1: (science, big) = 2</li>
<li>T2: (who, T1) = 3</li>
<li>T3: (data, am) = 4</li>
<li>T4: (love, T2) = 5</li>
<li>T5: (., T3) = 7</li>
<li>T6: (I, T4) = 9</li>
<li>T7: (T5, T6) = 15</li>
</ol>
<p>按照上述步骤构建完成的<code>Huffman树</code>一般如下图所示：</p>
<p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/word2vec4.png" alt="word2vec4"></p>
<p>可以发现：<strong>每个词都为树中的叶子节点，即高频词计算路径短，低频词计算路劲长</strong></p>
<p>剩下的步骤：</p>
<ol>
<li>将每个节点的选择构建成一个简单的二分类问题（比如LR），这样每个词语输入节点时都面临一个二项分布的问题。</li>
<li>通过Skip-Gram中“一预测多”的思想，利用根节点的输入词来预测多个输出的叶子节点，这样每个节点输出的概率为对应路径中的多项分布概率相乘。</li>
<li>最后遍历词典中的所有输入，完成整个数的节点参数确定，最后利用每个节点路径上的概率来形成对应单词的隐向量。</li>
</ol>
<p>由于在构建 Huffman 树的时候，保证了数的深度为 $log{|V|}$ ，因此在分层Softmax中只需做 $log{|V|}$ 次二分类即可求得最后预测单词的概率大小。</p>
<h3 id="4-2-Negative-Sampling"><a href="#4-2-Negative-Sampling" class="headerlink" title="4.2 Negative Sampling"></a>4.2 Negative Sampling</h3><p>除了上述的<code>分层Softmax</code>方法，另一个更为经典和常用的便是<code>Negative Sampling</code>（负采样）方法。它的核心思想是：</p>
<blockquote>
<p>放弃全局 Softmax 计算的过程，按照固定概率采样一定量的子集作为负例，从而转化成计算这些负例的sigmoid二分类过程，可以大大降低计算复杂度。</p>
</blockquote>
<script type="math/tex; mode=display">E = -\log{\sigma{(v_{w_o}^T h) } } - \sum_{w_j \in W_{neg } } \log{\sigma{(-v_{w_j}^T h) } }</script><p>上述便是新的 Loss 函数，公式中前者是 input 词，后部分为负采样得到的负样本词。容易发现，网络的计算空间从$|V|$降低到了$|w<em>O \cup W</em>{neg}|$。<strong>而这本质上是对训练集进行了采样，从而减小了训练集的大小。</strong></p>
<h2 id="5-问题思考"><a href="#5-问题思考" class="headerlink" title="5 问题思考"></a>5 问题思考</h2><h3 id="5-1-负采样方式"><a href="#5-1-负采样方式" class="headerlink" title="5.1 负采样方式"></a>5.1 负采样方式</h3><blockquote>
<p>算法的采样要求：高频词被采到的概率要大于低频词。<br>所以答案是非均匀采样，而是<code>带权采样</code>。</p>
</blockquote>
<p>之所以如此，是因为在大语料数据集中，有很多高频但信息量少的词，例如”the, a”等。对它们的下采样不仅可以加速还可以提高词向量的质量。为了平衡高低频词，一般采用如下权重：</p>
<script type="math/tex; mode=display">P(w_i) = 1 - \sqrt{\frac{t}{f(w_i) } }</script><p>其中，$f(w_i)$是单词$w_i$出现频率，参数$t$根据经验值一般取$10^{-5}$。如此可以确保频率超过$t$的词可以被欠采样，且不会影响原单词的频率相对大小。</p>
<h3 id="5-2-模型中两个embedding的取舍"><a href="#5-2-模型中两个embedding的取舍" class="headerlink" title="5.2 模型中两个embedding的取舍"></a>5.2 模型中两个embedding的取舍</h3><p>在word2vec模型的训练阶段，一般创建2个词表矩阵，<strong>Embedding 矩阵和 Context 矩阵</strong>。它们的大小都是 vocab_size x embedding_size，其中 vocab_size 是词表大小，embedding_size 是词向量维度。如下图所示：</p>
<p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/word2vec5.png" alt="word2vec5"></p>
<blockquote>
<p>训练结束后，一般丢弃 Context 矩阵，并使用 Embeddings 矩阵作为下一项任务的已被训练好的嵌入。那为什么这么做呢？</p>
</blockquote>
<p>在 Stack Overflow 上的问题<a href="https://stackoverflow.com/questions/29381505/why-does-word2vec-use-2-representations-for-each-word">Why does word2vec use 2 representations for each word?</a>中有提到一个直觉性的解释，核心就是<strong>前者是中心词下的 embedding ，后者是 context 时下的 embedding ，他所表征的是两种不同的分布。</strong></p>
<p>实际上，个人在实际应用中做了一些试错，这里记录共享一下：</p>
<ol>
<li>使用中心词表 embedding 一定程度符合问题的定义：为了获取中心词 emb，从而采样 context 来构建样本训练的；</li>
<li>中心词和上下文词的 embedding 表都可以单独使用，但是不能交叉使用，比如2个近义词在两个独立词表中 emb 有意义，跨表 embedding 的关联性会失去；</li>
<li>从 <code>CBOW</code> 和 <code>Skip-gram</code> 的算法逻辑看，中心词和上下文词 embedding 实际上是一个角色互换。</li>
</ol>
<h3 id="5-3-CBOW-amp-Skip-Gram-的优劣"><a href="#5-3-CBOW-amp-Skip-Gram-的优劣" class="headerlink" title="5.3 CBOW &amp; Skip-Gram 的优劣"></a>5.3 CBOW &amp; Skip-Gram 的优劣</h3><p>先总结一下结论：</p>
<ul>
<li>当语料较少时使用 CBOW 方法比较好，当语料较多时采用 skip-gram 表示比较好。</li>
<li>Skip-gram 训练时间长，但是对低频词(生僻词)效果好；</li>
<li>CBOW 训练时间短，对低频词效果比较差。</li>
</ul>
<p>对于上述的结论貌似业界较为统一，但是对于这个结论的原理解释众说纷纭，这里个人觉得下面这种逻辑分析更为合理。首先注意 CBOW 和 skip-gram 的训练形式区别：</p>
<ul>
<li>CBOW 是使用周围词预测中心词，周围词的emb表是最终使用的。其对于每个中心词的一组采样样本训练的 gradient 会同时反馈给周围词上；</li>
<li>skip-gram 则相反，使用中心词预测周围词，中心词的 emb 表是最重使用的。那么中心词每组采样样本训练的 gradient 都会调整中心词的 emb；</li>
</ul>
<p>如上情况，可能会认为两者虽然中心词和上下文词虽然角色不一样，但只是互换了位置，训练的次数和结果理应差不多。然而，当默认使用 embedding 词表的时候，情况是不一样的:</p>
<blockquote>
<p><strong>skip-gram 的主词表中每个 emb 的训练次数多于 CBOW 的</strong>。</p>
</blockquote>
<p>因为，在 skip-gram 中，中心词的 emb 表是主词表，其每次会抽样 K 个上下文词，这保证了主词表对应的每个上下文词都训练到 K 次。<br>而 CBOW 则不同，因为其上下文词是主词表，中心词是用来训练上下文词的  emb，而由于采样概率的问题，虽然也会采样 K 个上下文词，但依然不能保证下文词对应的主词表的每个emb 都能够至少训练 K 次。</p>
<h3 id="5-4-拓展应用"><a href="#5-4-拓展应用" class="headerlink" title="5.4 拓展应用"></a>5.4 拓展应用</h3><p>个人在实际工作和应用中，深刻的感受到 word2vec 的强大绝不止于预训练词向量这么简单，其算法原理的思想才是核心，可以应用在很多地方，也深深影响着我自己。</p>
<p>这里总结几个应用的场景：</p>
<ol>
<li>最传统的便是nlp中文本词向量的预训练；</li>
<li>搜索推荐场景做 item2vec，可以用来输入精排或直接做向量召回；</li>
<li>召回/粗排模型样本的采样逻辑，如样本的负采样完全可以参考此逻辑。</li>
</ol>
<p><strong>参考文献</strong><br><a href="https://blog.csdn.net/weixin_39910711/article/details/103696103">机器学习算法（十三）：word2vec</a><br><a href="https://zhuanlan.zhihu.com/p/447776752">Embedding知识点 —— word2Vec详解</a><br><a href="https://blog.csdn.net/weixin_42279926/article/details/106403211">word2vec对each word使用两个embedding的原因</a><br><a href="https://mp.weixin.qq.com/s/oIxCPNXEUEvnjC0ESNQvCg">图解Word2vec</a><br><a href="https://zhuanlan.zhihu.com/p/26306795">NLP秒懂词向量Word2vec的本质</a><br><a href="https://blog.csdn.net/lanyu_01/article/details/80097350">《word2vec Parameter Learning Explained》论文学习笔记</a></p>
<hr>

      
    </div>
    
    
    
    
    <div>
      
      <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
      
    </div>

    <div>
      
        
<div class="my_post_copyright">
  <script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>
  
  <!-- JS库 sweetalert 可修改路径 -->
  <script src="https://cdn.bootcss.com/jquery/2.0.0/jquery.min.js"></script>
  <script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script>
  <p><span>本文标题:</span><a href="/posts/word2vec.html">word2vec 详解</a></p>
  <p><span>文章作者:</span><a href="/" title="访问  的个人博客"></a></p>
  <p><span>原始链接:</span><a href="/posts/word2vec.html" title="word2vec 详解">https://www.xiemingzhao.com/posts/word2vec.html</a>
    <span class="copy-path"  title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="https://www.xiemingzhao.com/posts/word2vec.html"  aria-label="复制成功！"></i></span>
  </p>
  <p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">署名-非商业性使用-禁止演绎 4.0 国际</a> 转载请保留原文链接及作者。</p>  
</div>
<script> 
    var clipboard = new Clipboard('.fa-clipboard');
      $(".fa-clipboard").click(function(){
      clipboard.on('success', function(){
        swal({   
          title: "",   
          text: '复制成功',
          icon: "success", 
          showConfirmButton: true
          });
        });
    });  
</script>

      
    </div>

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/%E7%AE%97%E6%B3%95/" rel="tag"><i class="fa fa-tag"></i> 算法</a>
          
            <a href="/tags/word2vec/" rel="tag"><i class="fa fa-tag"></i> word2vec</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/posts/activefunc.html" rel="next" title="深度学习中的激活函数们">
                <i class="fa fa-chevron-left"></i> 深度学习中的激活函数们
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/posts/senalgo.html" rel="prev" title="SNE 和 t-SNE 算法">
                SNE 和 t-SNE 算法 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
        
  <div class="bdsharebuttonbox">
    <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
    <a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a>
    <a href="#" class="bds_sqq" data-cmd="sqq" title="分享到QQ好友"></a>
    <a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a>
    <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a>
    <a href="#" class="bds_tieba" data-cmd="tieba" title="分享到百度贴吧"></a>
    <a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a>
    <a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a>
    <a href="#" class="bds_more" data-cmd="more"></a>
    <a class="bds_count" data-cmd="count"></a>
  </div>
  <script>
    window._bd_share_config = {
      "common": {
        "bdText": "",
        "bdMini": "2",
        "bdMiniList": false,
        "bdPic": ""
      },
      "share": {
        "bdSize": "16",
        "bdStyle": "0"
      },
      "image": {
        "viewList": ["tsina", "douban", "sqq", "qzone", "weixin", "twi", "fbook"],
        "viewText": "分享到：",
        "viewSize": "16"
      }
    }
  </script>

<script>
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script>

      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="https://i.postimg.cc/vBxZQfvz/img-0182.jpg"
                alt="" />
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%7C%7C%20archive">
              
                  <span class="site-state-item-count">49</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">46</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/xiemingzhao" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-globe"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="514829265@qq.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-globe"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E5%BC%95%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">1 引言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Word-Embedding"><span class="nav-number">2.</span> <span class="nav-text">2 Word Embedding</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-One-hot"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 One-hot</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-Dristributed-representation"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 Dristributed representation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-Cocurrence-matrix"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 Cocurrence matrix</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-word2vec"><span class="nav-number">3.</span> <span class="nav-text">3. word2vec</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 基本模型结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-CBOW-Continuous-Bags-of-word"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 CBOW(Continuous Bags-of-word)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-Skip-Gram-Model"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 Skip-Gram Model</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E8%BE%93%E5%87%BA%E5%B1%82-softmax-%E4%BC%98%E5%8C%96"><span class="nav-number">4.</span> <span class="nav-text">4 输出层 softmax 优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-Hierarchical-SoftMax"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 Hierarchical SoftMax</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-Negative-Sampling"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 Negative Sampling</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E9%97%AE%E9%A2%98%E6%80%9D%E8%80%83"><span class="nav-number">5.</span> <span class="nav-text">5 问题思考</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-%E8%B4%9F%E9%87%87%E6%A0%B7%E6%96%B9%E5%BC%8F"><span class="nav-number">5.1.</span> <span class="nav-text">5.1 负采样方式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-%E6%A8%A1%E5%9E%8B%E4%B8%AD%E4%B8%A4%E4%B8%AAembedding%E7%9A%84%E5%8F%96%E8%88%8D"><span class="nav-number">5.2.</span> <span class="nav-text">5.2 模型中两个embedding的取舍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-CBOW-amp-Skip-Gram-%E7%9A%84%E4%BC%98%E5%8A%A3"><span class="nav-number">5.3.</span> <span class="nav-text">5.3 CBOW &amp; Skip-Gram 的优劣</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-%E6%8B%93%E5%B1%95%E5%BA%94%E7%94%A8"><span class="nav-number">5.4.</span> <span class="nav-text">5.4 拓展应用</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 &mdash; <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">小火箭</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">信仰</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>


<!--

  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>

-->



<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共236.8k字</span>
</div>

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<!-- 网站运行时间的设置 -->
<!--
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>  Sometimes your whole life boils down to one insame move.
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/09/2019 13:14:21");//此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);
</script>
-->
        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访客数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i> 总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'TpMiFGGr4T8FnG248uRRaf4H-gzGzoHsz',
        appKey: 'NYRTcUPshFEJWpUk54Bfu4nX',
        placeholder: '朋友记得留下昵称和邮箱，我会尽快回复您的！',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  


  

  <script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":true,"log":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true}});</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>

<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/love.js"></script>
