---
title: 深入浅出计算机组成原理——原理篇：处理器（30-34）
categories:
    - 学习笔记
    - 计算机组成原理

tags:
    - 计算机原理
    - 处理器

mathjax: true
copyright: true
abbrlink: computerOrgArc30to34
date: 2021-08-03

---

> 全文内容主要来自对课程[《深入浅出计算机组成原理》](https://time.geekbang.org/column/article/91427)的学习笔记。

## 30 | GPU（上）
### GPU 的历史进程
GPU 是为了改善计算机渲染三维图像，先驱是 `SGI`（Silicon Graphics Inc.），即硅谷图形公司，创始人 Jim Clark 是斯坦福的教授，图形学专家。

最早 3D 是大神卡马克开发的著名 Wolfenstein 3D（德军总部 3D），从不同视角看到的是 8 幅不同的贴图，不是真正的3D。直到90年代中期，才出现实时渲染多边形。

### 图形渲染的流程
3D画面实际上是多边形组合出来的。

![](https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac30p01.png)

<!--more-->

而完成这样的渲染需要5个步骤：
1. `顶点处理`（Vertex Processing）
2. `图元处理`（Primitive Processing）
3. `栅格化`（Rasterization）
4. `片段处理`（Fragment Processing）
5. `像素操作`（Pixel Operations）

![](https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac30p02.jpg)


#### 顶点处理
基于当前视角，把三维空间中的顶点转换到屏幕的二维空间上，即投影的过程。是一种通过线性代数计算得来，**顶点坐标的计算无依赖，可并行。**

#### 图元处理
将顶点处理后的各个顶点链接起来，变成多边形。但是，需要多做一个操作：**剔除和裁剪（Cull and Clip）**，即把不在屏幕里的内容去掉，减少后续工作量。（比如上图图元部分的 v0）


#### 栅格化
屏幕分辨率有限，`栅格化`就是把多边形转成屏幕里的像素（Pixel）点（图元处理后的多边形覆盖的所有像素点）。这里**每一个图元都可以并行独立地栅格化。**

#### 片段处理
栅格化后的像素点只有黑白色，需要计算每一个像素点的颜色、透明度等进行上色，就是`片段处理`。

#### 像素操作
最后就是将不同的多边形像素点混合到一起，输出到显示设备。

>以上5个渲染步骤就是`图形流水线`（Graphic Pipeline）。

### 解放图形渲染的 GPU
计算一下渲染需要的资源：
>假设90年代，屏幕640x480，每秒60帧。从栅格化开始每个像素3个流水线步骤，每步即使1个指令。那么：$640 \times 480 \times 60 \times 3 = 54M$。然而，当时的CPU一般主频60MHz，基本上要被上述渲染占满，实际上每个渲染步骤不止一个指令。

解决办法就是 Voodoo FX 这样的图形加速卡的出现。因为渲染流程固定，且计算逻辑简单（不需要流水线停顿、乱序执行等），并行度高，单独造硬件比用 CPU 划算。

于是，在当时整个顶点处理的过程还是都由 CPU 进行的，**不过后续所有到图元和像素级别的处理都是通过显卡去处理的。**

---

## 31 | GPU（下）

### Shader 和可编程图形处理器

GPU 也在逐步优化迭代。首先，1999 年 NVidia 推出的 GeForce 256 显卡，就**把顶点处理的计算能力，也从 CPU 里挪到了显卡里**。但渲染过程都是固定管线，程序员不能干预，只能调配置。

从 2001 年的 Direct3D 8.0 开始，微软第一次引入了`可编程管线`（Programable Function Pipeline）的概念。

![](https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac31p01.jpg)

一开始的可编程管线呢，**仅限于顶点处理（Vertex Processing）和片段处理（Fragment Processing）部分。**

可以编程的接口，我们称之为 `Shader`，即着色器，由可编程的模块功能决定。统一着色器架构（Unified Shader Architecture）就应运而生了。

顶点处理和片段处理上的逻辑不太一样，但指令集同一套。Vertex 和 Fragment Shader 分开，虽然设计简单，但资源有浪费，因为硬件串行的。于是，**统一着色器架构（Unified Shader Architecture）就应运而生了。**

![](https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac31p02.jpg)

将 Shader 变成通用模块，多加一点。如此，可以把 GPU 拿来做各种通用计算的用法，即 GPGPU（General-Purpose Computing on Graphics Processing Units，通用图形处理器）。

### 现代 GPU 的三个核心创意

#### 芯片瘦身
现代 CPU 里的晶体管变得越来越多，越来越复杂，不是主要为了“计算”这个功能，而是：**拿来处理乱序执行、进行分支预测，以及高速缓存部分。**

GPU 中没有上述那些，只有流式处理。只留下取指令、指令译码、ALU 以及执行这些计算需要的寄存器和缓存即可。

![](https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac31p03.jpg)

#### 多核并行和 SIMT
基于瘦身后，可以在 GPU 中塞多一些并行电路，实现多核并行。

CPU 里有 SIMD 技术，在做向量计算的时候，我们要执行的指令是一样的，只是同一个指令的数据有所不同而已。

GPU 就借鉴了 SIMD，做了 SIMT（Single Instruction，Multiple Threads）的技术。比 SIMD 更加灵活，CPU 一次性取出了固定长度的多个数据，放到寄存器里面，用一个指令去执行。而 **SIMT，可以把多条数据，交给不同的线程去处理。**

![](https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac31p04.jpg)

如上，在取指令和指令译码的阶段，可以给到后面多个不同的 ALU 并行进行运算。这样，就可以放下更多的 ALU，同时进行更多的并行运算了。

#### GPU 里的“超线程”
GPU 的指令可能也遇到流水线停顿，而解决方案类似CPU，可以调度别的计算任务给当前 ALU。一个 Core 里面的执行上下文的数量，需要比 ALU 多。

![](https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac31p05.jpg)


### GPU 性能差异
以 NVidia 2080 显卡为例，其算力如何？

![](https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac31p06.jpg)

>2080 显卡有 46 个 SM（Streaming Multiprocessor，流式处理器），每个 SM（也就是 GPU Core）里有 64 个 Cuda Core（ALU 的数量），也就是 Shader。
>还有 184 个 TMU，用来做纹理映射的计算单元，另一种 Shader。
>主频是 1515MHz，如果自动超频（Boost）的话，可以到 1700MHz。每个时钟周期可以执行两条指令。
>于是，浮点算力：（46 × 64 + 184）× 1700 MHz × 2 = 10.06 TFLOPS

Intel i9 9900K 的性能不到 1TFLOPS，它们的价格却差不多，算力差10倍。

---

## 32 | FPGA和ASIC
20 世纪末，计算机世界热衷于硬件的创新，21世纪初则转到了软件上。FPGA 和 ASIC 是2类经典的芯片。

### FPGA
一个四核 i7 的 Intel CPU，晶体管数量差不多有 20 亿个，设计和制作很困难。周期一般几月到几年，期间还需要各种测试，如果每次重做成本太高。

于是，编程化思想，能否制作一个硬件，通过不同的程序代码，来操作这个硬件之前的电路连线，以形成需要的芯片？有，`FPGA`，即`现场可编程门阵列`（Field-Programmable Gate Array）。

FPGA 有很多门电路，可以反复烧录，组合实现不同功能芯片。但如何编程呢？
**1. 用存储换功能实现组合逻辑；**
>其基本电路不用布线连接，而是通过软件设计真值表，存到LUT（Look-Up Table，查找表）这一存储空间中，其实就是不是真计算，而是存储不同逻辑计算结果映射关系。

**2. 对于需要实现的时序逻辑电路，直接放上 D 触发器，作为寄存器。**
>多个 LUT 的电路和寄存器组合成一个`逻辑簇`（Logic Cluster）的东西。在 FPGA 里，它也被叫做 CLB（Configurable Logic Block，可配置逻辑块）。基于 CLB 可以搭建我们需要的芯片。

![](https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac32p01.jpg)

**3. FPGA 是通过可编程逻辑布线，来连接各个不同的 CLB 组成芯片功能。**
>在 CLB 之间留有很多电路开关，通过控制此便可以实现不同 CLB 之间的连接方式。

### ASIC
除了 CPU、GPU、FPGA 这些通用型芯片外，日常中经常需要一些专用芯片处理某些固定任务。比如录音笔的音频芯片，遥控汽车的芯片等。

这种专用芯片一般称为 `ASIC`（Application-Specific Integrated Circuit），也就是专用集成电路。往往电路精简、设计简单、功耗低、成本低。

但，FPGA 成熟，没有硬件研发成本；ASIC 需要仿真、验证，还需要经过流片（Tape out）。单独研发成本也不一定低。

---

## 33 | 解读TPU
TPU 是过去几年比较知名的 ASIC。

### TPU V1 的来源
深度学习火起来后，计算量最大的是模型推理部分，第一代 TPU 便是为了优化此。

目标：
1. 响应快；
2. 功耗低。

### 深入理解 TPU V1
**TPU 需要快速上线和向前兼容。**
>TPU 设计成可插拔的板卡，甚至没有取指令的电路，而是通过 CPU，向 TPU 发送需要执行的指令。

**专用电路和大量缓存**

![](https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac33p01.jpg)

其中，
* 矩阵乘法单元（Matrix Multiply Unit）；
* 累加器（Accumulators）模块；
* 激活函数（Activation）模块和归一化 / 池化（Normalization/Pool）模块
  是顺序串联在一起的。

控制电路（Control）只占了 2%，没有冒险、分支预测等等。
超过一半的 TPU 的面积：作为 `本地统一缓冲区`（Local Unified Buffer）（29%）和`矩阵乘法单元`（Matrix Mutliply Unit）(24%)。其中，缓冲区使用 SRAM，比起内存使用的 DRAM 速度要快上很多，利于深度模型推理的高频读写。

**细节优化，使用 8 Bits 数据**
正常深度模型使用 32Bits 来处理浮点数，而在 TPU 内使用 8Bits。这是一种对深度模型进行 int8 量化的方案，可以使得存储更小，计算更快。

>当然，如果对精度比较依赖的话，可能会成为弊端。

---

## 34 | 理解虚拟机
>如何让空闲的机器分时段分大小租给不同需求的用户。


### 分时系统
多个终端连接同一个主机，会自动给程序或任务分配计算时间。

### 公有云
早期亚马逊租服务器只能整租，起步配置高，且换用户的时候需要清空数据和程序。

`虚拟机技术`，可以在一台物理服务器上，同时运行多个虚拟服务器，并且可以动态去分配，每个虚拟服务器占用的资源。不需要的可以关闭服务器，并保留数据资源。

### 虚拟机的技术变迁
`虚拟机`（Virtual Machine）技术，在现有硬件的操作系统上，模拟一个计算机系统的技术。

**解释型虚拟机**
模拟系统最简单的就是兼容这个计算机系统的指令集。

这种解释执行方式的最大的优势就是，模拟的系统可以跨硬件。

但，有缺陷：
* 无法精确“模拟”；
* 性能差。

**虚拟机的性能提升**
为了克服上述缺陷，又要支持一个操作系统上跑多个完整的操作系统，方案就是**加入一个中间层**。即`虚拟机监视器`，英文叫 VMM（Virtual Machine Manager）或者 Hypervisor。

实际的指令是怎么落到硬件上去实际执行？

`Type-2 虚拟机`：像一个运行在操作系统上的软件。对于最终到硬件的指令，客户机的操作系统->虚拟机监视器->宿主机的操作系统。
>只是把在模拟器里的指令翻译工作，挪到了虚拟机监视器里。更多是用在我们日常的个人电脑里，而不是用在数据中心里。

`Type-1 虚拟机`：客户机的指令交给虚拟机监视器之后呢，不再需要通过宿主机的操作系统调用硬件，而是可以直接由虚拟机监视器去调用硬件。

![](https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac34p01.jpg)


### Docker
Type-1 虚拟机缺点：实际的物理机上，我们可能同时运行了多个的虚拟机，每个都运行了一个属于自己的单独的操作系统。

![](https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/computeroa/computeroac34p02.jpg)

不管依赖什么，其实都是跑在 Linux 内核上的。通过 Docker，我们不再需要在操作系统上再跑一个操作系统，而只需要通过容器编排工具（如 Kubernetes），能够进行各个应用之间的**环境和资源隔离就好了。**

---