---
title: 推荐系统三十六式--读书笔记（5-7 内容推荐）
date: 2019-05-06
categories:
- 学习笔记
- 论文解读
tags:
- 推荐系统
- 机器学习
- 算法

---

[参考原作：推荐系统三十六式-刑无刀](https://time.geekbang.org/column/intro/74)

##5.【内容推荐】画鬼容易画人难：用户画像的“能”和“不能”
####**什么是用户画像**
英文可以分为Personas 和 User Profile。

- Personas 属于交互设计领域的概念。
- UserProfile原本用于营销领域。（推荐系统中的）

推荐系统一般是对用户和物品之间的匹配评分，也就是预测用户评分或者偏好。推荐系统在对匹配评分前，则首先就要将用户和物品都向量化，这样才能进行计算。

**建立用户画像的关键因素：第一个是维度，第二个是量化。**

对于维度，三个特点：
>1	每个维度的名称都是可理解的。
2	维度的数量是我拍脑袋定的。
3	有哪些维度也是我拍脑袋确定的。 

其次是量化，构建方法分成三类：
>1.  构建方法分成三类。
直接使用原始数据作为用户画像的内容，如注册资料等人口统计学信息，或者购买历史，阅读历史等，除了数据清洗等工作，数据本身并没有做任何抽象和归纳。通常对于用户冷启动等场景非常有用。
2.	第二类就是堆数据。
做统计工作，这是最常见的用户画像数据，常见的兴趣标签，就是这一类，就是从历史行为数据中去挖掘出标签，然后在标签维度上做数据统计，用统计结果作为量化结果。
3.	第三类就是黑盒子。
就是用机器学习方法，学习出人类无法直观理解的稠密向量。比如使用潜语义模型构建用户阅读兴趣，或者使用矩阵分解得到的隐因子，或者使用深度学习模型学习用户的 Embedding 向量。

---

##6.【内容推荐】从文本到用户画像有多远
基于内容推荐离不开为用户构建一个初级的画像，这种初级的画像一般叫做用户画像（User Profile），一些大厂内部还习惯叫做 UP。

从文本开始用户这一端比如说有： 
>1.	注册资料中的姓名、个人签名； 
2.	发表的评论、动态、日记等； 
3.	聊天记录。

物品的一端：
>1.	物品的标题、描述； 
2.	物品本身的内容（一般指新闻资讯类）； 
3.	物品的其他基本属性的文本。 

构建用户画像所需要的两步：
>1. 把所有非结构化的文本结构化，去粗取精，保留关键信息；   
2. 根据用户行为数据把物品的结构化结果传递给用户，与用户自己的结构化信息合并。

###**结构化文本**
物品端的文本信息，可以利用成熟的 NLP 算法分析得到的信息：
>1.	关键词提取：最基础的标签来源，也为其他文本分析提供基础数据，常用 **TF-IDF 和 TextRank**。 
2.	实体识别：人物、位置和地点、著作、影视剧、历史事件和热点事件等，常用基于词典的方法结合 **CRF** 模型。
3.	内容分类：将文本按照分类体系分类，用分类来表达较粗粒度的结构化信息。
4.	文本 ：在无人制定分类体系的前提下，无监督地将文本划分成多个类簇也很常见，别看不是标签，类簇编号也是用户画像的常见构成。
5.	主题模型：从大量已有文本中学习主题向量，然后再预测新的文本在各个主题上的概率分布情况，也很实用，其实这也是一种聚类思想，主题向量也不是标签形式，也是用户画像的常用构成。
6.	嵌入：也叫作 Embedding，从词到篇章，无不可以学习这种嵌入表达。嵌入表达是为了挖掘出字面意思之下的语义信息，并且用有限的维度表达出来。写留言下面我来介绍几种常用的文本结构化算法。 

####**1).TF-IDF**
TF 全称就是 Term Frequency，是词频的意思，IDF 就是 Inverse Document Frequency 是逆文档频率的意思。
**思想：在一篇文字中反复出现的词会更重要，在所有文本中都出现的词更不重要。**

>TF 和 IDF 两个指标：
1.	TF，就是词频，在要提取关键词的文本中出现的次数； 
2.	IDF，是提前统计好的，在已有的所有文本中，统计每一个词出现在了多少文本中，记为n，也就是文档频率，一共有多少文本，记为 N。那么IDF就为：
$$IDF=log\frac{N}{n+1}$$

DF 的计算公式有这么几个特点： 

- 所有词的 N 都是一样的，因此出现文本数越少 (n) 的词，它的 IDF 值越大； 
- 如果一个词的文档频率为 0，为防止计算出无穷大的 IDF，所以分母中有一个 1； 
- 对于新词，本身应该 n 是 0，但也可以默认赋值为所有词的平均文档频率。 

计算出 TF 和 IDF 后，将两个值相乘，就得到每一个词的权重。根据该权重筛选关键词的方式有： 
1. 给定一个 K，取 Top K 个词，这样做简单直接，但也有一点，如果总共得到的词个数少于K，那么所有词都是关键词了，显然这样做不合理； 
2. 计算所有词权重的平均值，取在权重在平均值之上的词作为关键词；

####**2).TextRank**
TextRank与PageRank的思想类似，著名的 PageRank 算法是 Google 用来衡量网页重要性的算法。TextRank可以概括为：
>1.	文本中，设定一个窗口宽度，比如K个词，统计窗口内的词和词的共现关系，将其看成无向图。图就是网络，由存在连接关系的节点构成，所谓无向图，就是节点之间的连接关系不考虑从谁出发，有关系就对了； 
2.	所有词初始化的重要性都是 1； 
3.	每个节点把自己的权重平均分配给“和自己有连接“的其他节点； 
4.	每个节点将所有其他节点分给自己的权重求和，作为自己的新权重； 
5.	如此反复迭代第 3、4 两步，直到所有的节点权重收敛为止。 

####**3).内容分类**
在门户网站时代，每个门户网站都有自己的频道体系，这就是个大的内容分类。移动互联网 UGC 时代，图文信息流 App 的资讯内容也需要被自动分类到不同的频道中。但，门户时代主要以长文本为主，可提取的信息量比较多，UGC时代短文本为主，分类较难。**短文本分类方面经典的算法是 SVM，在工具上现在最常用的是 Facebook 开源的 FastText**

####**4).实体识别**
命名实体识别（也常常被简称为 NER，Named-Entity Recognition）在 NLP 技术中常常被认为是序列标注问题，和分词、词性标注属于同一类问题。

>所谓序列标注问题，就是给你一个字符序列，从左往右遍历每个字符，一边遍历一边对每一个字符分类，分类的体系因序列标注问题不同而不同，**通常的算法就是隐马尔科夫模型（HMM）或者条件随机场（CRF）**： 
1.	分词问题：对每一个字符分类为“词开始”“词中间”“词结束”三类之一； 
2.	词性标注：对每一个分好的词，分类为定义的词性集合的之一； 
3.	实体识别：对每一个分好的词，识别为定义的命名实体集合之一。

实体识别还有比较实用化的非模型做法：词典法。提前准备好各种实体的词典，使用 trie-tree 数据结构存储，拿着分好的词去词典里找，找到了某个词就认为是提前定义好的实体了。以实体识别为代表的序列标注问题上，工业级别的工具上 spaCy 比 NLTK 在效率上优秀一些。

####**5).聚类**
传统聚类方法在文本中的应用，今天逐渐被主题模型取代，同样是无监督模型，以 LDA 为代表的主题模型能够更准确地抓住主题，并且能够得到软聚类的效果，也就是说可以让一条文本属于多个类簇。
LDA 模型需要设定主题个数，如果你有时间，那么这个 K 可以通过一些实验来对比挑选，方法是：每次计算 K 个主题两两之间的平均相似度，选择一个较低的 K 值；如果你赶时间，在推荐系统领域，只要计算资源够用，主题数可以尽量多一些。

>另外，需要注意的是，得到文本在各个主题上的分布，可以保留概率最大的前几个主题作为文本的主题。LDA 工程上较难的是并行化，如果文本数量没到海量程度，提高单机配置也是可以的，**开源的 LDA 训练工具有 Gensim，PLDA 等**可供选择。

####**6).词嵌入**
词嵌入，也叫作 Word Embedding。前面讲到的结构化方案，除了 LDA，其他都是得到一些标签，而这些标签无一例外都是稀疏的，而词嵌入则能够为每一个词学习得到一个稠密的向量。

向量可以做以下的事情： 
1.	计算词和词之间的相似度，扩充结构化标签； 
2.	累加得到一个文本的稠密向量； 
3.	用于聚类，会得到比使用词向量聚类更好的语义聚类效果。

**Word2Vec** 是用浅层神经网络学习得到每个词的向量表达，Word2Vec 最大的贡献在于一些工程技巧上的优化，使得百万词的规模在单机上可以几分钟轻松跑出来，得到这些词向量后可以聚类或者进一步合成句子向量再使用。

###**标签选择**
前面的介绍得到了诸如标签（关键词、分类等）、主题、词嵌入向量。接下来就是第二步：如何把物品的结构化信息给用户。
一种简单粗暴的办法是直接把用户产生过行为的物品标签累积在一起，但是这里要说的是另一种思路。我们把用户对物品的行为，消费或者没有消费看成是一个分类问题。用户用实际行动帮我们标注了若干数据，那么挑选出他实际感兴趣的特性就变成了特征选择问题。

最常用的是两个方法：**卡方检验（CHI）和信息增益（IG）**。基本思想是： 
>1.	把物品的结构化内容看成文档； 
2.	把用户对物品的行为看成是类别； 
3.	每个用户看见过的物品就是一个文本集合； 
4.	在这个文本集合上使用特征选择算法选出每个用户关心的东西。 

####**1).词嵌入**
CHI 就是卡方检验，本身是一种特征选择方法。前面的 TF-IDF 和 TextRank 都是无监督关键词提取算法，而卡方检验则是有监督的。卡方检验本质上在检验“词和某个类别 C 相互独立”这个假设是否成立。

计算一个词 Wi 和一个类别 Cj 的卡方值，需要统计四个值：
| 卡方检验   | 属于类别$C_j$   |  不属于类别$C_j$  | 总计 |
|  :-----   | :-----  | :----  | :---- |
| 包含词$W_i$     | A |  B     | A+B  |
| 不包含$W_i$    |   C   |  D   |  C+D  |
| 总计        |    A+C    |  B+D  |  N=A+B+C+D  |

每个卡方值的计算：
$$\mathcal X^2(W_i,C_j)=\frac{N(AD-BC)^2}{(A+C)(A+B)(B+D)(C+D)}$$

>1.	每个词和每个类别都要计算，只要对其中一个类别有帮助的词都应该留下； 
2.	由于是比较卡方值的大小，所以公式中的 N 可以不参与计算，因为它对每个词都一样，就是总的文本数； 
3.	卡方值越大，意味着偏离“词和类别相互独立”的假设越远，靠“词和类别互相不独立”这个备择假设越近。

####**2).信息增益**
IG 即 Information Gain，信息增益，也是一种有监督的关键词选择方法。
另一个，对于信息商：
1.	各个类别的文本数量差不多时，信息熵就比较大。 
2.	其中少数类别的文本数量明显较多时，信息熵就较小。

信息增益计算分成三步：
1.	统计全局文本的信息熵； 
2.	统计每个词的条件熵，就是知道了一个词后再统计文本的信息熵，只不过这里要分别计算包含词和不包含词两部分的信息熵，再按照各自文本比例加权平均； 
3.	两者相减就是每个词的信息增益。

>信息：$I=-log_2p(x_i)$
信息熵：$H=-\sum_{i=1}^np(x_i)log_2p(x_i)$
信息增益：分类前的信息熵-分类后的信息熵

信息增益应用最广就是数据挖掘中的决策树分类算法，经典的决策树分类算法挑选分裂节点时就是计算各个属性的信息增益，始终挑选信息增益最大的节点作为分裂节点。**卡方检验和信息增益不同之处在于：前者是针对每一个行为单独筛选一套标签出来，后者是全局统一筛选。**
这些方法都是在离线阶段批量完成的，把用户的画像生成配置成离线任务，每天更新一遍，次日就可以使用新的用户画像。

---

##7.【内容推荐】超越标签的内容推荐系统
所谓的基于内容推荐，通俗讲，就是一个包装成推荐系统的信息检索系统。为什么基于内容的推荐系统这么重要呢？因为内容数据非常易得，哪怕是在一个产品刚刚上线，用心找的话总能找到一些可以使用的内容，不需要有用户行为数据就能够做出推荐系统的第一版。

>要把基于内容的推荐做好，需要做好“抓、洗、挖、算”:。
1.	抓：大厂们也在不断地抓数据丰富自己的内容，抓取数据补充内容源，增加分析的维度，两者必不可少。
2.	洗：抓来的数据包含大量的脏数据，冗余的内容、垃圾内容、政治色情等敏感内容等等都需要被洗出去。
3.	挖：不管是抓来还是自家的数据，都包含大量的潜在的价值信息。很多推荐系统提升效果并不是用了更复杂的推荐算法，而是对内容的挖掘做得更加深入。
4.	算：匹配用户的兴趣和物品的属性，计算出更合理的相关性，这是推荐系统本身的使命，不仅仅是基于内容的推荐才要做的。

**内容推荐的框架**
[![内容推荐的框架图](https://i.postimg.cc/YqH8dYgC/image.jpg)](https://postimg.cc/z3xnBL0s)

- 内容这一端：内容源经过内容分析，得到结构化的内容库和内容模型，也就是物品画像。
- 用户这一端：用户看过推荐列表后，会产生用户行为数据，结合物品画像，经过用户分析得到用户画像。

内容源
在互联网中，抓数据是一件可做不可说的事情，哪怕是市值几千亿的大厂，也有专门的小分队抓数据，补充推荐系统每天的内容消耗。抓取后接着就是清洗数据，**去重与识别垃圾内容、色情内容、政治敏感内容等都是必修课。**

**内容分析和用户分析**
基于内容的推荐，最重要的不是推荐算法，而是内容挖掘和分析。内容挖掘越深入，哪怕早期推荐算法仅仅是非常硬的规则，也能取得不俗的效果。举个例子，如果推荐物品是短视频，我们分几种情况看：
>1.	如果短视频本身没有任何结构化信息，如果不挖掘内容，那么除了强推或者随机小流量，没有别的合理曝光逻辑了；
2.	如果对视频的文本描述，比如标题等能够有内容分类，比如是娱乐类，那么对于喜欢娱乐的用户来说就很合理；
3.	如果能够进一步分析文本的主题，那么对于类似主题感兴趣的用户就可能得到展示；
4.	如果还能识别出内容中主角是吴亦凡，那就更精准锁定一部分用户了；
5.	如果再对内容本身做到嵌入分析，那么潜藏的语义信息也全部抓住，更能表达内容了。

随着内容分析的深入，能抓住的用户群体就越细致，推荐的转化率就越高，用户对产品的好感度也就增加了。

内容分析的产出有两个：
1.	结构化内容库；
2.	内容分析模型。
容易被忽略的是第二个用途，在内容分析过程中得到的模型，比如说：
1.	分类器模型；
2.	主题模型；
3.	实体识别模型；
4.	嵌入模型。
**这些模型主要用在：当新的物品刚刚进入时，需要实时地被推荐出去，这时候对内容的实时分析，提取结构化内容，再与用户画像匹配。**

**内容推荐算法**
最简单的推荐算法当然是计算相似性即可，用户的画像内容就表示为稀疏的向量，同时内容端也有对应的稀疏向量，两者之间计算余弦相似度，根据相似度对推荐物品排序。
虽然粗糙但是效果好，基于内容的推荐天然有一个优点：可解释性非常强。
如果再进一步，要更好地利用内容中的结构化信息，因为一个直观的认识是：不同字段的重要性不同。

对于资讯类，可以借鉴信息检索中的相关性计算方法来做推荐匹配计算：**BM25F 算法。**

>前述的两种方法都不是机器学习方法，因为没有考虑推荐的目标。
机器学习一种最典型的场景：提高某种行为的转化率，如点击、收藏、转发等。那么标准的做法是：收集这类行为的日志数据，转换成训练样本，训练预估模型。
每一条样本由两部分构成：一部分是特征，包含用户端的画像内容，物品端的结构化内容，可选的还有日志记录时一些上下文场景信息，如时间、地理位置、设备等等，另一部分就是用户行为，作为标注信息，包含“有反馈”和“无反馈”两类。
**常用模型是逻辑回归（Logistic Regression）和梯度提升树（GBDT）或者两者的结合，即LR+GBDT。**

**总结**
基于内容的推荐一般是推荐系统的起步阶段，而且会持续存在，它的重要性不可取代。因为：
1.	内容数据始终存在并且蕴含丰富的信息量，不好好利用就可惜了；
2.	产品冷启动阶段，没有用户行为，别无选择；
3.	新的物品要被推荐出去，首选内容推荐。
[![内容推荐](https://i.postimg.cc/1tFkc6qx/image.jpg)](https://postimg.cc/TLRNTK0Q)

---





