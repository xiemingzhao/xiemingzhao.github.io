---
title: Deep and Cross Network for Ad Click Predictions (论文解析)
categories:
  - 学习笔记
  - 论文解析
tags:
  - 机器学习
  - 推荐
  - 排序
  - CTR预估
  - 神经网络
  - Deep & Cross
mathjax: true
copyright: true
abbrlink: 96501d7f
date: 2019-07-29 00:00:00
---
[原始论文：Deep & Cross Network for Ad Click Predictions](https://arxiv.org/pdf/1708.05123.pdf)


## 深度和交叉网络的广告点击预测

### **摘要**
特征工程已经成为许多预测模型成功的关键。然而，这个过程是不平常的并且经常会要手动特征工程或者穷举搜索。DNNs能够自动地学习特征交叉项；然而，它们都是隐式地生成所有交叉项，并且学习所有类型的交叉特征不一定有效。在本文中，我们提出深度和交叉网络(DCN)，它保持了深度模型的优势，并且又超越了这，它是一种在学习某种边界程度特征交叉项中更为有效的新奇网络。此外，DCN显示地在每一层应用特征交叉，不要求做人工程特征工程，同时也只是给DNN模型增加了一些可以忽略不计的复杂度。我们的实验结果已经证明它在CTR预测数据集和密集的分类数据集上，相对于其他高级模型在模型准确性和记忆方法上都具有优越性。

### **1 介绍**
点击率（CTR）预测是一个大规模的问题，它对数十亿美元的在线广告业来说至关重要。在广告业中，广告商会想发布商付费以在发布商的网站上展示他们的广告。一个普遍的付费模式是平均点击成本（CPC）模型，即广告商仅在点击发生的时候才会付费。因此，出版商的收入很大程度上依赖于能够准确预测CTR。

<!--more-->

识别出常用的预测特征且同时探索出那些看不见的或者稀少的交叉特征是做出好预测的关键。然而，网站级别的推荐系统的数据主要都是离散的和类别型的，这就导致了一个大的和稀疏的特征空间，而这对于特征探索来说是一个挑战。这就限制了大多数的大规模系统都是线性模型例如逻辑回归。

线性模型是简单的，可解释的并且容易扩展的；然而，它们受限于自己的表达能力。另一方面，交叉特征已经被证明能够有效地提高模型的表达力。不幸的是，它一般要求人工特征工程或者穷举来找到这些特征；再者，泛化出这些看不见的特征交叉项是很困难的。

在本文中，我们致力于通过引入一个新的神经网络结构来避免特征工程任务——一个*交叉网络*——它是以自动的方式显示地应用在特征交叉中。交叉网络由多层网络组成，其中特征的最高交叉维度完全由网络层的深度决定。每一层网络都以及已经存在的特征生成一个更高度的交叉项，同时又保留了前一层网络的交叉项。我们将交叉忘了和一个深度神经网络（DNN）进行联合训练。DNN能够捕获特征中的非常复杂的交叉项；然而，相比于我们的交叉忘了它需要同一数量级的参数，也无法形成显示的交叉特征，并且可能无法有效地学习某些特征交叉项。然而，交叉和深度部分的联合训练能够有效地捕获预测性的特征交叉项，并且在Criteo CTR数据集上提供了一个最先进的效果表现。

**1.1 相关工作**
由于数据集的规模和维度急剧性的增加，于是提出了很多的方法用来避免特定任务中的大规模特征工程，大部分都是基于嵌入技术和神经网络的。

因式分解机（FMs）将稀疏特征映射到低维的稠密向量上，并且从这些特征的内积中学习特征交叉项。场感知因式分解机（FFMs）让每个特征都可以学习到多个向量，其中每个响亮都是与一个场相关的。遗憾的是，FM和FFM浅显的结构限制了它们的模型表达力。有许多的工作都是为了将FM扩展到一个更高的级别，但是一个缺点就是产生了大量的参数从而大大增加了原本他们不期望产生的计算成本。深度神经网络（DNN）就可以学习到一些重要的高维的特征交叉项，这得益于它们的嵌入向量和非线性的激活函数。最近残差网络的成功使得训练一个非常深的网络有了可能。深度交叉扩展了残差网络，同时通过对所有输入类型的堆叠达到了自动特征学习的效果。

深度学习的非凡成功引出了它的表达力的理论分析。有研究表明，在给定足够多的的隐含单元或者隐含层的时候，DNN能够再某种平滑线的假设条件下取近似一个有任意准确性的函数。再者，实际上已经发现了DNN在有合适参数的时候就已经能够表现地很好了。一个关键的原因就是实际使用的大多数函数都不是任意选择的。

一个依然存在的问题就是DNN是否真的在那些实际中使用的表征函数中是最有效的一个。在Kaggle竞赛中，许多胜利者的解决方法中人工精心设计的特征都是低阶的、确切形式且有效地。另一方面，从DNN中学习到的特征都是隐含的且高度非线性的。这就表明了设计一个模型要能够学习到相比于普通的DNN更加有效且确切的有界阶特征交叉项。

wide-and-deep就是这种想法创建的模型。它将交叉特征作为线性模型的输入，然后将线性模型和DNN模型进行联合训练。然而，wide-and-deep是否成功很大程度上依赖于交叉特征的事前选择，一个指数级的问题就是是否存在还没发现的更有效的方法。

**1.2 主要贡献**
在本文中，我们提出了Deep & Cross Network（DCN）模型，它能够在同时有稀疏输入和密集输入的时候进行网站规模的自动化特征学习。DCN能够有效地抓取有界阶的有用特征交叉项，学习高度非线性的交叉项，并且不要求人工特征工程或者穷举，同时又只有较低的计算成本。

本文主要的贡献包括：

* 我们提出了一个将特征交叉应用在每一层的新交叉网络，它能够有效地学习到具有预测价值的有限阶交叉特征，并且不要求进行人工特征工程或者穷举。
* 交叉忘了是简单且有效的。通过设计，每一层多项式的最高阶都在增加并且由层数的深度决定。整个网络是由从低阶到高阶的交叉项以及所有不同的系数组成的。
* 交叉网络是能够有效记忆的，并且能够很简单地实现。
* 一个带有交叉网络的DNN，在参数个数少一个量级的情况下，它的对数损失依然比普通的DNN要低。

### **2 深度&交叉网络（DCN）**
在这一部分，我们将会介绍深度&交叉网络（DCN）模型的结构。DCN是开始于embedding和stacking层的，紧接着是一个交叉网络和一个深度网络并行。按顺序接着是一个最终的联合层用来合并两个网络的输出。完整的DCN模型如图1中所示。

![D&C-1.jpg](https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/deep%26cross-1.JPG)

**2.1 嵌入和堆叠层**
我们考虑包含稀疏和密集特征的输入数据。在网站级规模的推荐系统如CTR预估中，输入数据大部分都是类别型特征，例如“country=usa”。这样的特征经常会被进行one-hot编码，例如“[0,1,0]”；然而，这就经常导致产生过高维的特征空间来适用大型词典。

为了降低维度，我们使用了一个embedding过程来将这些二值特征转换成密集的实值向量（通常称为嵌入向量）：

$$x_{embed,i}=W_{embed,i}x_i$$

其中$x_{embed,i}$是嵌入向量，$x_i$是第i个类别的二值输入，$W_{embed,i} \ \in \mathbb R^{n_e \times n_v}$是对应的嵌入矩阵，它可以和网络中其他的参数一起进行优化，$n_e,n_v$分别是嵌入层大小和词典的大小。

最后，我们将嵌入向量和标准化后的密集特征进行堆叠，形成一个最终的向量：

$$x_0 = [x_{embed,1}^T,...,x_{embed,k}^T,x_{dense}^T]$$

然后再将这个向量喂入到网络中去。

**2.2 交叉网络**
我们创新交叉网络的关键思想就是以一个有效地方式来显示地应用特征交叉。交叉网络由交叉层组成，每一层都有如下的公式：

$$x_{l+1} = x_0 x_l^T w_l + b_l x_l = f(x_l , w_l, b_L) + x_l$$

其中$x_l,x_{l+1} \ \in \ \mathbb R^d$都是列向量，分别表示第l层和第l+1层交叉网络的输出$w_l, b_l \ \in \ \mathbb R^d$是第l层网络的权重和偏置项参数。每一交叉层在特征交叉f之后都反加上它的输入部分，并且映射函数$f:\mathbb R^d \rightarrow \mathbb R^d$拟合$x_{l+1}-x_l$的残差。一个交叉层的可视化展示如图2所示。

![D&C-2.jpg](https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/deep%26cross-2.JPG)

**高阶特征交叉项**。交叉网络特殊的结构造就了交叉特征的阶数随着网络层数增加而增加。第l层交叉网络的多项式最高阶数（相对于输入层来说）是l+1。事实上，交叉网络包含了所有的交叉项$x_1^{\alpha_1} x_2^{\alpha_2}...x_d^{\alpha_d}$，其中d取值从1到l+1。详细的分析在章节3。

**复杂度分析**$L_c$表示交叉层的个数，d表示输入层的维度。然后，交叉网络中的参数个数就是：
$$d \times L_c \times 2$$

交叉网络的时间和空间复杂度是关于输入层维度的线性增长。因此，交叉网络相比与其深度部分仅引入了一个微乎其微的复杂度部分，这使得DCN的整体复杂度与传统的DNN基本一致。这个有效性是得益于$x_0x_l^T$的秩为1的属性，这使得我们可以无需计算和存储整个矩阵的时候生成所有的交叉项。

交叉网络很少的参数限制了模型的能力。为了获得更高阶的非线性交叉项，我们并行引入了深度网络。

**2.3 深度网络**
深度网络部分是一个全连接的前向神经网络，其每一层的公式可以表示成如下：

$$h_{l+1} = f(W_lh_l+b_l)$$

其中$h_l \in \mathbb R^{n_l},h_{l+1}\in \mathbb R^{n_{l+1}}$分别是第l和第l+1隐含层；$W_l \in \mathbb R^{n_{l+1} \times n_l}, b_l \in \mathbb R^{n_{l+1}}$是第l深度层的参数；$f(\cdot)$是ReLU激活函数。

**复杂度分析**。为了简化，我们假设所有的深度网络层都是等维度的。$L_d$表示深度网络的层数，m表示深度网络层的大小。那么深度网络的参数个数就是：

$$d \times m + m + (m^2 + m) \times (L_d -1)$$

**2.4 联合层**
联合层是合并了了两个网络的输出部分，然后将合并后的向量喂入到标准的逻辑层中。

下面就是二分类问题的公式：
$$p = \sigma([x_{l_1}^L,h_{L_2}^T w_{logits})$$

其中$x_{L_1} \in \mathbb R^d, h_{L_2} \in \mathbb R^m$分别是交叉网络和深度网络的输出，$w_{logits} \in \mathbb R^{(d+m)}$是合并层的参数向量，并且$\sigma(x) = 1/(1+exp(-x))$。

损失函数是带有正则项的对数损失函数，

$$loss = -\frac{1}{N} \sum_{i=1}^N y_i log(p_i) + (1-y_i) log(1-p_i) + \lambda \sum_l ||w_l||^2$$

其中$p_i$是根据前一个公式计算的概率值，$y_i$是真实的标签，N是输入层的总数，$\lambda$是L2正则项参数。

我们将两个网络联合一起进行训练，这使得每一个单独的网络在训练过程中可以感知到其他部分。

### **3 交叉网络分析**
在这一部分，我们分析DCN的交叉网络为了更好地理解它的有效性。我们我们提拱了三个角度：多项式近似，泛化成FM，和高效映射。为了简化，我们假设$b_i = 0$。

*注意*。将$w_j$中的第i个元素表示成$w_j^{(i)}$。对于多索引$\alpha = [\alpha_1,...,\alpha_d] \in \mathbb N^d$和$x = [x_1,...,x_d] \in \mathbb R^d$，我们定义$|\alpha| = \sum_{i=1}^d \alpha_i$。

*术语*。交叉项（单个的）的等级$x_1^{\alpha_1}x_2^{\alpha_2}...x_d^{\alpha_d}$定义为$|\alpha|$。多项式的阶数由交叉项的最高阶来确定。

**3.1 多项式近似**
根据魏尔斯特拉斯逼近定理，闭区间上的连续函数可以用多项式函数一致逼近。因此，我们将从多项式逼近的角度来分析交叉网络。特别地，交叉网络近似的同次多项式类，以一种有效地、更具表达力的并且泛化的方式拟合现实数据集。

我们仔细地研究了关于交叉网络的同次多项式类的近似。我们定义$P_n(x)$表示n次多项式：

$$P_n(x) = \{\sum_{\alpha} w_{\alpha} x_1^{\alpha_1} x_2^{\alpha_2}...x_d^{\alpha_d}|0 \leq |\alpha| \leq n, \alpha \in \mathbb N^d\}$$

这个类中的每个多项式都有$O(d^n)$个系数。我们证明了，仅仅需要$O(d)$个参数，交叉网络就可以包含同次多项式汇总的所有交叉项，并且每一项的系数都互不相同。

*定理3.1* 考虑一个l层交叉网络，其第i+1层定义为$x_{i+1} = x_0x_i^Tw_i + x_i$。网络的输入设为$x_0 = [x_1,x_2,...,x_d]^T$，输出为$g_l(x_0) = x_l^Tw_l$，其中参数为$w_i,b_i \in \mathbb R^d$。然后，这个多项式$g_l(x_0)$将会衍生出下面的多项式类：

$$\{\sum_{\alpha} c_{\alpha}(w_0,...,w_L) x_1^{\alpha_1} x_2^{\alpha_2}...x_d^{\alpha_d}|0 \leq |\alpha| \leq l+1, \alpha \in \mathbb N^d \}$$

其中$c_{\alpha} = M_{\alpha} \sum_{i \in B_{\alpha}} \sum_{j \in P_{\alpha}} \prod_{k = 1}^{|\alpha|} w_{i_k}^{(j_k)}$，$M_{\alpha}$是常数，且与$w_i$无关，$i = [i_1,...,i_{|\alpha|}] 和 j = [j_1,...,i_{|\alpha|}]$是对应的索引，$B_{\alpha} = \{y \in \{0,1,...,l\}^{|\alpha|}||y_i < y_j \cap y_{|\alpha|} = l\}$，并且$P_{\alpha}$是索引所有排列组成的集合$(1,...,1 \cdots d,...,d)$。

定理3.1的证明在附录中。我们给定一个示例，考虑$x_1x_2x_3$的系数$c_{\alpha}$，其中$\alpha = (1,1,1,0,...,0)$。对于某些常数，当$l = 2, c_{\alpha} = \sum_{i,j,k \in P_{\alpha}} w_0^{(i)} w_1^{(j)} w_2^{(k)}$；当$l = 3, c_{\alpha} = \sum_{i,j,k \in P_{\alpha}} w_0^{(i)} w_1^{(j)} w_3^{(k)} +  w_0^{(i)} w_2^{(j)} w_3^{(k)} +  w_1^{(i)} w_2^{(j)} w_3^{(k)}$。

**3.2 FM的推广**
交叉网络这种参数分享的思想类似于FM模型，进一步将其扩展到一个深度结构。

在FM模型中，特征$x_i$是伴随着一个参数向量$v_i$，并且交叉项$x_ix_j$的权重是由$(v_i,v_i)$计算得到的。在DCN汇总，$x_i$是和标量集$\{w_k^{(i)} \}_k^l$相关的，并且$x_i x_j$的权重是由集合$\{w_k^{(i)} \}_{k=0}^l$ 和 $\{w_k^{(j)} \}_{k=0}^l$中的各参数相乘得到的。模型每个特征学习的一些参数是独立于其他特征的，交叉项的权重是对应参数的某种联合。参数贡献不仅能够是的模型更加有效，而且也能使得模型能够产生看不见的特征交叉项并且使其对于噪声更加稳健。例如，使用带有稀疏特征的数据集的时候。如果两个二值类特征$x_i$和$x_j$很少或者从来不会在训练集中出现，即$x_i \neq 0 \wedge x_j \neq 0$，所以学习到的$x_i,x_j$的权重就不会在预测中输出有异议的信息。

FM模型是一个比较浅显的结构，其职能表征出2次的交叉项。相反，DCN能够构建所有的交叉项$x_1^{\alpha_1}x_2^{\alpha_2}...x_d^{\alpha_d}$其中$|\alpha|$由一些决定于网络层深度的常数来界定，如定理3.1所声明。因此，交叉网络是将参数共享这种思想从单层扩展到了多层和高次交叉项。注意到，不同于高阶的FM模型，一个交叉网络中的参数的个数仅仅随着输入层维度而线性增的长。

**3.3 高效地投影**
每一个交叉网络层都会将$x_0和x_l$映射出它们之间所有的成对交叉项，并且以一种有效的方式产生输入层的维度。

考虑$\tilde x \in \mathbb R^d$作为一个交叉层的输入。交叉层会隐式地构建出$d^2$个成对交叉项$x_i \tilde x_j$，并且会以一个高效记忆的方式将它们映射到d维空间。然而，直接的方式将会带来三倍的成本。

我们的交叉层提供了有效地解决方案来将成本降低到关于d维的线性函数。对于$x_p = x_0 \tilde x^T w$。这实际上等于：

$$x_p^T = [x_1\tilde x_1 ... x_1 \tilde x_d \ ... \ x_d \tilde x_1 ... x_d \tilde x_d] \left[\begin{array}{cccc}w&0&...&0 \\ 0&w&...&0 \\ ...&...&...&... \\ 0&0&...&w \end{array} \right]$$

其中行向量包含所有的$d^2$个成对的交叉向量$x_i\tilde x_j$，投影矩阵有一个固定的对角结构，其中$w\in \mathbb R^d$是一个列向量。

### **4 实验结果**
在这一部分，我们字啊一些流行的预测数据及上评估DCN模型的表现。

**4.1 Criteo Display Ads 数据**
Criteo Display广告数据及是为了预测广告点击率的。它包含13个整数型特征和26个类别型特征，其中每个类别都有高基数集。对于这个数据集，**在对数损失上有0.001的提升就可以被认为是实践显著的。**当考虑一个大的用户基础的时候，预测准确率的一个小提升就潜在地带来公司收益的大增长。数据包含11GB的横跨7天的用户日志（大约4100万条记录）。我们使用前6天的数据进行预测，并且将第7天的数据随机地等量地分成测试集和验证集。

**4.2 实现细节**
DCN是在TensorFlow上实现的，我们简短地讨论一些DCN训练中的一些实现细节。

*数据的处理和嵌入**。实值特征是使用对数变化来进行标准化处理的。对于类别特征，我们将特征嵌入成具有维度$6 \times (category cardinality)^{1/4}$的密集向量。将所有的嵌入结果全部连接到一起形成一个1026维的向量。

*优化*。我们使用Adam这种小批量随机优化的优化器。批量大小社会为512。批量标准化应用在了深度网络中，并且将梯度裁剪常数（gradient clip norm）设为100.

*正则化*。我们使用early stopping机制，因为我们使用L2正在和dropout都不起作用。

*超参数*。我们汇报了对隐含层个数，隐含层大小，初始学习率以及交叉层个数进行grid search的结果。隐含层的个数是从2到5，隐含层的大小是从32到1024.对于DCN，交叉层的个数是从1到6,。初始学习率从0.0001到0.001，每次增加0.0001。所有的实验都使用了early stopping，训练步数设为150000，过拟合发生的时候就会提前停止。

**4.3 模型比较**
我们将DCN和5种模型进行了比较：没有交叉网络的DCN模型（DNN），逻辑回归（LR），因式分解机（FMs），Wide&Deep模型（W&D）和深度交叉模型（DC）。

*DNN*。嵌入层、输出层以及过程中的超参数都是用与DCN一致的。唯一和DCN不用的就是没有交叉层。

*LR*。我们使用Siby1——一个大型的机器学习系统来区分逻辑回归。整数型特征会被离散到一个对数尺度。交叉特征将会由一个精致且复杂的特诊供选择工具来进行筛选。所有的单特征是都会被使用。

*FM*。我们使用了带有特定细节的FM模型。

*W&D*。不同于DCN，它的宽部分作为输入原始稀疏特征，并且依赖于穷举和知识域来选取有预测价值的交叉特征。我们跳过了这一块的比较因为没有比较好的方法来选择交叉特征。

*DC*。相比于DCN，DC没有显示地构造交叉特征。它主要依靠堆叠和残差项来隐式地创造交叉特征。我们使用和DCN相同的嵌入层，紧接着是另一个ReLu层来生成输入数据到残差单元系列中。残差单元的个数一半设为1到5之间，输入维度和交叉维度一般是从100到1026。

### **4.4 模型表现**
在这一部分，我们首先会列出不同模型在对数损失下的最好的结果，然后我们会将DCN和DNN进行仔细对比，之后，我们再进一步分析引入交叉网络的效果。

**不同模型的表现**。不同模型的对数损失的最好测试结果都列在了表1中。最优的超参数设置是DCN模型有2个深层且大小为1024和6个交叉层，DNN则有大小为1024的5层网络，DC模型有5个输入维度为424交叉维度为537的残差单元，LR模型有42个交叉特征。最终发现最深的交叉结构获得了最好的表现结果，这表明交叉网络中高次的特征交叉项是有用的。如我们所见，DCN要远好于所有其他的模型。特别地，它优于最先进的DNN模型，而且仅仅是相对于DNN用了40%的内存消费。

*表1 不同模型的最优对数损失*


| Model | DCN | DC | DNN | FM | LR |
|:---:|:---:|:---:|:---:|:---:|:---:|
| Logloss | **0.4419** | 0.4425 | 0.4428 | 0.4464 | 0.4474 |


对于每个模型的最优超参数设置，我们也汇报了10次不同对数损失测试结果的均值和标准差：
$DCN:0.4422 \pm 9 \times 10^{-5}$
$DNN:0.4430 \pm 3.7 \times 10^{-4}$
$DC:0.4430 \pm 4.3 \times 10^{-4}$。
如我们如看到的，DCN一致的大幅优于其他模型。

**DCN和DNN之间的比较**。考虑到交叉网络仅仅额外引入了O(d)个参数，我们就将DCN和它——一个传统的DNN进行比较，并且将实验结果展现出来尽管存在较大的内存预算和损失公差。

接下来，我们将会汇报一定数量参数的损失数据，它们都是在所有的学习率和模型结构上得到的最好的验证集的损失。嵌入层的参数个数被省略了，因为在我们所有模型的计算中这一部分保持不变。

表2展示了要获得一个达到预期对数损失阈值的模型所需要的最少的参数个数。从表2中我们可以看出DCN的内存有效性要比单一的DNN模型高出近一个量级，这得益于交叉网络能够有效地学习到有限次的特征交叉项。

*表2 要获得一个达到预期对数损失阈值的模型所需要的最少的参数个数*


| Logloss | 0.4430 | 0.4460 | 0.4470 | 0.4480 |
|:---:|:---:|:---:|:---:|:---:|
| DNN | 3.2E6 | 1.5E5 | 1.5E5 | 7.8E4 |
| DCN | 7.9E5 | 7.3E4 | 3.7E4 | 3.7E4 |


表2对比了固定内存预算的神经网络的表现。我们可以看到，DCN一致的比DNN要好。在一个小参数体制里，交叉网络参数的个数和深度网络相比相差无几，但是可以看到明显提升这就表明交叉网络在学习有用特征交叉项中更为有效。在大参数体制中，DNN缩小了一些差距了。然而，DCN仍然要比DNN好一大截，这表明它可以有效地学习到一些很有用的甚至一个大DNN都学不到的特征交叉项。

*表3 不同的内存预算下获得的最好的对数损失*


| 参数 | 5E4 | 1E5 | 4E5 | 1.1E6 | 2.5E6 |
|:---:|:---:|:---:|:---:|:---:|:---:|
| DNN | 0.4480 | 0.4471 | 0.4439 | 0.4433 | 0.4431 |
| DCN | **0.4465** | **0.4453** | **0.4432** | **0.4426** | **0.4423** |


我们从更精确的细节来分析对于给定的DNN模型，引入交叉网络的DCN模型的影响。我们首先对比了拥有同样层数和层大小的DNN和DCN模型的最好表现，然后我们展示了验证集的对数损失是如何随着交叉网络层数的增加而变化的。表4展示了DCN和DNN模型在对数损失上面的区别。在同一实验设定下，从最优的对数损失上看DCN模型一致的优于有相同结构的单一DNN模型。这种对于所有超参数的改进是一致的，降低了参数在初始化和随机优化中的随机性影响。

*表4 DCN和DNN在验证集上的对数损失之间的区别*


| Layers/Nodes | 32 | 64 | 128 | 256 | 512 | 1024 |
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| 2 | -0.28 | -0.10 | -0.16 | -0.06 | -0.05 | -0.08 |
| 3 | -0.19 | -0.10 | -0.13 | -0.18 | -0.07 | -0.05 |
| 4| -0.12 | -0.10 | -0.06 | -0.09 | -0.09 | -0.21 |
| 5 | -0.21 | -0.11 | -0.13 | -0.00 | -0.06 | -0.02 |


图3展示了在随机选择的设置中我们增加交叉层数的改进效果。对于图3中的深度网络，当增加了一个交叉层的时候有一个明显的提升。随着更多的交叉层引入的时候，对于某些模型设置会使得对数损失继续下降，这表明引入交叉项对于预测是有效的；鉴于对于其他模型设置对数损失开始波动甚至出现微幅增加，这就表明高阶的特征交叉项的银如意是没有太大作用的

![D&C-3.jpg](https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/papers/deep%26cross-3.JPG)

### **4.5 非CTR数据集**
我们证明了DCN模型在非CTR预测问题中也表现得很好。我们使用来自UCI提供的森林植被类型（forest covertype）（581012样本和54个特征）和 希格斯粒子（Higgs）（11M样本和28个特征）数据集。数据集随机得被分为训练集（90%）和测试集（10%）。对于超参数进行了梯度搜索。深度网络层数从1到10，大小从50到300.交叉网络层数从4到10。残差单元的个数从1到5，他们的出入维度和交叉维度从50到300。对于DCN，输入向量会被直接喂入交叉网络。

对于森林植被类型数据，DCN在最少的内存消费下获得了最好的测试集准确率0.9740。DNN和DC都是0.9737。DCN最优的超参数设置是8个交叉层且大小为54，6个深度网络层且大小为292，DNN则是有7层大小为292的深度网络层，DC则是有输入维度为271交叉维度为287的4个残差单元。

对于希格斯粒子数据集，DCN模型获得的最好对数损失测试结果是0.4494，而DNN是0.4506。DCN最优的超参数设定是4层大小为28的交叉网络和4层大小为209深度网络层，DNN则是10层大小为196的深度网络层。DCN在仅用了DNN一半的内存情况下依然表现得比其要好。

### **5 结论和未来方向**
区分有效的特征交叉项已经称为了许多预测模型成功的关键。遗憾的是，过程往往需要进行手工特征和穷举。DNN是比较受欢迎的自动特征学习模型；然而，学到的特征是隐式的并且高度非线性的，同时网络并不一定需要很大而且无法学习到某些特征。本文剔除的Deep & Cross Network模型能够处理大的稀疏和密集特征集，并且可以联合传统的深度表示来显示地学习有限次的交叉特征。交叉特征的阶数在每一个交叉层都会增加一。我们的实验结果已经证明了它在系数数据集和密集数据集上都要优于其他最先进的算法，优势体现在模型的准确率和内存使用上。

我们会进一步地在其他模型中探索使用交叉层，使得深度交叉网络能够有效地训练，研究交叉网络在多项式近似中的有效性，并且在优化过程中可以更好地理解深度网络的交叉项。

---