---
title: Wide & Deep Learning for Recommender Systems
date: 2019-06-12
categories:
- 学习笔记
- 论文解析
tags:
- 机器学习
- 推荐系统
- Wide & Deep
mathjax: true
copyright: true
---

[原始论文：Wide & Deep Learning for Recommender Systems](http://scholar.google.com.hk/scholar_url?url=https://dl.acm.org/ft_gateway.cfm%3Fid%3D2988454%26type%3Dpdf&hl=zh-CN&sa=X&scisig=AAGBfm0TVpSA7DpxrGGn23_Zbb27fZpvyQ&nossl=1&oi=scholarr)

## 推荐系统之Wide & Deep机器学习算法

### **摘要**
不包含非线性特征变换的一般线性模型被广泛地应用在具有稀疏输入的大规模回归和分类问题中。通过一个*宽的*交叉积特征变换来实现对特征交叉的记忆是很有效和可解释的，而泛化能力需要更多的特征工程工作。考虑少用特征工程，深度神经网络可以更好地起到品泛化的作用，它会从稀疏的特征中学习到那些低维度看不见的密集嵌入。然而，具有嵌入的深度神经网络很容易过度泛化，并在用户-物品交互稀疏和稠密的时候会推荐一些不太相关的项物品。在本文中，我们提出广泛和深度学习——联合训练宽线性模型和深层神经网络——如此来结合记忆模型和泛化模型的好处从而构成更好的推荐系统。我们在Google Play上制作并评估了该系统，它是一个活跃的商业移动应用商店，上面超过10亿活跃用户和超过一百万个应用程序。在线实验结果表明相对于仅用wide模型和deep模型而言，Wide＆Deep显着增加了app的获取。我们也在TensorFlow中开源了我们的实现。

**关键词**  Wide & Deep学习，推荐系统

<!--more-->

### **1. 介绍**
一个推荐系统可以被看作是一个搜索排序系统，其中输入的请求是一个用户和上下文信息的集合，输出则是一个物品列表的排序。给定一个请求，推荐系统的任务就是在数据集中找到相关的物品，并且根据一定的目标，例如点击和购买，将所有的项目进行排序。

推荐系统的一个挑战是类似于一版的搜索排序问题，就是同时实现记忆和泛化的能力。记忆可以被宽泛地定义为学习物品或特征的频繁共现，并且开发历史数据中可用的相关性。另一方面，泛化是基于相关性的传递性和探索从来没有或很少发生在过去的新特征组合。推荐是基于记忆的，通常更具局部性并且与那些用户已经对其产生过行为的物品直接相关。与记忆相比，泛化倾向于改善推荐物品的多样性。在本文中，我们关注Google Play商店中应用程序推荐的问题，但方法应该适用于通用推荐系统。

对于工业环境中的大规模在线推荐和排序系统，一般的线性模型例如逻辑回归都是被广泛使用的，因为它们是简单，可扩展和可解释的。模型经常是使用one-hot编码的二值化稀疏特征进行训练的。例如，二进制特征“user_installed_app = netflix”，如果用户安装了Netflix那么该特征具有值1。使用通过对稀疏特征进行交叉积变化可以有效地实现记忆功能，例如AND（user_installed_app = netflix，impres-sion_app = pandora），如果用户安装了Netflix后有显示了Pandora，则其值为1。这解释了特征对的共现与目标标签有多么的相关。泛化功能是可以通过使用那些颗粒度较小的特征来添加的，例如AND（user_installed_category = video，impression_category =music），但手动特征工程常常还是需要的。交叉积变化的一个限制是他们没有办法泛化出那些没有出现在训练数据中的请求-物品特征对。

基于嵌入的模型，例如因式分解机或深度神经网络，可以通过对每个请求和物品特征学习出一个低维的嵌入向量来泛化出那些看不到的请求-物品特征对，如此可以减少特征工程的负担。然而，当底层的请求-物品矩阵是稀疏和高秩的时候，想要学习出一个有效的请求和物品的低维表示是非常困难的，例如具有特定偏好的用户或具有狭隘吸引力的商机物品。在这种情况下它与大多数的查询项对之间都是没有交互的，但密集的嵌入将会导致对所有的查询-物品对都有一个非零预测，因此可以过度泛化从而产生不相关的推荐。另一方面，有交叉积特征变换的线性模型在不用特别多参数的情况下可以记住这种“特殊规则”。

在本文中，我们提出Wide & Deep学习框架来在同一个模型中同时完成记忆和泛化的任务，如图1所示，它是通过联合训练出一个线性模型和一个神经网络模型。

这篇文章的主要贡献包括：

* Wide & Deep学习框架是通过联合训练一个有嵌入层的前向神经网络和一个有特征变换的线性模型，如此可以得到一个基于稀疏输入的一般推荐系统模型。
* Wide & Deep推荐系统的实现和评估是在Google Play上完成的，它是一个移动应用程序商店，其上拥有超过十亿的活跃用户和超过一百万的应用程序。
* 我们提供了一个开源的实现，它是通过TensorFlow中的一个高级API实现的。

尽管思想很简单，我们还是证明了Wide & Deep框架极大地提升了移动应用商店的app下载率，并且同时满足训练和高速服务的需求。

![wide&deep-1](https://i.postimg.cc/Ssdx68zn/relate-papers21-1.jpg)

![wide&deep-2](https://i.postimg.cc/nrLc0zv0/relate-papers21-2.jpg)

### **2. 推荐系统概述**
图2显示了app推荐系统的概述。一个查询，可以包括各种用户和用户访问应用商店时会生成的上下文特征。推荐系统返回应用列表（也被称为展示），在这上面用户可以执行一些特定的行为例如点击或购买。这些用户操作，随着查询和展示，都记录在日志中，作为模型的训练数据。

由于数据库中有超过一百万的应用，在查询服务的潜在要求（经常是O(10)毫秒）的条件下，为每个查询都对所有的应用进行打分是难以实现的。因此，在收到一个查询后的第一步是*检索*。检索系统返回一个较短的物品列表，这个列表是使用各种特征对请求的最好匹配，通常是机器学习模型和人为定义规则的一个联合。在降低了候选池之后，排序系统会通过它们打出的分数对所有物品进行排序。这个得分经常是$P(y|x)$，它是表示给定特征x后的定于行为标签y的概率，其中x包括用户的特征（例如国家、语言、人口统计学指标），上下文特征（例如设备、一天中的第几个小时、周几），还有展示特征（例如应用年龄、应用的历史统计）。在本文中，我们聚焦于使用Wide & Deep学习框架的排序模型。

### **3. Wide & Deep学习**
**3.1 Wide部分**
宽模型部分是一个广义线性模型，形式一般为$y = w^T x+b$，如图1左侧所示。y是预测，$x=[x_1,x_2,...,x_d]$是一个d维特征的向量，$w=[w_1,w_2,...w_d]$是模型参数且b是偏置项。特征集合包含原始输入的特征以及经过转换的特征。一个最终演的变换就是*交叉积变换*，如下定义：

$$\phi_k(x)=\prod_{i=1}^d x_i^{c_{ki}} \ , \ c_{ki} \ \in \ \{0,1\} $$

其中$c_{ki}$是一个布尔变量，也就是如果第i个特征是第k个变换$\phi_k$的一部分则其取值为1，否则是0。对于二值特征，当且仅当交叉项的组成特征（例如“gender=female” and “language=en”）全部是1的时候交叉积变换（例如“AND(gender=female, language=en)”）才是1，否则就是0。这就获得了二值特征的交叉项，并且往广义线性模型中增加了非线性。

**3.2 Deep部分**
深模型部分是一个前向神经网络，如图1中右侧所示。对于类别特征，原始输入是特征字符串（例如“language=en”）。每个这种稀疏、高维类别特征都会首先被转换成一个低维并且稠密的实值向量，通常被称为嵌入向量。嵌入层的维度一般在O(10)到O(100)。嵌入向量会被随机的初始化，然后其值会在训练过程中通过最小化最终损失函数来训练。这些低维的稠密嵌入向量会被喂入神经网络前向通过的隐含层中。特别地，每个隐含层的计算是：

$$a^{l+1} = f(W^{(l)}a^{(l)} + b^{(l)})$$

其中l是层数，f是激活函数，经常被设成整数线性单元(ReLUs)。$a^{(l)}, b^{(l)}和W^{(l)}$分别是激活项、偏置项和模型第l层的权重项。

**3.3 Wide & Deep模型的联合训练**
宽模型部分和深模型部分会被用加权求和联合到一起，然后输出部分会进行对数概率变换后作为预测结果，在这之后一般会喂入一个普通的逻辑损失函数中进行联合训练。注意到*联合训练*和*合并*之间是有区别的。合并，一般是每个模型独自分开训练互不干扰，他们的预测结果只在推断的时候才会联合，训练过程中并不会。相反，联合训练会同时优化所有的参数，是通过在训练的过程中汇总获取宽模型和深模型的所有参数进行计算。模型大小的含义：对于模型合并，由于训练是分开的，每个单独的模型大小通常需要比较的大（例如有特别多的特征和变换）从而来得到
一个合理准确的起作用的模型合并。相比之下，对于联合训练中的宽模型部分只需要去实现深度模型的弱势部分就可以了，所以它有一个很小量级的交叉积特征变换，而不是一个全量的宽模型。

Wide & Deep模型的联合训练是通过使用小批量随机优化从输出层同时进行宽模型和深模型的反向梯度传播来完成的。在实验中，我们使用Follow-the-regularized-leader (FTRL)算法和L1正则项作为宽模型部分的优化器，同时AdaGrad作为深度部分的优化器。

图1中间部分展示了联合模型。属于逻辑回归的问题，模型的预测结果为：

$$P(Y=1|x)=\sigma(w_{wide}^T [x,\phi (x)] + w_{deep}^T a^{(l_f)} +b)$$

其中Y是二值类别标签，$\sigma(\cdot)$是sigmoid函数，$\phi(x)$是原始特征x的交叉积变换，b是偏置项。$w_{wide}$是宽模型权重参数向量，$w_{deep}$是应用在最终激活项$a^{(l_f)}$的的权重参数
。

### **4. 系统实现**
应用推荐管道的实现包含三个步骤：数据生成，模型训练，以及模型服务，如图3中所示。

![wide&deep-3](https://i.postimg.cc/L5047pDR/relate-papers21-3.jpg)

**4.1 数据生成**
在这个步骤，在一个时期的用户和应用展示数据被用来生成训练数据。每个样本对应于一次展示。标签就是应用获取：1代表展示的应用被安装了，否则为0。

词汇表，是用来将类别特征中的字符串匹配成整数型IDs的，也是在这一步骤中生成的。系统为所有的字符型特征计算IDs的空间，其中特征只计算那些出现次数超过最小次数的。连续型的实值特征会被标准化到[0,1]，方法是将特征值x匹配到该特征的累积分布函数$P(X\leq x)$，分成了$n_q$分位数。第i个分位数标准化后的值是$\frac{i-1}{n_q-1}$。分位数的边界是在数据生成过程中计算的。

![wide&deep-4](https://i.postimg.cc/BZ2dVshC/relate-papers21-4.jpg)

**4.2 模型训练**
我们在实验中使用的模型结构如图4所示。在训练中，我们的输入层接手输入数据和词汇表，然后用一个标签一起生成稀疏的和密集的特征。宽模型部分由用户安装应用和展示应用做交叉积变换得到。对于模型的深度部分，一个32维的嵌入向量是从每个类别型特征中学到的。我们将所有的嵌入向量串联在一起形成一个密集的特征，这就构成了一个接近1200维的密集向量。串联的向量接着会被喂入3个ReLU网络层，最优通过逻辑层输出单元。

Wide & Deep模型是在5000亿个样本上训练得到的。每一次一个新的训练数据集到达时，模型需要被再次训练。然而，每次再训练花费的时间都是特别昂贵的计算成本同时新数据更新模型将会产生一定的延迟。为了战胜这个挑战，我们实现一个热启动的系统，它是用前一个模型的嵌入层和线性模型权重参数来初始化本次新模型。

**4.3 模型服务**
每一次模型被训练完成和确定之后，我们将其加载到模型服务中。对于每次请求，服务会从应用检索系统中接受到一个应用候选集和用户特征来对背个应用进行打分。然后，应用将会根据得分从高到低进行排序，我们将会按照此顺序将应用展示给用户。分数将会通过运行一个Wide & Deep模型的前向推断得到。

为了在10ms的需求下服务每一次请求，我们使用多线程并行来优化表现，它是通过并行运行小批量实现的而不是在一个但线程中对所有的候选应用进行打分的。

### **5. 实验结果**
为了评估Wide & Deep学习在实际推荐系统中的有效性，我们运行了一个实验并在多方面对系统进行了评估：应用获取和服务表现。

*表1：不同模型的离线和在线的效果矩阵。在线的获取Gain值是相对于对照组的。*


| Model | Offline AUC | Online Acqusition Gain |
| :--- | :---: | :---: |
| Wide (control) | 0.726 | 0% |
| Deep | 0.722 | +2.9% |
| Wide & Deep | 0.728 | +3.9% |


**5.1 应用获取**
我们在一个A/B测试框架中进行了3周的在线实验。对于对照组，随机选择1%的用户并使用之前的排序模型来生成推荐排序，先前的模型是一个高度优化后的仅有宽度逻辑回归的模型，它有很丰富的交叉积特征变换。对于实验组，我们随机选取另1%的用户并用Wide & Deep模型来生成推荐排序，训练数据使用同样的特征集合。如表1中所示，相对于对照组，Wide & Deep提高了主页面上的应用获取率大约+3.9%（统计显著）。结果也和另一1%用户组进行对比，这一组仅仅使用了深度模型结构和相同的特征，同样的Wide & Deep模型有1+%的一个收益（统计显著的）。

除了线上实验，我们也展示了离线对抗集的AUC。然而Wide & Deep有一个略高的离线AUC，对在线流量的影响更为显着。一个可能的原因是离线数据集的展示和标签是固定的额，而在线系统可以通过混合记忆和泛化来生成新的探索性的推荐，并且可以从新的用户反馈中学习到更多。

**5.2 服务表现**
在面临我们商业性移动应用商店的时候，高级别流量高吞吐量和低延迟的服务要求是一个挑战。在流量巅峰，我们的推荐服务在每秒内对超过1000万个应用进行打分。在单线程的时候，在一个单批量中对所有候选app进行打分将花费31毫秒。我们使用多线程实现，并将每一批量分成更小的批量，这显著地降低用户端延迟到14毫秒（包括服务高峰期），结果如表2所示。

*表2：批量大小和线程个数的服务延迟对比*


| Batch size | Number of Threads | Serving Latency (ms) |
| :---: | :---: | :---: |
| 200 | 1 | 31 |
| 100 | 2 | 17 |
| 50 | 4 | 4 |


### **6. 相关工作**
将带有交叉积特征变换的宽线性模型和带有密集嵌入的深度神经网络模型联合到一起的想法是受到前人工作启发的，例如因式分解机这种加入了泛化能力的线性模型，它通过将两个变量之间的交叉项分解成两个变量之间的点积。在本文中，我们通过在嵌入层通过神经网络之间的时候而不是点积来学习更高地非线性交叉项从而达到扩展模型的能力。

在语言模型中，循环神经网络（RNNs）和带有n-gram特征最大熵模型的联合训练已经被提出来了，通过学习输入层和输出层之间的直接权重能够显著地降低RNN的复杂度（例如隐含层的大小）。在计算机视觉中，深度残差学习已经被用于降低训练深度模型的困难度，并且通过跨越一层或多层的捷径连接达到了提高准确率的效果。带有图模型神经网络的联合训练已经被应用在了从图片中评估人类姿势。在我们提出的这个前向神经网络和线性模型的联合训练的工作中，在稀疏特征和输出单元之间带有直接连接，这是为了通用化带有稀疏输入数据的推荐和排序问题。

在推荐系统文献中，协同深度学习已经通过结合内容信息的深度学习和评分矩阵的协同过滤被探索了。同样也有很多关于移动应用推荐系统的先前工作，例如将CF用在用户的应用使用记录上的AppJoy。不同于基于CF的或者基于内容方法的这些先前工作，我们的推荐系统是在用户和展示数据上联合训练Wide & Deep模型。

### **7. 结论**
记忆和泛化对于推荐系统来说都很重要。宽线性模型通过使用交叉积特征变换能够有效的基于稀疏特征之间的交叉项，而深度神经网络可以通过低维嵌入层来泛化出那些重要确又看不见的特征交叉项。我们呈现的Wide & Deep学习框架是为了联合这两种模型的各自长处。我们在Google Play这个大规模的商业应用商店上对我们这个框架进行了产品化和评估。在线实验的结果证明了相对于仅用宽和深度模型来说，Wide & Deep模型在应用获取上带来了显著地提升。

---