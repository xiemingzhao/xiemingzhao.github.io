<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">


  <link rel="manifest" href="/images/manifest.json">


  <meta name="msapplication-config" content="/images/browserconfig.xml" />



  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="小火箭的博客" type="application/atom+xml" />






<meta name="description" content="夜深了，差不多该休息了">
<meta property="og:type" content="website">
<meta property="og:title" content="小火箭的博客">
<meta property="og:url" content="https://www.xiemingzhao.com/index.html">
<meta property="og:site_name" content="小火箭的博客">
<meta property="og:description" content="夜深了，差不多该休息了">
<meta property="og:locale">
<meta property="article:author" content="小火箭">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://www.xiemingzhao.com/"/>





  <title>小火箭的博客</title>
  








<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>


<script type="text/javascript"
color="0,0,0" opacity='0.5' zIndex="-1" count="150" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>


<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>
    
    <a href="https://github.com/xiemingzhao"><img style="position:absolute;top:0;right:0;border:0;" src="https://github.blog/wp-content/uploads/2008/12/forkme_right_darkblue_121621.png?resize=149%2C149" class="attachment-full size-full" alt="Fork me on GitHub" data-recalc-dims="1"></a>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">小火箭的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">愿世界和平！！！</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            公益404
          </a>
        </li>
      
        
        <li class="menu-item menu-item-guestbook">
          <a href="/guestbook/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-comments"></i> <br />
            
            留言板
          </a>
        </li>
      
        
        <li class="menu-item menu-item-others">
          <a href="/others/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-folder"></i> <br />
            
            其他
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.xiemingzhao.com/posts/ubsmodel.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.postimg.cc/vBxZQfvz/img-0182.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小火箭的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/posts/ubsmodel.html" itemprop="url">精排序列建模经典方案综述</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2024-12-21T00:00:00+08:00">
                2024-12-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/" itemprop="url" rel="index">
                    <span itemprop="name">算法总结</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/%E7%B2%BE%E6%8E%92%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index">
                    <span itemprop="name">精排模型</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/posts/ubsmodel.html#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/posts/ubsmodel.html" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                  <span class="post-meta-divider">|</span>
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  8.7k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  35
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>在互联网应用的精排模型中，往往在<code>特征工程</code>、<code>样本构建</code>、<code>Loss 设计</code>、<code>模型结构</code>等方向进行迭代优化。其中，涉及特征与结构的<strong>用户行为序列建模</strong>是近几年的热点之一。</p>
<p>序列建模一般有2大方向：</p>
<ul>
<li>检索的序列更长；</li>
<li>建模的更精准。</li>
</ul>
<p>下面梳理近几年的经典序列建模方案，基本也是围绕上述 2 大方向进行不断优化的。</p>
<h2 id="1-DIN"><a href="#1-DIN" class="headerlink" title="1 DIN"></a>1 DIN</h2><h3 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1 概述"></a>1.1 概述</h3><p>论文：<a href="https://arxiv.org/abs/1706.06978">DIN: Deep Interest Network for Click-Through Rate Prediction</a><br>来源：2018，阿里</p>
<p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel0.png" alt="ubsmodel0"></p>
<p><strong>思想：为了序列建模的更精准，通过 DIN 的 Attention 结构来替换 Base Model 的 Sum-Pooling 结构。</strong></p>
<h3 id="1-2-方案"><a href="#1-2-方案" class="headerlink" title="1.2 方案"></a>1.2 方案</h3><h4 id="1-2-1-DIN-的序列检索结构"><a href="#1-2-1-DIN-的序列检索结构" class="headerlink" title="1.2.1 DIN 的序列检索结构"></a>1.2.1 DIN 的序列检索结构</h4><p>历史行为中的不同物品对候选物品影响应该是有差异的，<code>Attention</code> 结构正是想打破 <code>Sum-Pooling</code> 的这种缺点。即在 <code>Sum-Pooling</code> 前，基于 <code>Activation Unit</code> （图右上）算出 <code>Weight</code>，然后做 <code>Weighted-Pooling</code>。</p>
<p>值得注意的是，<code>Activation Unit</code> 中的 <code>Out Product</code> 部分，在实践中往往如下处理（供参考），主要是为了增加非线性：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.concat([query, seq, query - seq, query * seq])</span><br></pre></td></tr></table></figure></p>
<h4 id="1-2-2-Dice-替代-PReLU"><a href="#1-2-2-Dice-替代-PReLU" class="headerlink" title="1.2.2 Dice 替代 PReLU"></a>1.2.2 Dice 替代 PReLU</h4><p><code>PReLU</code> 激活函数更容易出现参数更新缓慢甚至梯度消失的问题，论文使用更具泛化性的 <code>Dice</code> 激活函数。其二者的公式和函数图像如下所示：</p>
<p><code>PReLU</code>：</p>
<script type="math/tex; mode=display">f(s)= \begin{cases} s & \mathrm{if~}s>0 \\ \alpha s & \mathrm{if~}s\leq0. & \end{cases}=p(s) \cdot s+(1-p(s))\cdot\alpha s</script><p>其中， $p(s)=I(s &lt; 0)$ 为指示函数</p>
<p><code>Dice</code>：</p>
<script type="math/tex; mode=display">f(s)=p(s) \cdot s+(1-p(s)) \cdot \alpha s,p(s)=\frac{1}{1+e^{-\frac{s-E[s]}{\sqrt{Var[s]+\epsilon}}}}</script><p>其中，$\epsilon$一般取$10^{-8}$。可以发现：</p>
<ul>
<li><code>Dice</code> 是 <code>PReLu</code> 的推广，当 E[s] = 0，Var[s]=0 时，Dice 退化为 PReLU;</li>
<li>其核心思想是根据输入数据的分布自适应地调整校正。</li>
</ul>
<p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel1.png" alt="ubsmodel1"></p>
<h4 id="1-2-3-GAUC-替代-AUC"><a href="#1-2-3-GAUC-替代-AUC" class="headerlink" title="1.2.3 GAUC 替代 AUC"></a>1.2.3 GAUC 替代 AUC</h4><p><code>AUC</code> 代表模型对样本整体的排序能力，不区分用户类型，比如高低活。</p>
<p>而实际线上应用的时候，不同用户之间是不需要对比的，<strong>更重要的是：同一个用户下，不同 item 能否区分准确。</strong></p>
<p>故论文提出了 <code>GAUC</code>：</p>
<script type="math/tex; mode=display">\mathrm{GAUC}=\frac{\sum_{i=1}^n\#impression_i\times\mathrm{AUC}_i}{\sum_{i=1}^n\#impression_i}</script><p>其中，$n$ 表示 User 的数量。</p>
<blockquote>
<p>实际中，建议 AUC 和 GAUC 结合一起判断，且后者一般要求 user 级别正负样本兼有。</p>
</blockquote>
<h4 id="1-2-4-Mini-batch-Aware-Regularization"><a href="#1-2-4-Mini-batch-Aware-Regularization" class="headerlink" title="1.2.4 Mini-batch Aware Regularization"></a>1.2.4 Mini-batch Aware Regularization</h4><p>是一种 <code>Adaptive</code> 的正则化方法。行为物品的参数空间大，使得模型容易过拟合，但传统的 L2 正则会对所有参数应用，效率低。<br>故论文提出了 <code>Mini-batch Aware Regularization</code> 方案：</p>
<script type="math/tex; mode=display">\begin{aligned}
&
L_{2}\left(w\right)=\left|\left|w\right|\right|^{2}=\sum_{j=1}^{K}\left|\left|w_{j}\right|\right|^{2}=\sum_{\left(x,y\right)\in S}\sum_{j=1}^{K}\frac{I\left(x_{j}\neq0\right)}{n_{j}}\left|\left|w_{j}\right|\right|^{2} \\
&
=\sum_{j=1}^{K}\sum_{m=1}^{B}\sum_{(x,y)\in B_{m}}\frac{I(x_{j}\neq0)}{n_{j}}||w_{j}||^{2} \\
&
=\sum_{j=1}^{K}\sum_{m=1}^{B}\frac{max_{(x,y)\in B_{m}}[I(x_{j}\neq0)]}{n_{j}}\left|\left|w_{j}\right|\right|^{2}
\end{aligned}</script><p>其中，</p>
<ul>
<li>K：特征空间的维度；</li>
<li>S：全局样本；</li>
<li>B：mini-batch 的个数；</li>
<li>$x_j$：每个样本第 j 个特征值；</li>
<li>$I(x_j \ne 0)$：mini-batch 内，第 j 个特征值均为0的时候该值为0，否则为1；</li>
<li>$n_j$：样本中第 j 个特征出现的次数。</li>
</ul>
<p>最后一步，<strong>将所有的 $w_j$ 相加转为了只加最大（非0）的一次</strong>，如此高频（更重要）的特征正则权重就会变小，衰减慢一些。</p>
<h3 id="1-3-小记："><a href="#1-3-小记：" class="headerlink" title="1.3 小记："></a>1.3 小记：</h3><ul>
<li>DIN 优化了序列中不同 item 的权重、激活函数、正则方案以及评估指标；</li>
<li>但没有考虑序列先后关系、兴趣变化，且一般仅适用于短序列（论文中 14 天，序列长平均 35）。</li>
</ul>
<h2 id="2-DIEN"><a href="#2-DIEN" class="headerlink" title="2 DIEN"></a>2 DIEN</h2><h3 id="2-1-概述"><a href="#2-1-概述" class="headerlink" title="2.1 概述"></a>2.1 概述</h3><p>论文：<a href="https://arxiv.org/pdf/1809.03672">Deep Interest Evolution Network</a><br>来源：2019，阿里</p>
<p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel2.png" alt="ubsmodel2"></p>
<p><strong>思想：引入 GRU 构建抽取兴趣层，使用 AUGRU 结构来做兴趣演化层，意在改善 DIN 没有考虑的行为先后关系和兴趣演变过程。</strong></p>
<h3 id="2-2-方案"><a href="#2-2-方案" class="headerlink" title="2.2 方案"></a>2.2 方案</h3><h4 id="2-2-1-兴趣提取层（Interest-Extractor-Layer）"><a href="#2-2-1-兴趣提取层（Interest-Extractor-Layer）" class="headerlink" title="2.2.1 兴趣提取层（Interest Extractor Layer）"></a>2.2.1 兴趣提取层（Interest Extractor Layer）</h4><p>实际中，行为序列可能比较长（14天平均30+），用户兴趣也在不断变迁。所以使用 <code>GRU</code> 来对用户行为之间的依赖进行建模。</p>
<p>选择 GRU 的原因是：</p>
<ul>
<li>克服了 RNN 的梯度消失问题；</li>
<li>速度比 LSTM 快。</li>
</ul>
<p>结合模型图，GRU 的结构如下所示：</p>
<script type="math/tex; mode=display">\begin{aligned}
& \mathbf{u}_{t}=\sigma(W^{u}\mathbf{i}_{t}+U^{u}\mathbf{h}_{t-1}+\mathbf{b}^{u}), \\
& \mathbf{r}_{t}=\sigma(W^{r}\mathbf{i}_{t}+U^{r}\mathbf{h}_{t-1}+\mathbf{b}^{r}), \\
& \tilde{\mathbf{h}}_{t}=\mathrm{tanh}(W^{h}\mathbf{i}_{t}+\mathbf{r}_{t}\circ U^{h}\mathbf{h}_{t-1}+\mathbf{b}^{h}), \\
& \mathbf{h}_{t}=(\mathbf{1}-\mathbf{u}_{t})\circ\mathbf{h}_{t-1}+\mathbf{u}_{t}\circ\tilde{\mathbf{h}}_{t}
\end{aligned}</script><p>其中，</p>
<ul>
<li>$\sigma$是 simoid 激活函数；</li>
<li>$\circ$是元素乘；</li>
<li>$W^u,W^r,W^h \in \mathbb{R}^{n_H \times n_I}$，$U^z,U^r,U^h \in n_H \times n_H$，$n_H,n_I$分别是隐层和输入层的 size；</li>
<li>$i_t = e_b[t]$是序列中第 t 个物品的 embedding，也是 GRU 的输入。</li>
</ul>
<h4 id="2-2-2-辅助-Loss"><a href="#2-2-2-辅助-Loss" class="headerlink" title="2.2.2 辅助 Loss"></a>2.2.2 辅助 Loss</h4><p>使用辅助 Loss 想要解决的问题：</p>
<ul>
<li>GRU 只能学习行为间的依赖，不能有效地学习用户兴趣；</li>
<li>$L_{target}$ 只包含最终的目标信息，GRU 的隐层没有有效地监督信息；</li>
<li>辅助 item embedding 的学习更有效的信息。</li>
</ul>
<p>具体做法（结合上图）：用户 $i$ 的序列为 $b$，$t$ 时刻的$e_b^i[t]$对应的隐层状态为$h_t$，给其找一个正样本和一个负样本来构建辅助 Loss。</p>
<p><code>正样本</code>：点击序列的下一个 item，记为$e_b^i[t+1]$；<br><code>负样本</code>：除正样本$e_b^i[t+1]$之外的随机采样，记为$\hat e_b^i[t+1]$。</p>
<p>则辅助 Loss 为：</p>
<script type="math/tex; mode=display">L_{aux}= - \frac{1}{N}(\sum_{i=1}^N\sum_{t} \log \sigma {(h_t^i,e_b^i[t+1])} + \log (1 - \sigma{(h_t^i,\hat e_b^i[t+1]))})</script><p>故整体 Loss 为：</p>
<script type="math/tex; mode=display">L=L_{target}+α \ast L_{aux}</script><h4 id="2-2-3-兴趣演进层（Interset-Evolving-Layer）"><a href="#2-2-3-兴趣演进层（Interset-Evolving-Layer）" class="headerlink" title="2.2.3 兴趣演进层（Interset Evolving Layer）"></a>2.2.3 兴趣演进层（Interset Evolving Layer）</h4><p>该层是对 <code>target item</code> 相关的兴趣演化进行建模，使用的是带注意力更新门的 <code>GRU</code>，称为 <code>AUGRU</code>，即通过使用兴趣状态和 target item 计算得到的注意力权重。计算方式如下：</p>
<script type="math/tex; mode=display">a_t = \frac{\exp{(h_t \cdot W \cdot e_a)}}{\sum_{j = 1}^T \exp{(h_j \cdot W \cdot e_a)}}</script><p>其中，$e_a$ 是 target Ad 的 embedding。</p>
<p>针对此注意力，作者有 3 种用法：</p>
<ol>
<li><code>AIGRU</code>（GRU with attentional input）<br>直接与 Interset Evolving Layer 的输入相乘，即$i_t’ = h_t \asrt a_t$。</li>
<li><code>AGRU</code>（Attention based GRU）<br>替换 GRU 种的更新门，即 $h<em>t’ = (1 - a_t) \ast h</em>{t-1}’ + a_t \ast \tilde h_t’$。</li>
<li><code>AUGRU</code>（GRU with attentional update gate）<script type="math/tex; mode=display">\begin{aligned}
& \tilde{\mathbf{u}}_{t}=a_{t}*\mathbf{u}_{t}, \\
& \mathbf{h}_{t}=(1-\tilde{\mathbf{u}}_{t})\circ\mathbf{h}_{t-1}+\tilde{\mathbf{u}}_{t}\circ\tilde{\mathbf{h}}_{t}
\end{aligned}</script></li>
</ol>
<p>这里我们将公式各项表示跟前述的 GRU 做了对齐利于理解，实际上就是构建了 $\tilde{\mathbf{u}}<em>{t}$ 来代替 $\mathbf{u}</em>{t}$。</p>
<h3 id="2-3-小记"><a href="#2-3-小记" class="headerlink" title="2.3 小记"></a>2.3 小记</h3><ul>
<li><code>GRU</code> 和 <code>AUGRU</code> 增加了模型的复杂度，一定程度能够提高对兴趣的学习，辅助 Loss 的作用不可忽视；</li>
<li>结构复杂带来的算力瓶颈是一大要害，作者提到作者提到了 GPU 优化、并行化、模型压缩等缓解方式，整体依然只能扛住 30-50 长度的序列；</li>
<li>作者有提过，DIN 的 Attention 在序列变长（&gt;100）后容易出现信息淹没，此处 GRU 也做了不少优化；</li>
<li>该模型叠加了不少复杂结构，是否真的有效用，这可能需要以具体场景中的时间为准。</li>
</ul>
<h2 id="3-BST"><a href="#3-BST" class="headerlink" title="3 BST"></a>3 BST</h2><h3 id="3-1-概述"><a href="#3-1-概述" class="headerlink" title="3.1 概述"></a>3.1 概述</h3><p>论文：<a href="https://arxiv.org/pdf/1905.06874">Behavior Sequence Transformer for E-commerce Recommendation in Alibaba</a><br>来源：2019，阿里</p>
<p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel3.png" alt="ubsmodel3"></p>
<p><strong>思想：将当时比较火热的 Transformer 种的 Multi-head Self-attention 结构应用在用户行为序列建模中。</strong></p>
<h3 id="3-2-方案"><a href="#3-2-方案" class="headerlink" title="3.2 方案"></a>3.2 方案</h3><p><code>Transformer</code> 模型的细节这里不过多介绍，其 <code>Encoder</code> 中的每个 <code>Layer</code> 一般由 4 个子层构成，如上图右上。作者核心就是应用了这一部分。简单阐述为下面2块：</p>
<ul>
<li>将用户序列和 target item 看作整个 sequence 作为 Transformer Layer 的输入；</li>
<li>引入时序位置信息。</li>
</ul>
<p>其中，时序位置信息构建如下：</p>
<script type="math/tex; mode=display">pos(v_i) = t(v_t) - t(v_i)</script><p>实际上就是序列中每个点击行为距离 targte item 的时间差。</p>
<h3 id="3-3-小记"><a href="#3-3-小记" class="headerlink" title="3.3 小记"></a>3.3 小记</h3><p>客观上，这篇文章或多或少引起了一些<em>争议：是不是为了蹭 Transformer 热度，水分大不大。</em></p>
<p>至于到底如何，每个算法工程师可能都有自己的见解。这里我们罗列一些相对比较重要的疑问，供思考和讨论：</p>
<ol>
<li>Transformer 后做 concat 入模，是否合适？</li>
<li>为了做到上述1，限制了序列长度为20，是否具有效性？</li>
<li>时序位置信息的引入，在单位、分桶等处理细节上没有披露。</li>
<li>为何选择将 target item 并入一起做 Multi-head Self-attention，而没有做 Target Attention？</li>
</ol>
<h2 id="4-DSIN"><a href="#4-DSIN" class="headerlink" title="4 DSIN"></a>4 DSIN</h2><h3 id="4-1-概述"><a href="#4-1-概述" class="headerlink" title="4.1 概述"></a>4.1 概述</h3><p>论文：<a href="https://arxiv.org/pdf/1905.06482">Deep Session Interest Network for Click-Through Rate Prediction</a><br>来源：2019，阿里</p>
<p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel4.png" alt="ubsmodel4"></p>
<p><strong>思想：用户在不同的 session 中行为差异明显，这是 DIEN 等没有考虑的，DSIN 中将序列分成多个 session 来处理。</strong></p>
<h3 id="4-2-方案"><a href="#4-2-方案" class="headerlink" title="4.2 方案"></a>4.2 方案</h3><p><strong><code>DSIN</code> 网络结构分为四层</strong></p>
<h4 id="4-2-1-Session划分层（Session-Division-Layer）"><a href="#4-2-1-Session划分层（Session-Division-Layer）" class="headerlink" title="4.2.1 Session划分层（Session Division Layer）"></a>4.2.1 Session划分层（Session Division Layer）</h4><p>Session 的划分方法：用户在行为序列中，超过半小时间隔处作为 Session 的切分点。</p>
<h4 id="4-2-2-Session兴趣抽取层（Session-Interest-Extractor-Layer）"><a href="#4-2-2-Session兴趣抽取层（Session-Interest-Extractor-Layer）" class="headerlink" title="4.2.2 Session兴趣抽取层（Session Interest Extractor Layer）"></a>4.2.2 Session兴趣抽取层（Session Interest Extractor Layer）</h4><p>引入 <code>bias encoding</code>，如下所示：</p>
<script type="math/tex; mode=display">\mathbf{BE}_{(k,t,c)} = \mathbf{w}_k^K + \mathbf{w}_t^T + \mathbf{w}_c^C</script><p>其中，</p>
<ul>
<li>$\mathbf{w}^K \in \mathbb{R} ^{K}$ 是 session 的 bias；</li>
<li>$\mathbf{w}^T \in \mathbb{R} ^{T}$ 是 session 内行为位置的 bias；</li>
<li>$\mathbf{w}^C \in \mathbb{R} ^{d_{model}}$ 是行为序列 item 的 embedding 每个元素的 bias。</li>
</ul>
<p>最终的序列 embedding 为：</p>
<script type="math/tex; mode=display">\mathbf{Q} = \mathbf{Q} +\mathbf{BE}</script><p><strong>注意：虽然$\mathbf{BE}$维度是$K \times T \times d<em>{model}$，但实际上参数个数为$K + T + d</em>{model}$。</strong></p>
<p>最后针对用户序列应用 Multi-head Self-attention 来抽取兴趣，该结构不再赘述，输出记为 $\mathbf{I}$。</p>
<h4 id="4-2-3-Session兴趣交互层（Session-Interest-Interacting-Layer）"><a href="#4-2-3-Session兴趣交互层（Session-Interest-Interacting-Layer）" class="headerlink" title="4.2.3 Session兴趣交互层（Session Interest Interacting Layer）"></a>4.2.3 Session兴趣交互层（Session Interest Interacting Layer）</h4><p>对用户 Session 的兴趣迁移进行建模，作者使用了 <code>Bi-LSTM</code> 结构，该层的最终隐层状态是前后向隐层状态的融合：</p>
<script type="math/tex; mode=display">\mathbf{H}_t=\overrightarrow{\mathbf{h}_{ft}} \oplus \overleftarrow{\mathbf{h}_{bt}}</script><p>其中，$\overrightarrow{\mathbf{h}<em>{ft}}$ 和 $\overleftarrow{\mathbf{h}</em>{bt}}$ 分别是前向和后向的 LSTM 隐层输出状态。而 $\oplus$ 文中没有明确解释，但我们结合模型图符号惯例以及 Bi-LSTM 的原理，很容易理解其应该是 <code>concat</code>。</p>
<h4 id="4-2-4-Session兴趣激活层（Session-Interest-Activating-Layer）"><a href="#4-2-4-Session兴趣激活层（Session-Interest-Activating-Layer）" class="headerlink" title="4.2.4 Session兴趣激活层（Session Interest Activating Layer）"></a>4.2.4 Session兴趣激活层（Session Interest Activating Layer）</h4><p>该层主要就是通过 2 个 <code>Activation Unit</code> 结构来抽取和 target Itemv相关的 Session 兴趣 embedding。可以看到，图中 <code>Activation Unit</code> 是一个经典的 <code>Target Attention</code> 结构。</p>
<p>黄色的部分（更浅层）：</p>
<script type="math/tex; mode=display">\begin{aligned}
& a_k^{I}=\frac{\exp(\mathbf{I}_{k}\mathbf{W}^{I}\mathbf{X}^{I}))}{\sum_{k}^{K}\exp(\mathbf{I}_{k}\mathbf{W}^{I}\mathbf{X}^{I})} \\
& \mathbf{U}^{I}=\sum_k^Ka_k^I\mathbf{I}_k
\end{aligned}</script><p>其中，$\mathbf{X}^{I}$ 就是 Query 部分，来自图左侧的 Item Filed 构建的 Embedding，$\mathbf{W}^{I}$是转换矩阵。<br>而对于蓝色部分（更深层），则就是把$\mathbf{X}^{I}$、$\mathbf{W}^{I}$换成对应的深层参数$\mathbf{X}^{H}$、$\mathbf{W}^{H}$，其余计算保持不变。</p>
<h3 id="4-3-小记"><a href="#4-3-小记" class="headerlink" title="4.3 小记"></a>4.3 小记</h3><p><code>DSIN</code> 本身也是循着提升序列检索精度的方向：</p>
<ul>
<li>将序列拆分成不同的 Session 提供了一定的先验信息；</li>
<li>使用 <code>MHTA</code>、 <code>Bi-LSTM</code> 以及 <code>Target Attention</code> 一系列操作，具体有无效用，见仁见智，以具体的时间结果为准。</li>
</ul>
<h2 id="5-MIMN"><a href="#5-MIMN" class="headerlink" title="5 MIMN"></a>5 MIMN</h2><h3 id="5-1-概述"><a href="#5-1-概述" class="headerlink" title="5.1 概述"></a>5.1 概述</h3><p>论文：<a href="https://arxiv.org/pdf/1905.09248">Practice on Long Sequential User Behavior Modeling for Click-Through Rate Prediction</a><br>来源：2019，阿里</p>
<p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel5.png" alt="ubsmodel5"></p>
<p><strong>思想：基于 DIEN 和 DSIN 的优势，MIMN 构建独立的 UIC 模块来更新用户兴趣 embedding，更新只依赖行为 event，不依赖 request。</strong></p>
<h3 id="5-2-方案"><a href="#5-2-方案" class="headerlink" title="5.2 方案"></a>5.2 方案</h3><h4 id="5-2-1-挑战"><a href="#5-2-1-挑战" class="headerlink" title="5.2.1 挑战"></a>5.2.1 挑战</h4><ul>
<li>序列建模中使用的用户行为序列越长，收益越大。（这点相信大多数场景经验都满足）</li>
<li>直接扩增序列会带来显著的 存储问题 和 性能问题。（论文披露：序列150-1k时，存储1T-6T，QPS=500时性能14ms-200m，要求&lt;30ms）</li>
</ul>
<p>核心解决思路如下图：</p>
<ul>
<li>不存储用户原始的行为序列，只存储用户的兴趣 embedding；</li>
<li>用户的兴趣 embedding 是可迭代更新的，并且其只依赖用户行为的 event，独立于 Server，在 request 时直接获取可降低 RT。</li>
</ul>
<p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel6.png" alt="ubsmodel6"></p>
<h4 id="5-2-2-神经元图灵机（Neural-Turing-Machine）"><a href="#5-2-2-神经元图灵机（Neural-Turing-Machine）" class="headerlink" title="5.2.2 神经元图灵机（Neural Turing Machine）"></a>5.2.2 神经元图灵机（Neural Turing Machine）</h4><p>使用记忆参数 $\mathbf{M<em>t}$来存储序列信息，且有 $m$ 个槽位（slot），$\left {\mathbf{M_t}(i) \right } |</em>{i=1}^m$。</p>
<p>其更新和读取主要由下面2部分构成。</p>
<p><strong>1. 记忆读取（Memory Read）</strong><br>控制器生成一个寻址的 <code>key</code> 为 $k_t$，针对所有的 <code>memory slot</code> 计算权重：</p>
<script type="math/tex; mode=display">\mathbf{w}_t^r(i)=\frac{\exp(K(\mathbf{k}_t,\mathbf{M}_t(i)))}{\sum_j^m\exp(K(\mathbf{k}_t,\mathbf{M}_t(j)))},for \ i=1,2,...m</script><p>其中，</p>
<script type="math/tex; mode=display">K\left(\mathbf{k}_t,\mathbf{M}_t(i)\right)=\frac{\mathbf{k}_t^T\mathbf{M}_t(i)}{\|\mathbf{k}_t\|\|\mathbf{M}_t(i)\|}</script><p>最后输出为：</p>
<script type="math/tex; mode=display">\mathbf{r}_t=\sum_i^mw_t^r(i)\mathbf{M}_t(i)</script><p><strong>2. 记忆写入（memory write）</strong><br>首先控制器也会类似 <code>Memory Read</code> 生成一个 $\mathbf{w_t^w}$，此外还会生成加和向量项$\mathbf{a_t}$和衰减向量项$\mathbf{e_t}$。记忆矩阵$\mathbf{M_t}$的更新如下：</p>
<script type="math/tex; mode=display">\mathbf{M_t=(1-E_t)\odot M_{t-1}+A_t}</script><p>其中，</p>
<ul>
<li>$\mathbf{E_t} = \mathbf{w}_t^w \otimes \mathbf{e}_t$；</li>
<li>$\mathbf{A}<em>{\mathbf{t}}=\mathbf{w}</em>{t}^{\mathbf{w}} \otimes \mathbf{a}_{t}$；</li>
<li>$\odot$，$\otimes$ 分别表示向量内积和外积。</li>
</ul>
<h4 id="5-2-3-内存利用率正则（Memory-utilization-regularization）"><a href="#5-2-3-内存利用率正则（Memory-utilization-regularization）" class="headerlink" title="5.2.3 内存利用率正则（Memory utilization regularization）"></a>5.2.3 内存利用率正则（Memory utilization regularization）</h4><p>原始的 <code>NTM</code> 往往有<strong>内存利用不均衡问题，文章的解决方案是：根据不同记忆槽位的写入权重的方差来进行正则。</strong></p>
<script type="math/tex; mode=display">\mathbf{g}_t=\sum_{c=1}^t\mathbf{w}_c^{\tilde{w}}</script><p>如上所示是截止时间步$t$的累积更新权重，其中$\mathbf{w}_c^{\tilde{w}}$如下构建：</p>
<script type="math/tex; mode=display">\begin{aligned}&P_t= softmax(W_g \mathbf{g}_t) \\ & \mathbf{w}_t^{\tilde{w}}=\mathbf{w}_t^wP_t\end{aligned}</script><p>其中，$\mathbf{w}_t^w$是上述提到的原始写入权重，$P_t$是转换矩阵，$W_g$是由下列正则 Loss 学习得到：</p>
<script type="math/tex; mode=display">\begin{aligned}&\mathbf{w}^{\tilde{w}}=\sum_{t=1}^T\mathbf{w}_t^{\tilde{w}},\\&\mathbf{L}_{reg}=\lambda\sum_{i=1}^m\left(\mathbf{w}^{\tilde{w}}(i)-\frac{1}{m}\sum_{i=1}^m\mathbf{w}^{\tilde{w}}(i)\right)^2\end{aligned}</script><h4 id="5-2-4-记忆感知单元（Memory-Induction-Unit）"><a href="#5-2-4-记忆感知单元（Memory-Induction-Unit）" class="headerlink" title="5.2.4 记忆感知单元（Memory Induction Unit）"></a>5.2.4 记忆感知单元（Memory Induction Unit）</h4><p><code>NTM</code> 的 memory 一般是存储<code>原始信息</code>的，而 MIMN 的此模块的设计是<strong>为了捕捉高阶信息</strong>。如下图，<code>UBS</code> 会被分成多个 <code>Channel</code>，即 slot，假设 $m$ 个。<br>那么在第$\mathbf{t}$时间步的时候，会从 m 个 channel 中根据$\mathbf{w}_t^r(i)$选择 topK 个 channel，对于其中的每一个 channel i 按照下述更新：</p>
<script type="math/tex; mode=display">\mathrm{S}_t(i)=\mathrm{GRU}(\mathrm{S}_{t-1}(i),\mathrm{M}_t(i),e_t)</script><p>其中，</p>
<ul>
<li>$\mathrm{M}_t(i)$是 NTM 的第 i 个 memory slot；</li>
<li>$e_t$表示新增行为 item 的 embedding。</li>
</ul>
<p><strong>需要注意：不同 channel 的 GRU 参数是共享的。</strong><br><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel7.png" alt="ubsmodel7"></p>
<h3 id="5-3-小记"><a href="#5-3-小记" class="headerlink" title="5.3 小记"></a>5.3 小记</h3><ul>
<li>开篇的存储问题降到了 2.7T，性能压力降到了 19ms；</li>
<li>但模块上的独立，在效果上是否会有一定的折损，不同场景可能有一定差异；</li>
<li>普适度上也有一定限制，作者提到2点：行为数据较丰富；行为 event 量 &lt; 模型 request 量（否则 UIC 起不到缓解性能的作用）。</li>
</ul>
<p>此外，其团队提到由于资源占用、迭代受限，该框架不久后就放弃了这条路线。</p>
<h2 id="6-SIM"><a href="#6-SIM" class="headerlink" title="6 SIM"></a>6 SIM</h2><h3 id="6-1-概述"><a href="#6-1-概述" class="headerlink" title="6.1 概述"></a>6.1 概述</h3><p>论文：<a href="https://arxiv.org/pdf/2006.05639">Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction</a><br>来源：2020，阿里</p>
<p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel8.png" alt="ubsmodel8"></p>
<p><strong>思想：为了处理更长的行为序列，构建 GSU（泛检索）+ESU（精检索）两阶段的框架，是一个非常有实战价值的做法。</strong></p>
<h3 id="6-2-方案"><a href="#6-2-方案" class="headerlink" title="6.2 方案"></a>6.2 方案</h3><h4 id="6-2-1-挑战"><a href="#6-2-1-挑战" class="headerlink" title="6.2.1 挑战"></a>6.2.1 挑战</h4><ul>
<li>序列越长效果越好，尤其是用户行为活跃度高时，长序列就更重要；</li>
<li>MIMN 处理的序列超过 1k 时效果变差，缺少和 target item 的交互。</li>
</ul>
<p>作者提出了 <code>GSU</code>（General Search Unit） + <code>ESU</code>（Exact Search Unit） 的方案，如上模型图所示，可以说开辟了序列建模又一新范式。</p>
<h4 id="6-2-2-一阶段-GSU（General-Search-Unit，通用搜索单元）"><a href="#6-2-2-一阶段-GSU（General-Search-Unit，通用搜索单元）" class="headerlink" title="6.2.2 一阶段 GSU（General Search Unit，通用搜索单元）"></a>6.2.2 一阶段 GSU（General Search Unit，通用搜索单元）</h4><p>该阶段很明显是要从行为序列中粗筛 topK 个与 target item 相关的 candidate item，作者在这里介绍了2种方法，<code>hard-seach</code> 和 <code>soft search</code>。</p>
<script type="math/tex; mode=display">r_i=\begin{cases}Sign(C_i=C_a)&hard-search\\(W_b\mathbf{e}_i)\odot(W_a\mathbf{e}_a)^T&soft-search&\end{cases}</script><p><strong>1. hard-seach</strong><br>顾名思义，相对比较粗糙但直接有效，<strong>即行为序列中与 target item 具有同类目的就可以作为 candidate item（如模型图中上所示）。</strong></p>
<blockquote>
<p>这里有一个点：类目也是一种泛指，具体用几级类目？能不能用其他维度？都需要根据实际场景来选择。</p>
</blockquote>
<p><strong>但经验上，选择的维度一定要在业务场景中举足轻重，当然也可以是多个</strong>。比如电商的根类目、叶子类目、品牌，内容社区的话题、语言等。</p>
<p><strong>2. soft-search</strong></p>
<p>上述 hard 方式虽然简单直接，<em>但依赖检索类目的质量，相关性无法保障。</em></p>
<p><strong>一个朴素的想法便是：使用 target item 的 embedding 去检索序列中 item emebdding 距离近的 topK。</strong></p>
<p>如上公式所示，</p>
<ul>
<li>$W_b,W_a$ 均是<code>变换矩阵</code>；</li>
<li>$e_a,e_i$ 分别是 target item 和 candidate item 的 embedding；</li>
<li>$\odot$ 表示<code>内积</code>。</li>
</ul>
<p>需要注意，作者提出因短期兴趣和长期兴趣分布有差异，故它们的 item embedding 不能 <code>share</code>，<strong>针对 <code>soft-search</code> 模块单独构建了一个网络来辅助学习</strong>，如上图左所示。</p>
<h4 id="6-2-3-二阶段-ESU（Exact-Search-Unit，精准搜索单元）"><a href="#6-2-3-二阶段-ESU（Exact-Search-Unit，精准搜索单元）" class="headerlink" title="6.2.3 二阶段 ESU（Exact Search Unit，精准搜索单元）"></a>6.2.3 二阶段 ESU（Exact Search Unit，精准搜索单元）</h4><p>经过 <code>GSU</code>，序列长度一般下降一个量级以上，该阶段能够应用相对比较复杂的序列建模结构，如模型图右所示。</p>
<ul>
<li>短序列使用的是 <code>DEIN</code> 结构；</li>
<li>长序列经过 <code>GSU</code> 检索的 topK 则使用 <code>Multi-head Attention</code> 结构。</li>
</ul>
<p>最后则是将两个阶段进行联合 training (soft-search 的时候)：</p>
<script type="math/tex; mode=display">Loss=\alpha Loss_{GSU} + \beta Loss_{ESU}</script><h3 id="6-3-小记"><a href="#6-3-小记" class="headerlink" title="6.3 小记"></a>6.3 小记</h3><ul>
<li>文章使用 180 天数据构建长期序列，最长 54000，比 MIMN 提升 54 倍，性能增加 5ms；</li>
<li>在 GSU 部分，hard-search 方案几乎没有性能问题，针对 soft-search 文章提到可以使用 MIPS 指令集优化等加速；</li>
<li>该方案思路新颖，实践效果佳，也为业界开启了 GSU+ESU 的迭代方向。</li>
</ul>
<h2 id="7-ETA"><a href="#7-ETA" class="headerlink" title="7 ETA"></a>7 ETA</h2><h3 id="7-1-概述"><a href="#7-1-概述" class="headerlink" title="7.1 概述"></a>7.1 概述</h3><p>论文：<a href="https://arxiv.org/pdf/2108.04468">End-to-End User Behavior Retrieval in Click-Through RatePrediction Model</a><br>来源：2021，阿里</p>
<p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel9.png" alt="ubsmodel9"></p>
<p><strong>思想：用 LSH 来加速 GSU 环节，并将 GSU 融入到 ESU 环节，构建端到端，相对两阶段来增加检索一致性。</strong></p>
<h3 id="7-2-方案"><a href="#7-2-方案" class="headerlink" title="7.2 方案"></a>7.2 方案</h3><h4 id="7-2-1-SimHash"><a href="#7-2-1-SimHash" class="headerlink" title="7.2.1 SimHash"></a>7.2.1 SimHash</h4><p>这个是 <code>ETA</code> 在 <code>GSU</code> 加速的核心，<code>SimHash</code> 是<strong>一种局部敏感哈希(LSH)方法，能够近似的计算向量间的相似度</strong>，说白了就是为了改善向量内积检索的速度。</p>
<p>文章通过伪代码和向量旋转来解释 <code>SimHash</code> 的原理，我们这里直接讲实操可能更利于理解。假设 $\mathbf{e} \in \mathbb{R^{n \times d}}$表示行为序列的 item embedding，$n,d$ 是序列长度和 embedding size。</p>
<p>那么，<code>SimHash</code> 步骤如下：</p>
<ul>
<li>固定一个随机生成的 Hash 矩阵 $\mathbf{H} \in \mathbb{R}^{d \times m}$，其中 m 是超参数，代表 <code>Hash 编码后的维度</code>；</li>
<li>对于每个$e_k$，按照如下方式构建 SimHash 的编码 $sig_k \in \mathbb{R}^{1 \times m}$：</li>
</ul>
<script type="math/tex; mode=display">temp_k[i] =\sum_{j=1}^{d}\mathrm{sgn}(e_{k}[j]*H[j][i])</script><script type="math/tex; mode=display">sig_{k}[i] = 1 \ if \ temp_k[i] < 0 \ else \ 0</script><blockquote>
<p>相当于所有的 $d$ 维的 item embedding 都经过 $\mathbf{H} \in \mathbb{R}^{d \times m}$ 编码成了 $m$ 维的二进制向量了。</p>
</blockquote>
<h4 id="7-2-2-模型"><a href="#7-2-2-模型" class="headerlink" title="7.2.2 模型"></a>7.2.2 模型</h4><p>如上模型图所示:</p>
<ul>
<li>针对每个 target item（$e_t$），对其进行 SimHash 编码成<code>二进制向量</code>$h_t$；</li>
<li>对用户行为序列中的 candidate item（$e<em>{k+1}$）也进行同样的 SimHash 编码成二进制向量$h</em>{k+1}$；</li>
<li>基于上述，使用<code>汉明距离</code>来检索与 target item 最近的 topK 个candidate item，完成 GSU 部分；</li>
<li>将上述 topK 个 item 作为 ESU 的输入，构建 <code>Multi-head Target Attention</code>，其余雷同。</li>
</ul>
<p><strong>需要注意的是：</strong></p>
<ul>
<li><code>Offline Training</code> 时，ETA 中的 SimHash、GSU、ESU 这整个过程是一个 End-to-End 的，即每一 step，除了 Hash 映射不变外，其他参数都在 update；</li>
<li><code>Online Serving</code> 时，因为不管是 target 还是 candidate item，它们的 embeding 和 Hash Matrix 都是不变的，故可以提前计算它们的 <code>SimHash Sig</code>，线上直接 lookup 即可使用。</li>
</ul>
<h3 id="7-3-小记"><a href="#7-3-小记" class="headerlink" title="7.3 小记"></a>7.3 小记</h3><ul>
<li>文章提到 ETA 效果优于 SIM，且 SimHash 检索后 Attention 和直接全序列 Attention 在 AUC 只差 0.1%；</li>
<li>也提到 ETA 性能相比于 dot-product 更优（32ms-19ms），因为将检索依赖的 embedding 转换成了更低维的二进制向量，使得检索时速度增加；</li>
<li>加速 GSU、提高 GSU 和 ESU 环节的一致性，确实是沿着两阶段方向的一个重点迭代思路，当然这种改善具体提升多少还依赖实践情况。</li>
</ul>
<h2 id="8-SDIM"><a href="#8-SDIM" class="headerlink" title="8 SDIM"></a>8 SDIM</h2><h3 id="8-1-概述"><a href="#8-1-概述" class="headerlink" title="8.1 概述"></a>8.1 概述</h3><p>论文：<a href="https://arxiv.org/pdf/2205.10249">Sampling Is All You Need on Modeling Long-Term User Behaviors for CTR Prediction</a><br>来源：2022，美团</p>
<p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel10.png" alt="ubsmodel10"></p>
<p><strong>思路：结合 SIM 和 ETA 的优势，提出使用 Hash Transform 和 Sample-Based Attention 来替换 <code>GSU+ESU</code> 的框架（如模型图右侧所示），直接构建 <code>End-to-End</code> 模型。</strong></p>
<h3 id="8-2-方案"><a href="#8-2-方案" class="headerlink" title="8.2 方案"></a>8.2 方案</h3><h4 id="8-2-1-挑战"><a href="#8-2-1-挑战" class="headerlink" title="8.2.1 挑战"></a>8.2.1 挑战</h4><ul>
<li>GSU 的存在，使得检索 topK 可能会存在信息堵塞的情况，比如相似 item 占位太多；</li>
<li>既然 SimHash 已经应用到了 GSU 部分，有没有可能打通 ESU 部分构建 End-to-End。</li>
</ul>
<p>作者的解决方案的<strong>关键点是：既然 target item 和 candidate item 都可以通过 SimHash 来编码，并且还可以计算近似的相似度，如果基于此还能获取到 embedidng 就完成了全局 Attention 的替换。</strong></p>
<p><code>SDIM</code> 模型的全称是 Sampling-based Deep Interest Modeling。</p>
<p>如模型图左上所示，实际上是通过2步：</p>
<ul>
<li>将 UBS 进行 Hashing 后编码成<code>签名映射表</code>；</li>
<li>将 target item 也进行 Hashing 编码成签名，去上述映射表直接检索聚合成最终的 <code>Attention Embedding</code>。</li>
</ul>
<h4 id="8-2-2-Multi-Round-Hash"><a href="#8-2-2-Multi-Round-Hash" class="headerlink" title="8.2.2 Multi-Round Hash"></a>8.2.2 Multi-Round Hash</h4><p>这里的思路与 ETA 极其相似，但为了打通 ESU 部分，做了一些改进。</p>
<p>针对 UBS 中任一 item 的 embedding 记为 $x$，先构建基础的 <code>SimHash 编码</code>，这一步与 ETA 一致：</p>
<script type="math/tex; mode=display">h(\mathbf{x},\mathbf{R})=\mathrm{sign}(\mathbf{R}\mathbf{x})</script><p>其中，</p>
<ul>
<li>$\mathbf{R} \in \mathbb{R}^{m \times d}$是 <code>Hash 矩阵</code>，<strong>m 是 Hash 编码后的维度，d 是 item embedding size</strong>；</li>
<li>$h(\mathbf{x},\mathbf{R}) \in \mathbb{R}^{m}$是 Hash 编码结果，<code>m 维</code>。</li>
</ul>
<p>假设 UBS 长度为 T，我们就可以得到 T 个 m 维的 Hash Code。如下模型图左下，$T=4,\ m = 4$。</p>
<p>给定超参数 $\tau$，代表需要将 <code>Hash Code</code> 分组的宽度，如下图中 $\tau=2$，则每个 item 的 <code>Hash Code</code> 可以被分成 2 组，图中黄色和绿色部分。</p>
<p>然后我们将每个 item 同组的 Hash Code 聚合成一张 <code>Hash SigSignature Table</code>，其中：</p>
<ul>
<li><code>sig.</code> 存储的是该组<code>去重的 Hash Code</code>；</li>
<li><code>value</code> 存储的是对应的 <code>norm embedding</code>，它是由相同 sig. 对应的 item embedding 进行归一化（norm）得到。</li>
</ul>
<p>可以看到，这里的<strong>思想是基于 <code>Hash Code</code> ，将局部位置相似的 item embedding 聚合作为局部信息的表征，实际上是一种聚类的思想，容易联想到向量检索算法中的 PQ（乘积量化）。</strong></p>
<p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel11.png" alt="ubsmodel11"></p>
<h4 id="8-2-3-Hash-Based-Sampling"><a href="#8-2-3-Hash-Based-Sampling" class="headerlink" title="8.2.3 Hash-Based Sampling"></a>8.2.3 Hash-Based Sampling</h4><p>有了上述的基础，这里就比较明朗了：</p>
<ul>
<li>首先针对 target item 也行同样的 Hash 编码并按照 $\tau$ 宽度进行分组<br>（理论上组数应该与前述的 Hash SigSignature Table 个数一样。）</li>
<li>将每一组的 <code>sig.</code> 作为 <code>key</code> 去对应的 <code>Hash SigSignature Table</code> 中查询 <code>value</code>，作为结果 <code>embedding</code>；</li>
<li>将所有查到的 <code>value</code> 进行 pooling，得到最终 <code>Target Attention</code> 的结果 <code>Embedding</code>。</li>
</ul>
<p>至此，完成了对 <code>GSU+ESU</code> 的替换，是一种 End-to-End 的对长序列进行 Target Attention 建模的结构。</p>
<h3 id="8-3-小记"><a href="#8-3-小记" class="headerlink" title="8.3 小记"></a>8.3 小记</h3><p>实际上，个人直观的思路是直接用 SimHash 后的汉明距离倒数作为 Attention Weight 来计算，但作者没有选择，可能存在的原因：</p>
<ul>
<li>汉明距离作为召回可能尚可，作为 weight 可能噪声大，序的分辨度也许不高；</li>
<li>相似度计算简单了，但需要处理的长度依然太长。</li>
</ul>
<p>回到 <code>SDIM</code>，作者提到：</p>
<ul>
<li>效果上，对比 ETA 由 AUC+0.6%-1%；</li>
<li>性能上，较 ETA 快 3 倍。</li>
<li>如下图所示，<code>SDIM</code> 与传统的 Target Attention 的结果对比，相似度很高。</li>
<li>参数 m 越大效果越好，但过大性价比不高；</li>
<li>参数$\tau$的增大，AUC 先增后减。<em>因为：太小，分组太多，泛化不够；太大，分组太少，组内区分度不够</em>。</li>
</ul>
<p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel12.png" alt="ubsmodel12"></p>
<h2 id="9-TWIN"><a href="#9-TWIN" class="headerlink" title="9 TWIN"></a>9 TWIN</h2><h3 id="9-1-概述"><a href="#9-1-概述" class="headerlink" title="9.1 概述"></a>9.1 概述</h3><p>论文：<a href="https://arxiv.org/pdf/2302.02352">TWIN: TWo-stage Interest Network for Lifelong User Behavior Modeling in CTR</a><br>来源：2023，快手</p>
<p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel13.png" alt="ubsmodel13"></p>
<p><strong>思想：ESU 是 Target Attention，GSU 的检索方式越对齐一致性越好。将 GSU 的 Target Attention 计算进行拆分，固有属性部分做缓存后 Lookup，交叉部分降维后作 Bias。</strong></p>
<h3 id="9-2-方案"><a href="#9-2-方案" class="headerlink" title="9.2 方案"></a>9.2 方案</h3><h4 id="9-2-1-挑战"><a href="#9-2-1-挑战" class="headerlink" title="9.2.1 挑战"></a>9.2.1 挑战</h4><blockquote>
<p>ESU 和 GSU 往往存在一致性问题： GSU 和 ESU 在序列 Item 与 Target Item 的相似计算方式上不一样, 从而导致 GSU 检索的 topK 往往与 ESU 有差异。（如下图）</p>
</blockquote>
<p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel14.png" alt="ubsmodel14"></p>
<p><strong>诸如 ETA、SDIM 都是通过使用其他近似算法来优化 GSU 过程</strong>，使得 GSU 可以处理更长序列，同时逼近 Target Attention。但，上述近似算法始终与 <code>MHTA</code> 算法有一定差异，<strong><code>TWIN</code> 则是通过将 Attention 进行拆解，将 GSU 的 Target Attention 部分更进一步的逼近于 <code>MHTA</code>。</strong></p>
<h4 id="9-2-2-Behavior-Feature-Splits-and-Linear-Projection"><a href="#9-2-2-Behavior-Feature-Splits-and-Linear-Projection" class="headerlink" title="9.2.2 Behavior Feature Splits and Linear Projection"></a>9.2.2 Behavior Feature Splits and Linear Projection</h4><p>序列特征的分解与线性映射，是为了提升 Attention 模块性能。因 Multi-Head Target Attention（MHTA）的<strong>主要耗时在于两部分：序列信息做线性映射、内积加权和。</strong></p>
<p>序列特征可以分为：</p>
<ul>
<li><code>固有特征</code>（如标题、作者、视频ID等）；</li>
<li><code>交互特征</code>（如点击时间、观看时长等）。</li>
</ul>
<p>其中，</p>
<ul>
<li><code>固有特征</code>独立于 user，包括其行为序列，可以提前计算存储下来，线上直接 lookup 即可。</li>
<li><code>交叉特征</code>不能使用缓存方案，与 user 行为序列有关，但每个 user 最多只看每个 item 一次。</li>
</ul>
<p>基于上述特性，我们<strong>将交叉特征线性映射为 1 维。</strong></p>
<p>假设 UBS 为$[s_1,s_2,…,s_L]$ ，对应的<code>特征矩阵</code>为 $K$。则$K$可以拆分为两部分，如下：</p>
<script type="math/tex; mode=display">K\triangleq[K_h,K_c]\in R^{L\times(H+C)}</script><p>其中 $K_h \in R^{L \times H}$ 是<code>固有特征</code>， $K_c \in R^{L \times C}$ 是则是<code>交互特征</code>部分。</p>
<p>如上所述， $K_h$ 可以提前离线计算并缓存供线上 Lookup 使用。<br>对于<code>交互特征</code> $K_c$，假设有$J$个，每个 8 维，文章提到可将其均映射为 1 维，如下所示：</p>
<script type="math/tex; mode=display">K_{c}W^{c}\triangleq[K_{c,1}W_{1}^{c},\ldots,K_{c,J}W_{J}^{c}</script><p>其中 $K_{c,j} \in R^{L \times 8}$ 为第 $j$ 个<code>交互特征</code>，$W_j^c \in R^8$ 则是对应的<code>权重参数</code>。</p>
<h4 id="9-2-3-Target-Attention-in-TWIN"><a href="#9-2-3-Target-Attention-in-TWIN" class="headerlink" title="9.2.3 Target Attention in TWIN"></a>9.2.3 Target Attention in TWIN</h4><p>上述的操作主要都是为了提速，当然在 Attention 部分也做了适配改造。</p>
<ul>
<li><code>Q、K</code> 的固有属性部分直接 Lookup <code>缓存</code>得到；</li>
<li>降维后的交叉特征部分则作为 <code>Bias</code> 项；</li>
<li>Target Item 仅与固有特征做内积（<em>快手曝光频控一次，故 Target Item 没有交叉特征</em>）。</li>
</ul>
<script type="math/tex; mode=display">\alpha = \frac{(K_h W^h)(q^T W^q)^T}{\sqrt{d_k}}+(K_c W^c) \beta</script><p>则，<strong>这里的 $\alpha$ 实际上就是 Target Attention 的内积结果</strong>。</p>
<ul>
<li>GSU 阶段用这个对序列 Item 做粗筛 Top100；</li>
<li>ESU 阶段对这 Top100 再做一次简化的 Target Attention。</li>
</ul>
<p>如下所示：</p>
<script type="math/tex; mode=display">Attention(q^{T} W^{q},K_{h} W^{h},K_{c} W^{c},K W^{v})=Softmax(\alpha)^{T}K W^{v}</script><p><strong>注意：ESU 的$\alpha$实际上是重新计算的，不是 GSU 中的。</strong></p>
<p>文章提到，实际业务中使用 <code>MHTA</code>，且 head 数为 4，所以最终如下：</p>
<script type="math/tex; mode=display">TWIN=Concat(\mathrm{head}_1,...,\mathrm{head}_4)W^o</script><script type="math/tex; mode=display">\mathrm{head}_a=\mathrm{Attention}(\mathbf{q}^\top W_a^q,K_hW_a^h,K_cW_a^c,KW_a^v),a\in\{1,...,4\}</script><p>其中，$W^o$是 head 之间的权重，也是模型学习得到。</p>
<h3 id="9-3-小记"><a href="#9-3-小记" class="headerlink" title="9.3 小记"></a>9.3 小记</h3><p><code>TWIN</code> 的有效性主要得益于 3 点：</p>
<ol>
<li>作者将序列特征 拆分成了 固有属性 和 交互特征，分别使用缓存（命中率99.3%）和降维分而治之；</li>
<li>基于上述，对 Target Attention 做了简化；</li>
<li>业务上，Target Item 与 UBS 没有交互提供了上述可拆分的支持。</li>
</ol>
<p><code>TWIN</code> 进一步提高了 GSU 和 ESU 部分的一致性（如下图所示），GSU 也用上了 Target Attention，且能够支持 $10^5$的序列。</p>
<p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel15.png" alt="ubsmodel15"></p>
<p><strong>但这里有一个问题：如果 GSU 能够做到 Target Attention 为什么不统一成全局 ESU，还要保留 GSU 来粗筛 top100 来给 ESU？</strong></p>
<p>实际上，这有 2 个原因：</p>
<ol>
<li>Q、K 做了简化，V 的 Project、 Weight Pooling 以及 bp 都是很耗时的过程，且 100 后的$\alpha$往往都很小信息量不大，所以截取 top100 还是很具有性价比的；</li>
<li>虽然 GSU 和 ESU 的 Attention 结构一样，但分数上依然存在些许差异。因为 GSU 是离线计算，其参数更新速度没有 ESU 部分快。故 ESU 部分重新计算$\alpha$，性能可支持、实时性更高、准确度更好。</li>
</ol>
<h2 id="10-TWIN-V2"><a href="#10-TWIN-V2" class="headerlink" title="10 TWIN-V2"></a>10 TWIN-V2</h2><h3 id="10-1-概述"><a href="#10-1-概述" class="headerlink" title="10.1 概述"></a>10.1 概述</h3><p>论文：<a href="https://arxiv.org/pdf/2407.16357v2">TWIN V2: Scaling Ultra-Long User Behavior Sequence Modeling for Enhanced CTR Prediction at Kuaishou</a><br>来源：2024，快手</p>
<p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/rankmodel/ubsmodel16.png" alt="ubsmodel16"></p>
<p><strong>思想：为了使得 TWIN 可以支持到10^6量级，TWIN-V2 基于层次聚类，对 UBS 中的 Item 做聚类后得到簇序列，从而将序列量级从 Item 元素的$10^6$量级降到簇元素下的$10^5$量级，之后再应用 TWIN 即可。</strong></p>
<h3 id="10-2-方案"><a href="#10-2-方案" class="headerlink" title="10.2 方案"></a>10.2 方案</h3><h4 id="10-2-1-Hierarchical-Clustering"><a href="#10-2-1-Hierarchical-Clustering" class="headerlink" title="10.2.1 Hierarchical Clustering"></a>10.2.1 Hierarchical Clustering</h4><p>Item 数量太多，则将 Item 聚类成<code>簇</code>，变成<code>簇序列</code>，量级下来后，以簇为 <code>新 Item</code>支持完成 <code>TWIN</code> 模型。</p>
<p><code>Item 分层</code>：对 UBS 的各个 Item $v_j$，根据<code>完播率</code>$p_j=playing \ timevideo \ duration$分成 <code>M 组</code>(文章中M=5)，可以使用<code>等宽分组</code>。这里实际上是对用户偏好进行了显式分层。</p>
<p><code>Item 聚类成簇</code>：这里文章给了算法的伪代码，这里简述要点。</p>
<ul>
<li>逐个处理 M 组序列，分别对其进行聚类；</li>
<li>每个簇内部最多包含 $\gamma$个 Item，如某组序列的 Item 总数少于此，整体作为一簇；</li>
<li>数量够的，计算需要的聚类数 $\delta \leftarrow \lfloor |V|^{0.3} \rfloor$；</li>
<li>根据 Item 的 Embedding，将该组内的 Item 进行 Kmeans 聚类，聚类数为上述 $\delta$。</li>
</ul>
<p>最终将原始 UBS 的 Item 序列即 $S=[s<em>1,s_2,\cdots,s_T]$转化成了<code>簇序列</code>，即$C=[c</em>{1},c<em>{2},\cdots,c</em>{\hat{T}}]$。</p>
<p>此外，文章提到：</p>
<ul>
<li>层次聚类 2 周完整更新一次，毕竟是全生命周期的，计算量大；</li>
<li>Embedding Server 来源 GSU 的固有属性, 每隔15分钟进行同步；</li>
<li>实践中簇的内部大小$\gamma=20$，而最终的簇个数平均为 10，相当于将序列量级下降1级。</li>
</ul>
<h4 id="10-2-2-Extracting-Cluster-Representation"><a href="#10-2-2-Extracting-Cluster-Representation" class="headerlink" title="10.2.2 Extracting Cluster Representation"></a>10.2.2 Extracting Cluster Representation</h4><p>在得到各个簇之后，需要构建<code>簇的表征</code>，否则下游的模型无法使用。逻辑上也是将簇内 Item 两种类型的特征单独分开处理。</p>
<p><code>连续型特征</code>，<strong>取簇内各 Item 的均值</strong>:</p>
<script type="math/tex; mode=display">\mathbf{c}_{1:N_2}^{(i)}=\frac{1}{|c_i|}\sum_{v\in c_i}\mathbf{x}_{1:N_2}^{(v)}</script><p>但<code>分类型特征</code>，均值就没意义了。文中提到<strong>从簇中选取一个代表性的 Item 来表示，筛选方案是：与聚类中心的距离最小的</strong>。</p>
<script type="math/tex; mode=display">v=\arg\min_{v\in c_{i}}\|\mathrm{k}_{v}-\mathrm{k}_{\mathrm{centroid}}\|_{2}^{2}</script><p>最后将分类型和连续型特征 <code>concat</code> 即可作为簇的 Embedding 了。</p>
<h4 id="10-2-3-Cluster-aware-Target-Attention"><a href="#10-2-3-Cluster-aware-Target-Attention" class="headerlink" title="10.2.3 Cluster-aware Target Attention"></a>10.2.3 Cluster-aware Target Attention</h4><blockquote>
<p>原始序列从$S$已经下降一个量级到$C$了，并且对应的 Embedding 也具备，可以直接应用 TWIN 模型了。</p>
</blockquote>
<script type="math/tex; mode=display">\alpha=\frac{(\mathrm{K}_h\mathrm{W}^h)(\mathrm{q}^\top\mathrm{W}^q)^\top}{\sqrt{d}_k}+(\mathrm{K}_c\mathrm{W}^c)\beta</script><p>注意力分数依然按照上述计算，但文章提到，这时候的元素已经不再是 Item 了，<strong>如果不同的类簇有相同的 Score，那么簇内 Item 数更多的理论上更置信。</strong></p>
<p>故，对注意力分做了矫正：</p>
<script type="math/tex; mode=display">\alpha^{\prime}=\alpha+\ln\mathbf{n}</script><p>其中$\mathbf{n}$是簇内 Item 的数量。在 GSU 和 ESU 环节均使用$\alpha^{\prime}$来计算注意力分，其余环节与 TWIN 保持一致。</p>
<h3 id="10-3-小记"><a href="#10-3-小记" class="headerlink" title="10.3 小记"></a>10.3 小记</h3><p>为了支撑更大的量级，在 TWIN-V2 中，选择<strong>将问题转化为 TWIN 能处理的量级，方法就是对原始的 Item 进行分层聚类，从而将原始的 Item 序列转化为低一个量级的聚类簇序列。</strong></p>
<p>文章在实验部分提到效果较为显著，<strong>但聚类本身容易带来信息丢失</strong>，尤其是下面2个环节：</p>
<ul>
<li>$M,\gamma,\delta$的超参数选择；</li>
<li>簇的类型特征的表征。</li>
</ul>
<p>故，该方法的实际效用如何，还需要以具体场景的实践结果为准。</p>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>除了上述提到的一系列文章外，业内当然还有不少其他方面的研究成果。笔者没有再进一步整理，一方面考虑到篇幅过大，另一方面也是个人判断方案的普适性。</p>
<p>上述展开的一系列成果比较契合序列建模迭代的 2 大方向且成果往往也在多个场景实践落地，更具参考价值。</p>
<p>当然，这里也附上部分近年的相关文章供参考：<br><a href="https://arxiv.org/pdf/2311.10764">DGIN</a>（2024，美团）<br><a href="https://dl.acm.org/doi/pdf/10.1145/3589335.3648308">ASIF</a>（2024，蚂蚁）<br><a href="https://dl.acm.org/doi/pdf/10.1145/3589335.3648301">SUM</a>（2024，META）<br><a href="https://arxiv.org/pdf/2110.11337">LURM</a>（2023，阿里）<br><a href="https://arxiv.org/pdf/2312.06424">LCN</a>（2024，腾讯）<br><a href="https://arxiv.org/pdf/2402.02842">Trinity</a>（2024，字节）</p>
<p><strong>参考文章：</strong></p>
<p><a href="https://zhuanlan.zhihu.com/p/4544607237">抖音/阿里/美团/微信/快手长序列兴趣建模经典方案探索</a><br><a href="https://zhuanlan.zhihu.com/p/699924066">一文梳理近年推荐长序列兴趣建模经典方案</a><br><a href="https://mp.weixin.qq.com/s/RQ1iBs8ftvNR0_xB7X8Erg">阿里妈妈点击率预估中的长期兴趣建模</a><br><a href="https://zhuanlan.zhihu.com/p/433135805">推荐系统——精排篇【3】</a><br><a href="https://zhuanlan.zhihu.com/p/51623339">推荐系统中的注意力机制——阿里深度兴趣网络（DIN）</a><br><a href="https://zhuanlan.zhihu.com/p/50758485">详解阿里之Deep Interest Evolution Network(AAAI 2019)</a><br><a href="https://zhuanlan.zhihu.com/p/78544498">简析阿里 BST: 当用户行为序列邂逅Transformer</a><br><a href="https://zhuanlan.zhihu.com/p/89700141">DSIN（Deep Session Interest Network ）分享</a><br><a href="https://zhuanlan.zhihu.com/p/94432395">阿里妈妈长期用户历史行为建模——MIMN模型详解</a><br><a href="https://zhuanlan.zhihu.com/p/154401513">[SIM论文] 超长兴趣建模视角CTR预估：Search-based Interest Model</a><br><a href="https://zhuanlan.zhihu.com/p/444065581">阿里ETA(End-to-End Target Attention)模型</a><br><a href="https://zhuanlan.zhihu.com/p/525604184">【论文解读|CIKM’2022】基于采样的超长序列建模算法 SDIM</a><br><a href="https://zhuanlan.zhihu.com/p/606047328">快手终身序列建模方案—TWIN</a><br><a href="https://zhuanlan.zhihu.com/p/699725252">精排最终也是样本的艺术</a></p>
<hr>

          
        
      
    </div>
    
    
    
    
    <div>
      
    </div>

    <div>
      
    </div>

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.xiemingzhao.com/posts/listwisererank.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.postimg.cc/vBxZQfvz/img-0182.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小火箭的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/posts/listwisererank.html" itemprop="url">Listwise 在重排的应用</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2024-09-08T00:00:00+08:00">
                2024-09-08
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/" itemprop="url" rel="index">
                    <span itemprop="name">算法总结</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/%E9%87%8D%E6%8E%92%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index">
                    <span itemprop="name">重排模型</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/posts/listwisererank.html#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/posts/listwisererank.html" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                  <span class="post-meta-divider">|</span>
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  5.1k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  18
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p><em>本文先假定读者对推荐系统有一定了解，尤其是召回、精排和重排3个部分，如此阅读本文可能更顺畅。</em></p>
<p><code>LTR</code>（Learning to Rank）即排序学习，在当下的搜广推业务中应用广泛，思路上主要分为四类：</p>
<ol>
<li>基于 <code>Point-wise</code>。主要考虑 user、item、以及 context 信息特征，仅关注当前 item 自身的效率，比如 <code>Wide &amp; Deep</code>、<code>ESSM</code> 等。</li>
<li>基于 <code>Pair-wise</code>。训练时通过损失函数来预估 <code>item pair</code> 之间的相对位置关系，高 Label 排在低 Label 前则为正例，否则，为反例。模型的目标是减少错误 item pair 的个数，不考虑列表信息，模型训练复杂度问我较高，比较典型的如 <code>RankNet</code>。</li>
<li>基于 <code>List-wise</code>。训练时候对每个 <code>request</code> 下的整个 <code>item list</code> 构建 Loss，拟合最优序列分布。比较典型如 <code>LambdaMART</code>、 <code>DLCM</code>、以及 <code>ListMLE</code> 等。</li>
<li>基于 <code>Generator-evaluator</code>。一般分为 2 模块，序列的召回和排序。重排召回即按照一定排序算法，生成一系列预期 <code>reward</code> 较高的候选 List，再针对这些上下关系已经固定的 List 进行整页建模预估 <code>reward</code>，选择 <code>top1 list</code> 输出。</li>
</ol>
<p>本文将重点聚焦在第 4 种，即基于 <code>Generative-evaluate</code> 的 <code>LTR</code> 模型，因为此类做法有两个特点：</p>
<ul>
<li>可真正实现对 List 进行整页建模的；</li>
<li>可在重排环节实践落地的。</li>
</ul>
<p>虽然第 3 种一般也称为 <code>List-wise</code>，但实际上更多的是对精排模型训练的 <code>Loss</code> 进行优化，最终 infer 的时候依然是 <code>Point-wise</code>。当然，它也是有一定效果，不过不是本文想讨论的重点。本文主要基于 <code>Generative-evaluate</code> 来讨论对整个 List 的建模和预估，并总结部分笔者的实践经验。</p>
<h2 id="2-模型原理"><a href="#2-模型原理" class="headerlink" title="2 模型原理"></a>2 模型原理</h2><h3 id="2-1-场景"><a href="#2-1-场景" class="headerlink" title="2.1 场景"></a>2.1 场景</h3><blockquote>
<p>重排的环节，往往是基于用户 U 和上下文 C，需要从上游（一般是精排）给到候选物品集合 I 中选出 K 个并组成有序 List 展示给前端用户。</p>
</blockquote>
<p>故，<strong>重点就在如何挑选并排序成最优列表 O，以达到业务目标上的最大化</strong>，比如点击、下拉曝光、订单转化等。一般传统做法都是基于精排多目标预估融合分，使用 <code>MMR/DPP</code> 等重排算法，再结合业务规则，进行重排。</p>
<p><strong>精排缺陷</strong>：</p>
<ul>
<li>精排 <code>pointwise</code> 建模缺少空间信息的考虑，即当前 item 的上下内容（一般该假设都成立）；</li>
<li>基于精排分+规则的贪心重排方案往往与全局最优偏差较远；</li>
<li><code>MMR,DPP</code> 重排也是在目标中给予多样性一些权重，各有优劣。</li>
</ul>
<p><strong>重排目标</strong>：</p>
<ul>
<li>需要考虑序列空间信息，例如序列窗口特征、或 <code>transformer</code> 结构；</li>
<li>建模优化目标要做到 List 整体，而不是 Pointwise 式；</li>
<li>框架能够具备不断发掘更优序列的能力，防止系统模型退化。</li>
</ul>
<h3 id="2-2-方案思路"><a href="#2-2-方案思路" class="headerlink" title="2.2 方案思路"></a>2.2 方案思路</h3><p><strong>一个排序结果的好坏，需要 List 固定后，才能结合整个空间排布和上下文信息来真正得进行 Listwise 评估。</strong></p>
<p>基于上述讨论，我们将重排分为了两个阶段：</p>
<ul>
<li><code>重排-召回</code>：使用 MMR 等不同算法，朝着业务目标生成多样的候选序列作为序列召回集；</li>
<li><code>重排-排序</code>：构建序列评估模型，对候选序列集进行真正的 Listwise 评估，选出最优序列。</li>
</ul>
<p>如下图所示，右侧上半部分单点重排便是一个典型的传统重排方案。相对应的，下半部分是结合了序列检索和序列评估的重排模块。</p>
<p>我们将从候选集合中选择不同的物品排列成不同的序列视作推荐系统的召回，再从这些序列中选出最好的就可以作为推荐系统中的排序，之后便可以直接输出了。</p>
<p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/listwise/listwise0.png" alt="listwise0"></p>
<h3 id="2-3-重排-召回"><a href="#2-3-重排-召回" class="headerlink" title="2.3 重排-召回"></a>2.3 重排-召回</h3><p>之所以要有该环节，除了需要让序列的空间分布固定以利于 Listwise 评估外，还有<strong>一个重要的假设：传统的贪心重排算法是很难趋向全局最优的。</strong></p>
<p>所以该环节<strong>重点是如何生成尽可能高效但又具有差异的序列集合</strong>，序列的多和差异是为了增加逼近最优序列的概率。</p>
<blockquote>
<p>虽然最优序列我们可能永远做不到，但相比 Base 的单序列（Original List）增加了找到更优 List 的可能，因为一个<strong>兜底的做法就是将 Original List 也加入到候选序列集合中。</strong></p>
</blockquote>
<p>实际上也是<strong>多路召回思路</strong>：</p>
<ul>
<li>使用不同的序列生成模型来生成序列；</li>
<li>同一序列生成模型下，生成不同目标的序列。</li>
</ul>
<p>这里介绍几个常用的序列生成模型和如何生成不同目标的序列，其实<strong>核心思想就是不同算法生成序列，或同一个算法采用不同的参数来生成</strong>。可以想一下，这些生成算法单独应用时，想必有一些调参的工作，现在好了，我们可以把想要的参数都作为一个序列生成的候选。</p>
<h4 id="2-3-1-MMR-模型"><a href="#2-3-1-MMR-模型" class="headerlink" title="2.3.1 MMR 模型"></a>2.3.1 MMR 模型</h4><p><code>MMR</code> 全称 <code>Maximal Marginal Relevance</code>，即<code>最大边界相关法</code>，是一种应用较为广泛的重排模型。</p>
<p>其核心公式如下所示：</p>
<script type="math/tex; mode=display">MMR=Arg \mathop{max}\limits_{ D_i \in R \backslash S } [\lambda Sim_1(D_i, Q) - (1-\lambda) \mathop{max}\limits_{D_j \in S} Sim_2(D_I,D_j)]</script><p>相信有不少算法工程师在早期也用过此模型，那可以基于此生成多样的序列候选。一般有 3 种方式：</p>
<ol>
<li><code>Lambda</code>的超参数变化，可以生成效率和多样性侧重程度不同的序列；</li>
<li>$Sim_1$ 往往是<code>效率分</code>，其可以有不同的融合方式，比如更侧重点击、或者订单，如此也可以生产不同的序列；</li>
<li>$Sim_2$ 往往是<code>多样性分</code>，其也可以有不同的构建方式，比如增强品类差异，削弱序列品牌，或者增加多样性计算窗口等。</li>
</ol>
<p>而上述三种方式独立使用外，又可以互相叠加，如此一种重排模型便可以生产多种多样的序列。当然数量上也需要控制，主要是出于性能的考虑，所以也需要算法工程师结合业务认识来选取序列生成的配置。</p>
<h4 id="2-3-2-DPP-模型"><a href="#2-3-2-DPP-模型" class="headerlink" title="2.3.2 DPP 模型"></a>2.3.2 DPP 模型</h4><p>其是通过构建核矩阵后使用行列式求解的一种算法，逻辑细节不是这里的重点，我们主要看其在生成中对序列价值度量方式：</p>
<script type="math/tex; mode=display">logdet(L_{R_u}) = \sum_{i \in R_u} log(r_{u,i}^2) + logdet(S_{R_u})</script><p>其中，第一项是<code>效率分的变换</code>，第二项是<code>相似性矩阵的行列式变换</code>。</p>
<p>同样的逻辑，该模型也可以通过修改不同的环节来实现生成不同的序列，如：</p>
<ul>
<li>效率分 $r_{u,i}$ 的融合分方式差异化；</li>
<li>相似性 $S_{ij}$ 的构建方式差异化。</li>
</ul>
<h4 id="2-3-3-Beamsearch"><a href="#2-3-3-Beamsearch" class="headerlink" title="2.3.3 Beamsearch"></a>2.3.3 Beamsearch</h4><p><code>Beam Search</code> （束搜索）是一种在有限集合中寻找最优有序子集的搜索算法。可以简述为：</p>
<ul>
<li>初始 m 个序列；</li>
<li>每轮给 m 个序列各选前 k 个最优的且满足业务逻辑的候选 item；</li>
<li>从 m x k 个候选序列再选择 m 个序列进入下一轮；</li>
<li>重复上述步骤直到序列长度满足要求。</li>
</ul>
<p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/listwise/listwise1.png" alt="listwise1"></p>
<p>可以看到，该方法实际上是基于贪心算法扩大了搜索范围，<strong>涉及调参的环节也很多，都可以作为序列生成的差异化来源</strong>。例如：</p>
<ul>
<li>序列个数 m 以及每轮搜索物品数 k；</li>
<li>选取物品时依赖的分数，效率分、多样性分等；</li>
<li>业务逻辑（如打散）的强弱；</li>
<li>首轮种子、中间轮保留序列的评估方式。</li>
</ul>
<h4 id="2-3-4-GRN"><a href="#2-3-4-GRN" class="headerlink" title="2.3.4 GRN"></a>2.3.4 GRN</h4><p>既然经常强调序列重排的时候要关注空间信息，那么参考 NLP 的 <code>seq2seq</code> 结构，如果构建一个深度模型对候选序列循环排序，每轮将已排序的作为 input，似乎能够解此问题。例如 <code>GRN</code>（Generative Reranking Network）就是这样一个探索，模型结构如下图。</p>
<p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/listwise/listwise2.png" alt="listwise2"></p>
<p>可以看到使用了 <code>GRU</code> 来捕捉序列信息的，当然也有用 <code>Transformer</code> 结构的做法。但不管是结构 ，都是一种 <code>往前看</code>的生成模型，即输入包含前面已经排好的序列。不过这类生产模型有一些<strong>缺点：构建和迭代复杂；线上推理时间长；差异化生成复杂度高</strong>。</p>
<h3 id="2-4-重排-排序"><a href="#2-4-重排-排序" class="headerlink" title="2.4 重排-排序"></a>2.4 重排-排序</h3><p>在<code>重排-召回</code>环节完成之后，我们可以得到了一系列高效且丰富多样的候选 List，一般建议在几十至几百，当然这个主要由场景的价值增益、资源成本等决定。</p>
<p>这些候选 List 可能<strong>在物料上有差异，但空间排布上的差异更是重点</strong>。所以，对这些 List 的价值预估可以基于精排分设计 <code>Reward</code> 公式，但如果想更精准的预估，就需要进行真正的 Listwise 建模。<br><strong>思路：要模拟用户浏览时的决策，对于一个物料是否有兴趣，除了精排关注的因素外，其上下的空间排版就至关重要，就需要在特征和模型结构上重点弥补这部分信息。</strong></p>
<p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/listwise/listwise3.png" alt="listwise3"></p>
<h4 id="2-4-1-分位置建模"><a href="#2-4-1-分位置建模" class="headerlink" title="2.4.1 分位置建模"></a>2.4.1 分位置建模</h4><p><strong>方法：对已排好序列的每个位置的 item 进行相关目标的预估</strong>。例如 pctr、pcvr 等，目标就是比精排预估得更准。</p>
<p><strong>优势</strong>：</p>
<ul>
<li>逻辑简单，利用空间排版特征、Listwise 模型结构来弥补精排不足；</li>
<li>方便评估，可直接与精排进行 AUC 等指标对比；</li>
<li>兼容性高，下游度量序列价值的融合公式往往不用修改。</li>
</ul>
<p><strong>缺点</strong>：非整序列建模，依然通过对每个 item 进行预估。</p>
<h4 id="2-4-2-整序列建模"><a href="#2-4-2-整序列建模" class="headerlink" title="2.4.2 整序列建模"></a>2.4.2 整序列建模</h4><p>与分位置建模对应的就是整序列建模，<strong>即 Label 不再是序列中每个 item 的业务价值，而是整个序列的</strong>。比如，分位置建模 ctr（点击率） 就可以变成对整序列建模 ipv（点击数）。</p>
<p>但是，在这里有一个<code>不同点</code>：</p>
<ul>
<li>分位置的时候，Label 还是可以对齐精排，如 ctr 使用 sigmoid 做二分类任务；</li>
<li>整序列建模的时候，Label 往往就不是二值的了，不能简单的做二分类，会损失信息。</li>
</ul>
<p><strong>解法：分层多分类</strong>。</p>
<blockquote>
<p>以 ctr 对应的 ipv 为例，假设序列长度为 n，ctr 的 Label 为 0/1，那么 ipv 的 Label 范围就是 0-n，这 n+1 种可能。</p>
</blockquote>
<p>那么就可以在每两个取值之间作为分割点，构建二分类任务，总计 n 个。<br>比如，第 $i (1&lt;= i &lt;= n)$ 个 Label 就是：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Label_i = \begin&#123;cases&#125;</span><br><span class="line">   0 &amp;\text&#123;if &#125; ipv&lt;i \\</span><br><span class="line">   1 &amp;\text&#123;if &#125; ipv&gt;=i</span><br><span class="line">\end&#123;cases&#125;</span><br></pre></td></tr></table></figure></p>
<p>如果这 n 个分层 Label 的预估项为$p_i,i \in [1,n]$，那么最后 <code>ipv</code> 的预估项应该为：</p>
<script type="math/tex; mode=display">p^{ipv} = \sum_{i = 1} ^ n (i \cdot p_i \cdot (1 - p_{i+1}))</script><p>其中，最后一项中 $p_{n+1}=1$即可。</p>
<h4 id="2-4-3-评估目标"><a href="#2-4-3-评估目标" class="headerlink" title="2.4.3 评估目标"></a>2.4.3 评估目标</h4><p>评估目标的设立，依赖前述对业务建模的目标，最后进<strong>多目标之间的融合评估</strong>，这点与精排环节类似。<br>一般可能会涉及，<strong>曝光PV，点击IPV，转化ORD，多样性DIV，客单价AOV，停留时长TIME等</strong>。<br>这里重点介绍2个具有代表性的，点击IPV，多样性DIV。</p>
<p><strong>点击IPV</strong><br>这是一个比较明确的统计量，用户的点击次数。<br>该指标具有<code>局部累加性</code>：即每个请求贡献的 IPV 是可以直接累积到整体的，不会有衰减。<br>一个建议的构建方式是：</p>
<script type="math/tex; mode=display">p^{IPV} = \sum_{pos = 1}^n p^{expose} * p^{ctr}</script><p>其中，$p^{expose}$ 是曝光概率，可以是模型预估的，如没有可以去掉或者用离线统计代替。<br>当然，这里针对的是分位置建模，如果是整序列建模，则对应的 $p^{ipv}$ 在上述已介绍。</p>
<p><strong>多样性DIV</strong><br>相对于上面而言，该指标需要依赖业务来明确具体的统计量是什么。<br>且往往不具有<strong>局部累加性</strong>，比如<code>用户点击的类目数</code>，它是有一个 user 维度去重的概念。<br>这种情况比较复杂，可以尝试下面2种。</p>
<p>类目信息熵：</p>
<script type="math/tex; mode=display">cate_{ce} = -\sum_{cid \in C} p_{cid} \cdot (Math.log(p_{cid}) / Math.log(2)) \cdot ctr_{cid}</script><p>其中，$p<em>{cid}$是对应 cid 类目的占比，$ctr</em>{cid}$则是该类目的点击率某统计量，比如均值等。</p>
<p>未点击类目期望：</p>
<script type="math/tex; mode=display">newcate_{clk} = -\sum_{cid \in C} isnew \cdot ctr_{cid}</script><p>其中，isnew 是指当前 cid（类目id）是否是 user 未点击的，$ctr<em>{cid}$同样是一个统计量，可以采用反概率：$ctr</em>{cid}=1 - \prod_{i} (1 - p^{ctr}_i)$。</p>
<blockquote>
<p><strong>需要强调的是：真正是否有效，与数据的分布、业务的逻辑等有很大关系，需要结合具体业务理解来尝试和优化，以实验反馈为准。</strong></p>
</blockquote>
<h2 id="3-实战思考"><a href="#3-实战思考" class="headerlink" title="3 实战思考"></a>3 实战思考</h2><p>在实际落地应用中，实际上有很多的坑需要踩，这里将结合自己的一些实践经历，总结一些相对比较重要的细节，供大家参考和自己复习。</p>
<h3 id="3-1-序列生成的多样性"><a href="#3-1-序列生成的多样性" class="headerlink" title="3.1 序列生成的多样性"></a>3.1 序列生成的多样性</h3><p>在<code>重排-召回</code>环节我们提到需要生成丰富多样的候选序列，那么这其中一个便是序列的多样性。当然，具体多样性是指什么，需要根据实际业务场景来，比如:</p>
<ul>
<li>电商的品牌、品类、店铺等；</li>
<li>视频的作者、类型、演员等；</li>
<li>图文的作者、主题、风格等。</li>
</ul>
<p>在重排中，往往会有一个多样性的设置，一般是 <strong>hard 规则（兜底） + soft 度量（排序）</strong>。其中 soft 度量多样性的部分，以 MMR 为例，会有2个要素：</p>
<ul>
<li><strong>物品间多样性计算方式</strong>。比如 embedding cos、属性 match 等；</li>
<li><strong>物品选择时多样性统计窗口</strong>。比如 MMR 每轮多样性分数的统计窗口，即往前看多少个物品。</li>
</ul>
<p>在<strong>多样性计算方式</strong>上：</p>
<ul>
<li>属性 <code>match</code> 简单且往往有效，但维度的选择一定要根据业务场景来，要选用户关注的多样性维度；</li>
<li>使用 <code>embedding cos</code>，需要注意 embedding 的生成方式，用户关注的多样性维度要体现在其中，反之像基于行为训练的往往会不尽如意；</li>
<li>一般建议结合 <code>embedding cos</code> + 属性 <code>match</code> 的方式；</li>
<li>多样性不同属性维度的权重可以作为序列生成的差异性来源。</li>
</ul>
<p>在<strong>多样性统计窗口</strong>上：</p>
<ul>
<li>离当前 item 越近的权重可以设置越高；</li>
<li>窗口的大小可以作为序列差异性的来源，往往比较有效。</li>
</ul>
<h3 id="3-2-“Beamsearech-X”序列生成框架"><a href="#3-2-“Beamsearech-X”序列生成框架" class="headerlink" title="3.2 “Beamsearech+X”序列生成框架"></a>3.2 “Beamsearech+X”序列生成框架</h3><p><code>Beamsearch</code> 是束搜索，<strong>它是对穷举搜索和贪心搜索这两种偏极端算法的一种折中之策</strong>。即在每一轮选择候选的时候，既不是贪心的只选得分最高的，也不是穷举得遍历每种组合，而是选择 topK 个候选增加寻到更优 List 的可能性，具体算法细节这里不再赘述。</p>
<blockquote>
<p>这里笔者重点想介绍的是基于其构建的一种拓展方案，即“Beamsearech+X”序列生成框架。怎么理解呢？</p>
</blockquote>
<p>前面提到 <code>Beamsearch</code> 主要是在每轮选取的时候拓展了候选，增加靠近全局最优的可能性。但，需要注意的是，在每一轮选取的时候依然需要有一个 <code>Function</code> 来度量候选加入后对 List 的影响好坏。而 Function 的构建实际上就退化成了单 List 的排序，这部分有一些成熟的候选：</p>
<ul>
<li>效率分贪心；</li>
<li>MMR；</li>
<li>DPP等。</li>
</ul>
<p>于是，上面的候选算法都可以作为 <code>Beamsearch+X</code> 中的 <code>X</code> 项。</p>
<p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/listwise/listwise4.png" alt="listwise4"></p>
<p>我们以 <code>MMR</code> 为例，如上图所示是一种 <code>Beamsearch+MMR</code> 实现方案：</p>
<ol>
<li><code>种子选取</code>：根据物品分 Score 选取 K 个作为 List 种子；</li>
<li><code>生成候选</code>：中间每轮，为每个种子 List 选取 M 个候选，例如使用 MMR；</li>
<li><code>序列子评估</code>：从每个种子的 M 个候选 List 中选择一个整体价值最高的保留；</li>
<li><code>完成序列</code>：重复上述直到序列长度满足业务要求，将序列集合作为该路召回。</li>
</ol>
<p>如此，其他的单序列生成都可以融入到 <code>Beamsearch+X</code> 体系中，拓展生成能力。那么 <code>Beamsearch+X</code> 的召回丰富度，除了可以继承 <code>X</code> 生成算法的丰富度调整方式外，<code>Beamsearch</code> 本身的一些特征也可以作为调整项，比如：</p>
<ul>
<li>种子物品的选取方式，如效率 topK、多样性 topK等；</li>
<li>种子个数 K、每轮候选 M；</li>
<li>子评估的构建方式，如统计、LR等；</li>
<li>子评估的排序方式，比如子序列候选混合或独立。</li>
</ul>
<h3 id="3-3-候选评估模型"><a href="#3-3-候选评估模型" class="headerlink" title="3.3 候选评估模型"></a>3.3 候选评估模型</h3><p>在序列生成环节，前述提到的 <code>MMR</code>、<code>Beamsearch+X</code>、<code>DPP</code>等均是动态规划类或搜索类模型，并不是一个真正训练得到的排序模型。<br>当然，<code>GRN</code> 类的 <code>Seq2Seq</code> 模型属于训练得到的深度模型，但往往难以覆盖广泛的序列召回需要，并且性能容易陷入瓶颈。</p>
<p>在序列生成每轮检索候选时，也是可以通过构建一个简单的模型来代替的，如下图所示：</p>
<p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/listwise/listwise5.png" alt="listwise5"></p>
<p>生成的时候，没有办法同时考虑上文和下文，往往只有上文，即已经排序好的 List 部分。故，就可以将<strong>前序物品序列</strong>作为主要的 input 信息，然后评估候选物品加入后的效率。如此构建一个有监督模型。但往往要注意性能，因为在生成的时候，要多轮调用，所以<strong>一般建议使用简单的 LR 模型即可</strong>。</p>
<h3 id="3-4-Listwise-特征设计"><a href="#3-4-Listwise-特征设计" class="headerlink" title="3.4 Listwise 特征设计"></a>3.4 Listwise 特征设计</h3><p>前面介绍过重排与精排的差异及其目标，所以在这里的特征设计上，我们要结合上述2点来考虑：</p>
<ul>
<li><code>上游特征</code>的使用:<ul>
<li>例如精排分，召回渠道，供给分布等；</li>
<li>此处非常重要，<strong>一方面是不浪费推荐系统上游的产出，另一方面也是重排环节效果&gt;=精排的保障</strong>。</li>
</ul>
</li>
<li><code>空间分布</code>的特征:<ul>
<li>例如窗口类目分布等，可以是前置窗口，也可是后置，当然前后横跨的窗口也是需要的；</li>
<li>这部分是重排模型的<strong>核心差异和优势之处，对这部分信息的捕捉好决定了重排效果增益空间</strong>。</li>
</ul>
</li>
</ul>
<blockquote>
<p>这里有一个风险：如果使用不当，容易对上游精排分数等形成强依赖，使得系统上下游耦合过重。</p>
</blockquote>
<p>如，过度依赖精排分，如果其因迭代带来分数分布明显变化，容易直接带崩重排，从而让重排成为精排迭代的一个限制。</p>
<p><strong>分析原因</strong>：带崩往往是由于分数分布变化，出现一些重排没见过的特征值。<br><strong>解法思路</strong>：分布可以变化，但新特征值/量纲跨越要尽量少，故可对上游特征进行脱敏。</p>
<blockquote>
<p>比如将原始分，归一化，分段，取分位点等等，以此尽量捕捉有效信息的同时，避免因分值变化而带来新特征值爆发、量纲跨越等问题。</p>
</blockquote>
<p><strong>参考文章</strong>：<br><a href="https://www.zhihu.com/question/20569832/answer/2292533580">LTR （learning to Rank） 在互联网中目前发展如何？</a><br><a href="https://www.infoq.cn/article/a1tj74y7V2EKFikKYcwv?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">KDD23|RankFormer:用Transformer去做LTR任务</a><br><a href="https://zhuanlan.zhihu.com/p/518821008">ListNet和ListMLE</a><br><a href="https://zhuanlan.zhihu.com/p/685323123">KDD’23 | 阿里, 排序和校准联合建模: 让listwise模型也能用于CTR预估</a><br><a href="https://mp.weixin.qq.com/s/YoQNDdsRE6LZWJXrRKubLw">Generator-Evaluator重排模型在淘宝流式场景的实践</a><br><a href="https://mp.weixin.qq.com/s/mYLFqBEM79hn9YSFGjVu3w">序列检索系统在淘宝首页信息流重排中的实践</a><br><a href="https://www.6aiq.com/article/1644883063745">渠江涛：重排序在快手短视频推荐系统中的演进</a><br><a href="https://juejin.cn/post/7210310775276519484">算法实践总结V3：重排在快手短视频</a><br><a href="https://geek.zshipu.com/post/%E4%BA%92%E8%81%94%E7%BD%91/%E5%90%8C%E5%9F%8E%E5%A4%9A%E6%A0%B7%E6%80%A7%E7%AE%97%E6%B3%95%E5%9C%A8%E9%83%A8%E8%90%BD%E7%9A%84%E5%AE%9E%E8%B7%B5%E5%92%8C%E6%80%9D%E8%80%83/">同城多样性算法在部落的实践和思考</a><br><a href="https://www.biaodianfu.com/learning-to-ranking.html">排序优化算法Learning to Ranking</a><br><a href="https://blog.csdn.net/qq_36478718/article/details/122598406">Learning to Rank : ListNet与ListMLE</a></p>
<hr>

          
        
      
    </div>
    
    
    
    
    <div>
      
    </div>

    <div>
      
    </div>

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.xiemingzhao.com/posts/transformer.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.postimg.cc/vBxZQfvz/img-0182.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小火箭的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/posts/transformer.html" itemprop="url">Transformer 解析</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2022-07-24T00:00:00+08:00">
                2022-07-24
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/" itemprop="url" rel="index">
                    <span itemprop="name">算法总结</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/posts/transformer.html#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/posts/transformer.html" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                  <span class="post-meta-divider">|</span>
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  6.7k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  29
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1 背景"></a>1 背景</h2><p>算法工程师在成长道路上基本绕不开深度学习，而 <code>Transformer</code> 模型更是其中的经典，它在2017年的<a href="https://arxiv.org/abs/1706.03762">《Attention is All You Need》</a>论文中被提出，直接掀起了 <code>Attention</code> 机制在深度模型中的广泛应用潮流。</p>
<p>在该模型中有许多奇妙的想法启发了诸多算法工程师的学习创造，为了让自己回顾复习更加方便，亦或让在学习的读者更轻松地理解，便写了这篇文章。形式上，在参考诸多优秀文章和博客后，这里还是采用结构与代码并行阐述的模式。</p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/posts/transformer.html#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    
    
    <div>
      
    </div>

    <div>
      
    </div>

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.xiemingzhao.com/posts/pidcontrol.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.postimg.cc/vBxZQfvz/img-0182.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小火箭的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/posts/pidcontrol.html" itemprop="url">PID 调控算法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2022-07-09T00:00:00+08:00">
                2022-07-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/" itemprop="url" rel="index">
                    <span itemprop="name">算法总结</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/posts/pidcontrol.html#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/posts/pidcontrol.html" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                  <span class="post-meta-divider">|</span>
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  2.3k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  9
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p><code>PID</code> 全称 <code>Proportional Integral Derivative</code>，拆分项分别是 <strong>比例（Proportional）、积分（Integral）和微分（Derivative）</strong>。是应用最为广泛的控制模型，有 100 余年的历史了，应用场景有四轴飞行器，汽车的定速巡航等。<br>官方流程图：</p>
<p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/notes/pidcontrol0.png" alt="pidcontrol0"></p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/posts/pidcontrol.html#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    
    
    <div>
      
    </div>

    <div>
      
    </div>

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.xiemingzhao.com/posts/mindmodel.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.postimg.cc/vBxZQfvz/img-0182.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小火箭的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/posts/mindmodel.html" itemprop="url">MIND（多兴趣）召回模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2022-06-16T00:00:00+08:00">
                2022-06-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index">
                    <span itemprop="name">召回模型</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/" itemprop="url" rel="index">
                    <span itemprop="name">算法总结</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/posts/mindmodel.html#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/posts/mindmodel.html" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                  <span class="post-meta-divider">|</span>
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  6.7k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  27
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>在深度学习召回算法领域，比较经典的包括了以下2大类：</p>
<ul>
<li>基于 <code>item2vec</code> 模型构建在线的i2i召回；</li>
<li>基于 <code>user2item</code> 泛双塔模型构建在线的u2i召回；</li>
</ul>
<blockquote>
<p>当然还有2阶以上的召回，<code>i2u2i</code>、<code>u2u2i</code>等，在这里不做重点介绍，最终目的都是为了召回 item。</p>
</blockquote>
<p>对于第一种，相信大家比较熟知的有从 <code>word2vec</code> 衍生出的<code>item2vec</code>、阿里的<code>deepwalk</code>以及<code>FM</code>等，核心方式都是离线构建出 item 的 Embedding，<strong>在online侧基于用户的行为序列，取其中的 item 作为 trigger 来进行倒排/近邻召回</strong>。</p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/posts/mindmodel.html#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    
    
    <div>
      
    </div>

    <div>
      
    </div>

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.xiemingzhao.com/posts/ranki2imodel.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.postimg.cc/vBxZQfvz/img-0182.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小火箭的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/posts/ranki2imodel.html" itemprop="url">RankI2I 召回简述</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2022-05-21T00:00:00+08:00">
                2022-05-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index">
                    <span itemprop="name">召回模型</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/" itemprop="url" rel="index">
                    <span itemprop="name">算法总结</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/posts/ranki2imodel.html#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/posts/ranki2imodel.html" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                  <span class="post-meta-divider">|</span>
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  1.2k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  4
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1 背景"></a>1 背景</h2><p>在推荐系统中，<code>i2i</code>类型的召回往往在多路召回中扮演中重要的角色，具有<strong>效率高、覆盖广、可解释、易调控</strong>等优势。常用的算法一般有<code>swing</code>,<code>icf</code>,<code>wbcos</code>以及<code>item2vec</code>等，虽然不同算法逻辑不同，实际上构建的倒排结果往往具有一定的重复，并且多路 i2i 在线上并存往往也会带来维护成本高，迭代效率低等问题。那么，<code>ranki2i</code> 是较为通用的将各种 i2i 有效整合到一起的一种方案。</p>
<h2 id="2-算法逻辑"><a href="#2-算法逻辑" class="headerlink" title="2 算法逻辑"></a>2 算法逻辑</h2><p><code>ranki2i</code> 算法承载着两个目标：</p>
<ul>
<li>合并分散的 i2i 召回；</li>
<li>提高 i2i 召回效率。</li>
</ul>
<p>为了完成上述 2 个目标，相应的转化成以下两个方案：</p>
<ul>
<li>构建一个 i2i 预估模型；</li>
<li>对所有的 i2i 候选 pair 对进行预估构建截断倒排结果。</li>
</ul>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/posts/ranki2imodel.html#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    
    
    <div>
      
    </div>

    <div>
      
    </div>

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.xiemingzhao.com/posts/wbcosrecall.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.postimg.cc/vBxZQfvz/img-0182.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小火箭的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/posts/wbcosrecall.html" itemprop="url">wbcos 召回</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2022-05-09T00:00:00+08:00">
                2022-05-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index">
                    <span itemprop="name">召回模型</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/" itemprop="url" rel="index">
                    <span itemprop="name">算法总结</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/posts/wbcosrecall.html#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/posts/wbcosrecall.html" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                  <span class="post-meta-divider">|</span>
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  1.6k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  6
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1 背景"></a>1 背景</h2><blockquote>
<p><code>wb</code> 意为 <code>weight base</code>，wbcos 即加权式的 cos。</p>
</blockquote>
<p><strong>思想：其实就是改进的 itemcos 来计算相似度。</strong></p>
<p>核心在于两点：</p>
<ul>
<li>user+session 内的 pair 重复出现的时候如何聚合，主要就是时间衰减和类目等维度加权；</li>
<li>user+session 间的 pair 如何聚合，主要是 session 丰富度加权；</li>
</ul>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/posts/wbcosrecall.html#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    
    
    <div>
      
    </div>

    <div>
      
    </div>

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.xiemingzhao.com/posts/swingrecall.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.postimg.cc/vBxZQfvz/img-0182.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小火箭的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/posts/swingrecall.html" itemprop="url">Swing 召回</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2022-04-24T00:00:00+08:00">
                2022-04-24
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index">
                    <span itemprop="name">召回模型</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/" itemprop="url" rel="index">
                    <span itemprop="name">算法总结</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/posts/swingrecall.html#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/posts/swingrecall.html" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                  <span class="post-meta-divider">|</span>
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  1k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  3
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p><strong>思想：来源于传统的 CF：</strong></p>
<ul>
<li>如果多个 user 都只共同点了 i1 和 i2，那么其一定是强关联的，这种关联是通过用户来传递的；</li>
<li>如果两个 user pair 对之间构成的 swing 结构越多，则每个结构越弱，在这个 pair 对上每个节点分到的权重越低。</li>
</ul>
<h2 id="1-原理"><a href="#1-原理" class="headerlink" title="1 原理"></a>1 原理</h2><p><code>Swing</code>意为摇摆或者秋千，它是基于图结构的一种实时推荐算法。主要公式为：</p>
<script type="math/tex; mode=display">Sim(i, j) = \sum_{u \in U_i \cup U_j} \sum_{v \in U_i \cup U_j} \frac{1}{\alpha + |I_u \cup I_v|}</script><p>结合前面的思想，公式表达的就是为了衡量物品 i 和 j 的<code>相似性</code>：<strong>考察都购买了物品 i 和 j 的用户 u 和 v， 如果这两个用户共同购买的物品越少，则物品 i 和 j 的相似性越高</strong>。</p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/posts/swingrecall.html#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    
    
    <div>
      
    </div>

    <div>
      
    </div>

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.xiemingzhao.com/posts/biasnet.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.postimg.cc/vBxZQfvz/img-0182.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小火箭的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/posts/biasnet.html" itemprop="url">推荐模型中的 position bias 和 debias</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2022-03-27T00:00:00+08:00">
                2022-03-27
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%B2%BE%E6%8E%92%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index">
                    <span itemprop="name">精排模型</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%B2%BE%E6%8E%92%E6%A8%A1%E5%9E%8B/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/" itemprop="url" rel="index">
                    <span itemprop="name">算法总结</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/posts/biasnet.html#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/posts/biasnet.html" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                  <span class="post-meta-divider">|</span>
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  1.9k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  7
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>在推荐系统中一个重要的任务就是 CTR 建模，其本质的思想便是<strong>预估 user 对 item 的点击率</strong>。但是实际中获取的样本往往是在一定条件（时间、机型、位置等）下的后验结果，所以使得建模的 Label 往往是夹杂了这些因素的结果。</p>
<p>这些影响后验结果的因素一般称为 <code>偏置（bias）项</code>，而去除这些偏置项的过程就称为 <code>消偏（debias）</code>。在这其中最重要的便是 <code>位置偏置（position bias）</code>，即 item 展示在不同位置会有不同的影响，且用户往往更偏向点击靠前的位置。本文将重点介绍业界在 <code>position bias</code> 消除上的一般做法和相关经验。</p>
<h2 id="2-Position-Bias"><a href="#2-Position-Bias" class="headerlink" title="2 Position Bias"></a>2 Position Bias</h2><p>看下面的图，是笔者实际工作场景中部分位置的 CTR 趋势图。可以明显地看到：</p>
<ul>
<li>呈现每 20 个 position 位一个周期；每刷请求的个数是 20.</li>
<li>周期内位置越靠前，CTR 越大；靠前效率高，用户更偏好点靠前的。</li>
</ul>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/posts/biasnet.html#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    
    
    <div>
      
    </div>

    <div>
      
    </div>

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.xiemingzhao.com/posts/youtubednn.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://i.postimg.cc/vBxZQfvz/img-0182.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小火箭的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/posts/youtubednn.html" itemprop="url">YouTubeDNN 和 WCE</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-12-18T00:00:00+08:00">
                2021-12-18
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%B2%BE%E6%8E%92%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index">
                    <span itemprop="name">精排模型</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%B2%BE%E6%8E%92%E6%A8%A1%E5%9E%8B/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/" itemprop="url" rel="index">
                    <span itemprop="name">算法总结</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/posts/youtubednn.html#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/posts/youtubednn.html" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                  <span class="post-meta-divider">|</span>
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  3.1k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  11
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1 背景"></a>1 背景</h2><p>这是一篇推荐算法领域经典的论文，它由 YouTube 在2016年发表在 RecSys 上的文章<a href="https://dl.acm.org/doi/pdf/10.1145/2959100.2959190">Deep Neural Networks for YouTube Recommendations</a>。<br>这篇文章是诸多推荐算法工程师的必学经典，可能很多人多次重读都会有新的思考，本文也重点总结文章的核心内容与一些实战经验的思考。</p>
<h2 id="2-原理"><a href="#2-原理" class="headerlink" title="2 原理"></a>2 原理</h2><p>首先便是其展示的系统链路示意图，这块与大多主流方案没有什么区别。</p>
<p><img src="https://mzxie-image.oss-cn-hangzhou.aliyuncs.com/algorithm/deepmodel/youtubednn0.png" alt="youtubednn0"></p>
<p>论文分别介绍了在 <code>recall</code> 和 <code>ranking</code> 两个模块的方案，但可以说，recall 部分的重要性远大于 ranking。就此文章发表后的几年而言，<em>recall 往往还在工业界主流召回的候选方案中，但 ranking 的方案基本已经成为历史，很少再使用了</em>，不过其思想还是值得学习的。</p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/posts/youtubednn.html#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    
    
    <div>
      
    </div>

    <div>
      
    </div>

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/2/">&gt;</a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="https://i.postimg.cc/vBxZQfvz/img-0182.jpg"
                alt="" />
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%7C%7C%20archive">
              
                  <span class="site-state-item-count">63</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">13</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">81</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/xiemingzhao" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-globe"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="514829265@qq.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-globe"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 &mdash; <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">小火箭</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">信仰</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>


<!--

  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>

-->



<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共285.6k字</span>
</div>

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<!-- 网站运行时间的设置 -->
<!--
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>  Sometimes your whole life boils down to one insame move.
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/09/2019 13:14:21");//此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);
</script>
-->
        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访客数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i> 总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'TpMiFGGr4T8FnG248uRRaf4H-gzGzoHsz',
        appKey: 'NYRTcUPshFEJWpUk54Bfu4nX',
        placeholder: '朋友记得留下昵称和邮箱，我会尽快回复您的！',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  


  

  <script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":true,"log":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true}});</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>

<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/love.js"></script>
